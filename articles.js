const articles = [
    {
        title:'Attitude',
        author:'Margaret Atwood',
        article:`I am of course overjoyed to be here today in the role of ceremonial object. There is more than the usual amount of satisfaction in receiving an honorary degree from the university that helped to form one’s erstwhile callow and ignorant mind into the thing of dubious splendor that it is today; whose professors put up with so many overdue term papers, and struggled to read one’s handwriting, of which ‘interesting’ is the best that has been said; at which one failed to learn Anglo-Saxon and somehow missed Bibliography entirely, a severe error which I trust no one present here today has committed; and at which one underwent excruciating agonies not only of soul but of body, later traced to having drunk too much coffee in the bowels of Wymilwood.

        It is to Victoria College that I can attribute the fact that Bell Canada, Oxford University Press and McClelland and Stewart all failed to hire me in the summer of ‘63, on the grounds that I was a) overqualified and b) couldn’t type, thus producing in me that state of joblessness, angst and cosmic depression which everyone knows is indispensable for novelists and poets, although nobody has ever claimed the same for geologists, dentists or chartered accountants. It is also due to Victoria College, incarnated in the person of Northrop Frye, that I didn’t run away to England to become a waitress, live in a garret, write masterpieces and get tuberculosis. He thought I might have more spare time for creation if I ran away to Boston, lived in a stupor, wrote footnotes and got anxiety attacks, that is, if I went to Graduate School, and he was right. So, for all the benefits conferred upon me by my Alma Mater, where they taught me that the truth would make me free but failed to warn me of the kind of trouble I’d get into by trying to tell it - I remain duly grateful.
        
        But everything has its price. No sooner had I tossed off a graceful reply to the letter inviting me to be present today than I began to realize the exorbitance of what was expected of me. I was going to have to come up with something to say,“You may not be able to alter reality, but you can alter your attitude towards it, and this, paradoxically, alters reality. Try it and see.”to a graduating class in 1983, year of the Ph.D. taxi driver, when young people have unemployment the way they used to have ugly blackheads; something presumably useful, wise, filled with resonance and overview, helpful, encouraging and optimistic. After all, you are being launched - though ever since I experienced the process, I’ve wondered why “convocation” is the name for it. “Ejection” would be better. Even in the best of times, it’s more or less like being pushed over a cliff, and these are not the best of times. In case you haven’t figured it out already, I’m here to tell you that it’s an armpit out there. As for your university degree, there are definitely going to be days when you will feel that you’ve been given a refrigerator and sent to the middle of a jungle, where there are no three-pronged grounded plugholes.
        
        Not only that, the year will come when you will wake up in the middle of the night and realize that the people you went to school with are in positions of power, and may soon actually be running things. If there’s anything more calculated to thick men’s blood with cold, it’s that. After all, you know how much they didn’t know then, and, given yourself as an example, you can’t assume they know a great deal more now. “We’re all doomed,” you will think. (For example: Brian Mulroney is only a year older than I am.) You may feel that the only thing to do when you’ve reached this stage is to take up nail-biting, mantras, or jogging, all of which would be recognized by animal behavior specialists as substitution activities, like scratching, which are resorted to in moments of unresolved conflict. But we’ll get around to some positive thinking in a moment.
        
        “What shall I tell them!” I thought, breaking out into a cold sweat, as I tossed and turned night after night. (Lest you leap to indulge in Calvinistic guilt at the idea of having been the proximate cause of my discomfort, let me hasten to add that I was on a boat. The tossing and turning was par for the course, and the cold sweat can be cured by Gravol). For a while I toyed with the idea of paraphrasing Kurt Vonnegut, who told one graduating class, “Everything is going to become unbelievably worse and will never get better again,” and walked off the stage. But that’s the American style: boom or bust. A Canadian would be more apt to say, “things may be pretty mediocre but let’s at least try to hold the line.”
        
        Then I thought that maybe I could say a few words on the subject of a liberal arts education, and how it prepares you for life. But sober reflection led me to the conclusion that this topic too was a washout; for, as you will soon discover, a liberal arts education doesn’t exactly prepare you for life. A preparation-for-life curriculum would not consist of courses on Victorian Thought and French Romanticism, but of things like How to Cope With Marital Breakdown, Getting More for your Footwear Dollar, Dealing With Stress, and How To Keep Your Fingernails from Breaking Off by Always Filing Them Towards the Center; in other words, it would read like the contents page of Homemakers Magazine, which is why Homemakers Magazine is so widely read, even by me. Or, for boys, Forbes or The Economist , and Improving Your Place in the Power Hierarchy by Choosing the Right Suit. (Dark blue with a faint white pinstripe, not too far apart, in case you’re interested.)
        
        Or maybe, I thought, I should expose glaring errors in the educational system, or compile a list of things I was taught which are palpably not true. For instance, in high school I made the mistake of taking Home Economics instead of Typing - we thought, in those days, that if you took the commercial course most of your eyebrows would come off and would have to be drawn on with a pencil for the rest of your life - where I was told that every meal should consist of a brown thing, a white thing, a yellow thing and a green thing; that it was not right to lick the spoon while cooking; and that the inside of a dress seam was as important as the outside. All three of these ideas are false and should be discarded immediately by anyone who still holds them.
        
        Nor did anyone have the foresight to inform me that the best thing I could do for myself as a writer would be back and wrist exercises. No one has yet done a study of this, but they will, and when they start excavating and measuring the spines and arm bones of the skeletons of famous writers of the past I am sure they will find that those who wrote the longest novels, such as Dickens and Melville, also had the thickest wrists. The real reason that Emily Dickinson stuck to lyric poems with relatively few stanzas is that she had spindly fingers. You may scoff, but future research will prove me right.
        
        But I then thought, I shouldn’t talk about writing. Few of this graduating class will wish to be writers, and those that do should by no means be encouraged. Weave a circle round them thrice, and close your eyes holy dread, because who needs the competition? What with the proliferation of Creative Writing courses, a mushroom of recent growth all but unknown in my youth, we will soon have a state of affairs in which everybody writes and nobody reads, the exact reverse of the way things were when I was composing dolorous verses in a rented cupboard on Charles Street in the early sixties.
        
        Or maybe, I thought, I should relate to them a little known fact of shocking import, which they will remember vividly when they have all but forgotten the rest of this speech. For example: nobody ever tells you, but did you know that when you have a baby your hair falls out? Not all of it, and not all at once, but it does fall out. It has something to do with a zinc imbalance. The good news is that it does grow back in. This only applies to girls. With boys, it falls out whether you have a baby or not, and it never grows back in; but even then there is hope. In a pinch, you can resort to quotation, a commodity which a liberal arts education teaches you to treat with respect, and I offer the following: “God only made a few perfect heads, and the rest lie covered with hair.”
        
        Which illustrates the following point: when faced with the inevitable, you always have a choice. You may not be able to alter reality, but you can alter your attitude towards it. As I learned during my liberal arts education, any symbol can have, in the imaginative context, two versions, a positive and a negative. Blood can either be the gift of life or what comes out of you when you cut your wrists in the bathtub. Or, somewhat less drastically, if you spill your milk you’re left with a glass which is either half empty or half full.
        
        Which brings us to the hidden agenda of this speech. What you are being ejected into today is a world that is both half empty and half full. On the one hand, the biosphere is rotting away. The raindrops that keep falling on your head are also killing the fish, the trees, the animals, and, if they keep being as acid as they are now, they’ll eventually do away with things a lot closer to home, such as crops, front lawns and your digestive tract. Nature is no longer what surrounds us, we surround it, and the switch has not been for the better. On the other hand, unlike the ancient Egyptians, we as a civilization know what mistakes we are making and we also have the technology to stop making them; all that is lacking is the will.
        
        Another example: on the one hand, we ourselves live daily with the threat of annihilation. We’re just a computer button and a few minutes away from it, and the gap between us and it is narrowing every day. We secretly think in terms not of “If the Bomb Drops” but of “When the Bomb Drops”, and it’s understandable if we sometimes let ourselves slide into a mental state of powerlessness and consequent apathy. On the other hand, the catastrophe that threatens us as a species, and most other species as well, is not unpredictable and uncontrollable, like the eruption of the volcano that destroyed Pompeii. If it occurs, we can die with the dubious satisfaction of knowing that the death of the world was a man-made and therefore preventable event, and that the failure to prevent it was a failure of human will.
        
        This is the kind of world we find ourselves in, and it’s not pleasant. Faced with facts this depressing, the question of the economy - or how many of us in this country can afford two cars doesn’t really loom too large, but you’d never know it from reading the papers. Things are in fact a lot worse elsewhere, where expectations center not on cars and houses and jobs but on the next elusive meal. That’s part of the down side. The up side, here and now, is that this is still more or less a democracy; you don’t get shot or tortured yet for expressing an opinion, and politicians, motivated as they may be by greed and the lust for power, are nevertheless or because of this, still swayed by public opinion. The issues raised in any election are issues perceived by those who want power to be of importance to those in a position to confer it upon them. In other words, if enough people show by the issues they raise and by the way they’re willing to vote that they want changes made, then change becomes possible. You may not be able to alter reality, but you can alter your attitude towards it, and this, paradoxically, alters reality.
        
        Try it and see.`
    },
    {
        title:'This is Water',
        author:'David Foster Wallace',
        article:`This is Water

        “Greetings parents and congratulations to Kenyon’s graduating class of 2005. There are these two young fish swimming along and they happen to meet an older fish swimming the other way, who nods at them and says “Morning, boys. How’s the water?” And the two young fish swim on for a bit, and then eventually one of them looks over at the other and goes “What the hell is water?”
        
        This is a standard requirement of US commencement speeches, the deployment of didactic little parable-ish stories. The story thing turns out to be one of the better, less bullshitty conventions of the genre, but if you’re worried that I plan to present myself here as the wise, older fish explaining what water is to you younger fish, please don’t be. I am not the wise old fish. The point of the fish story is merely that the most obvious, important realities are often the ones that are hardest to see and talk about. Stated as an English sentence, of course, this is just a banal platitude, but the fact is that in the day to day trenches of adult existence, banal platitudes can have a life or death importance, or so I wish to suggest to you on this dry and lovely morning.
        
        Of course the main requirement of speeches like this is that I’m supposed to talk about your liberal arts education’s meaning, to try to explain why the degree you are about to receive has actual human value instead of just a material payoff. So let’s talk about the single most pervasive cliché in the commencement speech genre, which is that a liberal arts education is not so much about filling you up with knowledge as it is about “teaching you how to think.” If you’re like me as a student, you’ve never liked hearing this, and you tend to feel a bit insulted by the claim that you needed anybody to teach you how to think, since the fact that you even got admitted to a college this good seems like proof that you already know how to think. But I’m going to posit to you that the liberal arts cliché turns out not to be insulting at all, because the really significant education in thinking that we’re supposed to get in a place like this isn’t really about the capacity to think, but rather about the choice of what to think about. If your total freedom of choice regarding what to think about seems too obvious to waste time discussing, I’d ask you to think about fish and water, and to bracket for just a few minutes your scepticism about the value of the totally obvious.
        
        Here’s another didactic little story. There are these two guys sitting together in a bar in the remote Alaskan wilderness. One of the guys is religious, the other is an atheist, and the two are arguing about the existence of God with that special intensity that comes after about the fourth beer. And the atheist says: “Look, it’s not like I don’t have actual reasons for not believing in God. It’s not like I haven’t ever experimented with the whole God and prayer thing. Just last month I got caught away from the camp in that terrible blizzard, and I was totally lost and I couldn’t see a thing, and it was 50 below, and so I tried it: I fell to my knees in the snow and cried out ‘Oh, God, if there is a God, I’m lost in this blizzard, and I’m gonna die if you don’t help me.’” And now, in the bar, the religious guy looks at the atheist all puzzled. “Well then you must believe now,” he says, “After all, here you are, alive.” The atheist just rolls his eyes. “No, man, all that was was a couple Eskimos happened to come wandering by and showed me the way back to camp.”
        
        It’s easy to run this story through kind of a standard liberal arts analysis: the exact same experience can mean two totally different things to two different people, given those people’s two different belief templates and two different ways of constructing meaning from experience. Because we prize tolerance and diversity of belief, nowhere in our liberal arts analysis do we want to claim that one guy’s interpretation is true and the other guy’s is false or bad. Which is fine, except we also never end up talking about just where these individual templates and beliefs come from. Meaning, where they come from INSIDE the two guys. As if a person’s most basic orientation toward the world, and the meaning of his experience were somehow just hard-wired, like height or shoe-size; or automatically absorbed from the culture, like language. As if how we construct meaning were not actually a matter of personal, intentional choice. Plus, there’s the whole matter of arrogance. The nonreligious guy is so totally certain in his dismissal of the possibility that the passing Eskimos had anything to do with his prayer for help. True, there are plenty of religious people who seem arrogant and certain of their own interpretations, too. They’re probably even more repulsive than atheists, at least to most of us. But religious dogmatists’ problem is exactly the same as the story’s unbeliever: blind certainty, a close-mindedness that amounts to an imprisonment so total that the prisoner doesn’t even know he’s locked up.
        
        The point here is that I think this is one part of what teaching me how to think is really supposed to mean. To be just a little less arrogant. To have just a little critical awareness about myself and my certainties. Because a huge percentage of the stuff that I tend to be automatically certain of is, it turns out, totally wrong and deluded. I have learned this the hard way, as I predict you graduates will, too.
        
        Here is just one example of the total wrongness of something I tend to be automatically sure of: everything in my own immediate experience supports my deep belief that I am the absolute centre of the universe; the realest, most vivid and important person in existence. We rarely think about this sort of natural, basic self-centredness because it’s so socially repulsive. But it’s pretty much the same for all of us. It is our default setting, hard-wired into our boards at birth. Think about it: there is no experience you have had that you are not the absolute centre of. The world as you experience it is there in front of YOU or behind YOU, to the left or right of YOU, on YOUR TV or YOUR monitor. And so on. Other people’s thoughts and feelings have to be communicated to you somehow, but your own are so immediate, urgent, real.
        
        Please don’t worry that I’m getting ready to lecture you about compassion or other-directedness or all the so-called virtues. This is not a matter of virtue. It’s a matter of my choosing to do the work of somehow altering or getting free of my natural, hard-wired default setting which is to be deeply and literally self-centered and to see and interpret everything through this lens of self. People who can adjust their natural default setting this way are often described as being “well-adjusted”, which I suggest to you is not an accidental term.
        
        Given the triumphant academic setting here, an obvious question is how much of this work of adjusting our default setting involves actual knowledge or intellect. This question gets very tricky. Probably the most dangerous thing about an academic education–least in my own case–is that it enables my tendency to over-intellectualise stuff, to get lost in abstract argument inside my head, instead of simply paying attention to what is going on right in front of me, paying attention to what is going on inside me.
        
        As I’m sure you guys know by now, it is extremely difficult to stay alert and attentive, instead of getting hypnotised by the constant monologue inside your own head (may be happening right now). Twenty years after my own graduation, I have come gradually to understand that the liberal arts cliché about teaching you how to think is actually shorthand for a much deeper, more serious idea: learning how to think really means learning how to exercise some control over how and what you think. It means being conscious and aware enough to choose what you pay attention to and to choose how you construct meaning from experience. Because if you cannot exercise this kind of choice in adult life, you will be totally hosed. Think of the old cliché about “the mind being an excellent servant but a terrible master.”
        
        This, like many clichés, so lame and unexciting on the surface, actually expresses a great and terrible truth. It is not the least bit coincidental that adults who commit suicide with firearms almost always shoot themselves in: the head. They shoot the terrible master. And the truth is that most of these suicides are actually dead long before they pull the trigger.
        
        And I submit that this is what the real, no bullshit value of your liberal arts education is supposed to be about: how to keep from going through your comfortable, prosperous, respectable adult life dead, unconscious, a slave to your head and to your natural default setting of being uniquely, completely, imperially alone day in and day out. That may sound like hyperbole, or abstract nonsense. Let’s get concrete. The plain fact is that you graduating seniors do not yet have any clue what “day in day out” really means. There happen to be whole, large parts of adult American life that nobody talks about in commencement speeches. One such part involves boredom, routine and petty frustration. The parents and older folks here will know all too well what I’m talking about.
        
        By way of example, let’s say it’s an average adult day, and you get up in the morning, go to your challenging, white-collar, college-graduate job, and you work hard for eight or ten hours, and at the end of the day you’re tired and somewhat stressed and all you want is to go home and have a good supper and maybe unwind for an hour, and then hit the sack early because, of course, you have to get up the next day and do it all again. But then you remember there’s no food at home. You haven’t had time to shop this week because of your challenging job, and so now after work you have to get in your car and drive to the supermarket. It’s the end of the work day and the traffic is apt to be: very bad. So getting to the store takes way longer than it should, and when you finally get there, the supermarket is very crowded, because of course it’s the time of day when all the other people with jobs also try to squeeze in some grocery shopping. And the store is hideously lit and infused with soul-killing muzak or corporate pop and it’s pretty much the last place you want to be but you can’t just get in and quickly out; you have to wander all over the huge, over-lit store’s confusing aisles to find the stuff you want and you have to manoeuvre your junky cart through all these other tired, hurried people with carts (et cetera, et cetera, cutting stuff out because this is a long ceremony) and eventually you get all your supper supplies, except now it turns out there aren’t enough check-out lanes open even though it’s the end-of-the-day rush. So the checkout line is incredibly long, which is stupid and infuriating. But you can’t take your frustration out on the frantic lady working the register, who is overworked at a job whose daily tedium and meaninglessness surpasses the imagination of any of us here at a prestigious college.
        
        But anyway, you finally get to the checkout line’s front, and you pay for your food, and you get told to “Have a nice day” in a voice that is the absolute voice of death. Then you have to take your creepy, flimsy, plastic bags of groceries in your cart with the one crazy wheel that pulls maddeningly to the left, all the way out through the crowded, bumpy, littery parking lot, and then you have to drive all the way home through slow, heavy, SUV-intensive, rush-hour traffic, et cetera et cetera.
        
        Everyone here has done this, of course. But it hasn’t yet been part of you graduates’ actual life routine, day after week after month after year.
        
        But it will be. And many more dreary, annoying, seemingly meaningless routines besides. But that is not the point. The point is that petty, frustrating crap like this is exactly where the work of choosing is gonna come in. Because the traffic jams and crowded aisles and long checkout lines give me time to think, and if I don’t make a conscious decision about how to think and what to pay attention to, I’m gonna be pissed and miserable every time I have to shop. Because my natural default setting is the certainty that situations like this are really all about me. About MY hungriness and MY fatigue and MY desire to just get home, and it’s going to seem for all the world like everybody else is just in my way. And who are all these people in my way? And look at how repulsive most of them are, and how stupid and cow-like and dead-eyed and nonhuman they seem in the checkout line, or at how annoying and rude it is that people are talking loudly on cell phones in the middle of the line. And look at how deeply and personally unfair this is.
        
        Or, of course, if I’m in a more socially conscious liberal arts form of my default setting, I can spend time in the end-of-the-day traffic being disgusted about all the huge, stupid, lane-blocking SUV’s and Hummers and V-12 pickup trucks, burning their wasteful, selfish, 40-gallon tanks of gas, and I can dwell on the fact that the patriotic or religious bumper-stickers always seem to be on the biggest, most disgustingly selfish vehicles, driven by the ugliest [responding here to loud applause] — this is an example of how NOT to think, though — most disgustingly selfish vehicles, driven by the ugliest, most inconsiderate and aggressive drivers. And I can think about how our children’s children will despise us for wasting all the future’s fuel, and probably screwing up the climate, and how spoiled and stupid and selfish and disgusting we all are, and how modern consumer society just sucks, and so forth and so on.
        
        You get the idea.
        
        If I choose to think this way in a store and on the freeway, fine. Lots of us do. Except thinking this way tends to be so easy and automatic that it doesn’t have to be a choice. It is my natural default setting. It’s the automatic way that I experience the boring, frustrating, crowded parts of adult life when I’m operating on the automatic, unconscious belief that I am the centre of the world, and that my immediate needs and feelings are what should determine the world’s priorities.
        
        The thing is that, of course, there are totally different ways to think about these kinds of situations. In this traffic, all these vehicles stopped and idling in my way, it’s not impossible that some of these people in SUV’s have been in horrible auto accidents in the past, and now find driving so terrifying that their therapist has all but ordered them to get a huge, heavy SUV so they can feel safe enough to drive. Or that the Hummer that just cut me off is maybe being driven by a father whose little child is hurt or sick in the seat next to him, and he’s trying to get this kid to the hospital, and he’s in a bigger, more legitimate hurry than I am: it is actually I who am in HIS way.
        
        Or I can choose to force myself to consider the likelihood that everyone else in the supermarket’s checkout line is just as bored and frustrated as I am, and that some of these people probably have harder, more tedious and painful lives than I do.
        
        Again, please don’t think that I’m giving you moral advice, or that I’m saying you are supposed to think this way, or that anyone expects you to just automatically do it. Because it’s hard. It takes will and effort, and if you are like me, some days you won’t be able to do it, or you just flat out won’t want to.
        
        But most days, if you’re aware enough to give yourself a choice, you can choose to look differently at this fat, dead-eyed, over-made-up lady who just screamed at her kid in the checkout line. Maybe she’s not usually like this. Maybe she’s been up three straight nights holding the hand of a husband who is dying of bone cancer. Or maybe this very lady is the low-wage clerk at the motor vehicle department, who just yesterday helped your spouse resolve a horrific, infuriating, red-tape problem through some small act of bureaucratic kindness. Of course, none of this is likely, but it’s also not impossible. It just depends what you want to consider. If you’re automatically sure that you know what reality is, and you are operating on your default setting, then you, like me, probably won’t consider possibilities that aren’t annoying and miserable. But if you really learn how to pay attention, then you will know there are other options. It will actually be within your power to experience a crowded, hot, slow, consumer-hell type situation as not only meaningful, but sacred, on fire with the same force that made the stars: love, fellowship, the mystical oneness of all things deep down.
        
        Not that that mystical stuff is necessarily true. The only thing that’s capital-T True is that you get to decide how you’re gonna try to see it.
        
        This, I submit, is the freedom of a real education, of learning how to be well-adjusted. You get to consciously decide what has meaning and what doesn’t. You get to decide what to worship.
        
        Because here’s something else that’s weird but true: in the day-to-day trenches of adult life, there is actually no such thing as atheism. There is no such thing as not worshipping. Everybody worships. The only choice we get is what to worship. And the compelling reason for maybe choosing some sort of god or spiritual-type thing to worship–be it JC or Allah, be it YHWH or the Wiccan Mother Goddess, or the Four Noble Truths, or some inviolable set of ethical principles–is that pretty much anything else you worship will eat you alive. If you worship money and things, if they are where you tap real meaning in life, then you will never have enough, never feel you have enough. It’s the truth. Worship your body and beauty and sexual allure and you will always feel ugly. And when time and age start showing, you will die a million deaths before they finally grieve you. On one level, we all know this stuff already. It’s been codified as myths, proverbs, clichés, epigrams, parables; the skeleton of every great story. The whole trick is keeping the truth up front in daily consciousness.
        
        Worship power, you will end up feeling weak and afraid, and you will need ever more power over others to numb you to your own fear. Worship your intellect, being seen as smart, you will end up feeling stupid, a fraud, always on the verge of being found out. But the insidious thing about these forms of worship is not that they’re evil or sinful, it’s that they’re unconscious. They are default settings.
        
        They’re the kind of worship you just gradually slip into, day after day, getting more and more selective about what you see and how you measure value without ever being fully aware that that’s what you’re doing.
        
        And the so-called real world will not discourage you from operating on your default settings, because the so-called real world of men and money and power hums merrily along in a pool of fear and anger and frustration and craving and worship of self. Our own present culture has harnessed these forces in ways that have yielded extraordinary wealth and comfort and personal freedom. The freedom all to be lords of our tiny skull-sized kingdoms, alone at the centre of all creation. This kind of freedom has much to recommend it. But of course there are all different kinds of freedom, and the kind that is most precious you will not hear much talk about much in the great outside world of wanting and achieving…. The really important kind of freedom involves attention and awareness and discipline, and being able truly to care about other people and to sacrifice for them over and over in myriad petty, unsexy ways every day.
        
        That is real freedom. That is being educated, and understanding how to think. The alternative is unconsciousness, the default setting, the rat race, the constant gnawing sense of having had, and lost, some infinite thing.
        
        I know that this stuff probably doesn’t sound fun and breezy or grandly inspirational the way a commencement speech is supposed to sound. What it is, as far as I can see, is the capital-T Truth, with a whole lot of rhetorical niceties stripped away. You are, of course, free to think of it whatever you wish. But please don’t just dismiss it as just some finger-wagging Dr Laura sermon. None of this stuff is really about morality or religion or dogma or big fancy questions of life after death.
        
        The capital-T Truth is about life BEFORE death.
        
        It is about the real value of a real education, which has almost nothing to do with knowledge, and everything to do with simple awareness; awareness of what is so real and essential, so hidden in plain sight all around us, all the time, that we have to keep reminding ourselves over and over:
        
        “This is water.”
        
        “This is water.”
        
        It is unimaginably hard to do this, to stay conscious and alive in the adult world day in and day out. Which means yet another grand cliché turns out to be true: your education really IS the job of a lifetime. And it commences: now.
        
        I wish you way more than luck.`    
    },
    {
        title:'Why Go Out?',
        author:'Sheila Heti',
        article:`This lecture was one of three lectures delivered at Trampoline Hall in New York on March 22, 2006.

        I wonder why I am up here on this stage when I’d rather be at home, when being at home would be so much more comforting. And I wonder why all of you are sitting there in the audience, when so many of you would also be happier at home.

        At home, you can wear your pyjamas. No one is going to snub you or disappoint you. At Trampoline Hall, you could be snubbed, or disappointed. The scotch is not cheap. It is less depressing to think the same thoughts you thought yesterday, than to have the same conversation you had last week. Few of us will get laid. Why did we go out? My father never goes out. His emotional life is absolutely even keel. He is a deeply rational person. He doesn’t see the advantages.
        
        For many years I have asked myself, Why do you spend time with other people? but I never really attempted to come up with an answer. I always believed I was asking myself a rhetorical question, but this week I thought I would try and find an answer, because a question you ask yourself a thousand times eventually deserves to be answered.

        And I figure if I know why I go out, I might feel less suspicious of myself for going out. I might criticize myself less. I might be able to look around a party without thinking, What a fool – why did you come – you should have stayed at home.

        II.
        The first thing I did in my search for an answer to “why go out” was write down a list of every single reason I could think of to go out – there were about twelve – and then I noticed, after staring at the paper, that those smaller reasons could be divided up into four major reasons for leaving the house:

        1. Desire (for sex, love, companionship, whatever).
        2. Sociological curiosity / aesthetic appreciation.
        3. To test ourselves.
        4. Someone else wants to hang out.

        III.
        A couple of years ago I quit smoking, and to help myself along, I read a book called Allen Carr’s Easyway to Stop Smoking. (‘Easyway’ is written as one word and has a little R beside it, meaning it’s a registered trademark. Despite those two details, it is a really excellent book, and I highly recommend it.)

        Now, Alan Carr’s basic premise is twofold:

        First: you have to accept that smoking is not a habit, it is a drug addiction; and

        Second: the only way to quit smoking is to never have a cigarette again.

        He goes on to explain that every smoker has brainwashed themselves into believing that smoking helps them in some way – calms them down, allows them to focus, makes an event feel more celebratory – when the truth is, all smoking a cigarette does is temporarily satisfy the craving for a cigarette, while reintroducing into your body the very substance you will once again crave.

        What the smoker needs to do to quit, is undo the brainwashing that cigarettes help them in any way, then suffer several weeks of physical withdrawal – a feeling he likens to a physical longing, but not unbearable – and then never have another cigarette again. Oh, and a positive frame of mind is essential. When you experience a craving, you’re to take this as a sign your body is transforming into the body of a non-smoker, and you should cheer, “Yippee! I’m free!”

        Well, I followed his advice, and it worked.

        The other day, I was sitting alone in a Mexican restaurant and wondering whether it is possible to quit people, and good old Alan Carr came to mind. It’s maybe because I recently ended a relationship, and also have not been spending much time in my city, and my body has been experiencing very similar sensations as it did when I gave up cigarettes two years ago; it’s a physical ache that comes and goes, that’s almost painful, a sort of gaping emptiness, a void that needs to be filled. It often seems like the only way to cure myself of this craving is to give in – to return to him, to sleep with someone new… Not until you tear yourself from everyone you love does it appear that you are actually physically addicted to people. The longing for a person is almost identical to the longing for a smoke. It’s weird.
        
        Anyway, I am not a stoic. My response to withdrawal – which has been to flee into semi-soothing rebound relationships – has prevented me from being able to stand before you today and declare with confidence that it is possible to renounce people, to bear the weeks of physical withdrawl symptoms, and thereafter attain the qualities that Alan Carr claims the non-smoker is in possession of: “health, energy, wealth, peace of mind, confidence, courage, self-respect, happiness and freedom.”

        But though it wasn’t recent, I have spent time alone in the past, and in my memories of these times – the happiest times of my life – I really did seem possessed of substantially more courage, confidence, self-respect, freedom, energy, and peace of mind, than those times when I’ve surrounded myself with people.

        And if that’s the truth, and my memory’s not lying – why go out?

        Alan Carr advises smokers who are considering quitting to put the following three questions to themselves, and I think we can also ponder them as we consider whether it is worthwhile to try and be cured of our addiction to people. As the smoker considers smoking, we ask of socializing:

        1. What is it doing for me?
        2. Do I actually enjoy it?
        3. Do I really need to go through life paying through the nose just to stick these things in my mouth and suffocate myself?

        1. What is it actually doing for me?

        As I suggested earlier, we get together with people to satisfy desires – the desire to love and be loved, the desire for sex, talk, companionship, good times, all those things. To which Alan Carr might retort: “We talk about smoking being relaxing or giving satisfaction. But how can you be satisfied unless you were dissatisfied in the first place?

        And truly, who has ever been satisfied by people?

        A few weeks ago, for instance, I was deeply insulted by a conceptual poet who lives in your town, who had come to my town to do a reading. I admire his work, so I went – knowing as I left my apartment that I was risking my admiration for him – “What if he is an asshole?” I asked myself, closing the door. “Never mind,” I replied, turning the key, for my curiosity surpassed my fear.

        Arriving at the bar that night, I spotted a small man of nearly forty years old, wearing an ostentatious suit and hat, walking about the room like he had a cock the size of Kansas. “He must be the conceptual poet,” I said to myself, and I was right. I begged not to be introduced, but my friend introduced us anyway, calling me, as she did so, a “novelist.” I told him how much I admired a particular book of his, and when I was done, he sort of looked me over and said, “You’re a novelist? Really? What interest could you possibly have in my work?.
        
        … … … In case you missed it, that was the terrible insult.

        Of course, telling someone your insult is like telling someone your dream; the specific emotional core of it cannot be communicated; all that comes across are disconnected and meaningless symbols. But let me assure you, this conceptual poet was digging his nails into my heart – he knew it, and, five minutes later, I suddenly felt it, too – which led to a week and a half of fuming in bed, unable to sleep, me declaring this man my enemy, the reconceiving of a magazine article I was writing in such a way as to include a subtextual layer that would annihilate conceptual poetics, a week and a half of going out every night and talking through the insult with each of my friends – what am I even saying? It took leaving the continent for the insult to finally recede into the background of my days, and for me to regain my equilibrium.

        But anyway, it is pretty be far-fetched to claim that people provide satisfaction and relaxation. Or at least, if they sometimes do, they as often do not.

        Alan Carr’s second question: “Do I actually enjoy it?”

        Does anyone actually enjoy more than one party in six? Does sex lead to satisfaction, or merely make us want more sex, better sex, different sex, even as we’re having it? The same goes for conversation, companionship, everything.

        No, other people don’t satisfy us, but rather, like cigarettes, give us the temporary illusion of satisfaction, while prolonging our dependence. And if we weren’t dependent on other people?
        
        Alan Carr’s Easyway lists the following psychological gains from quitting:

        1. The return of your confidence and courage;
        2. Freedom from the slavery;
        3. Not having to go through life suffering the awful black shadows at the back of your mind, knowing you are being despised by half of the population, and worst of all, despising yourself.
        
        And so, let us for the moment renounce people! Not in the doomed-to-failure way – renouncing while imagining we are depriving ourselves, forever plagued by doubts –

        “how long will the craving last?”
        “will I ever be happy again?
        “will I ever enjoy a meal again?”
        “how will I cope with stress in the future?”
        “will I ever want to get up in the morning?
    
        – but rather joyfully and willingly let us renounce people… and bring on self-confidence, courage, energy, peace of mind, and self-respect.

        IV.
        I have a friend who has made it his sort of art project to set up nights at which people amuse themselves in various ways. He has taught charades classes, he has invited the city into a bar to play board games, he has organized a roomful of people to play Torx, which is a child’s toy, a robot stick that issues instructions on how to bend it. He has been profiled in a local newspaper as someone who is providing fun alternatives to concerts and bars and house parties, which, of course, are old-fashioned and worn-out. But I know him well enough to know that he doesn’t much care whether Nadia or Jim are getting enough fun in their lives. What my friend is up to, I believe, is something more sinister.

        First, a few details to paint the scene:

        1. His calls his games night ‘Room 101.” The event is held in a bar and people eat cheesies from bowls and play Scrabble and Pictionary and other games at small tables, and every twenty minutes or so he get up at the front of the room on a little stage and rings a bell and forces only those people who seem to be enjoying their game overly much to terminate the game and disperse and play something else. If he had peoples’ fun in mind, I contend that he would not force those who are having the most fun to abandon their game.

        2. His promotional poster for these nights show a boy playing Monopoly with two rats. Also, if you look closely, you can see there are little bars on the window. He took the name ‘Room 101’ from the book 1984; it refers to the room in which they torture people, and it turns out his secret motto for these games nights is: “We torture you with fun!” Which might be the motto of every partyever.

        Finally: His charades class was not called “How to play charades” or “How to have fun playing charades,” but rather: “How to be good at playing charades.” And his introductory talk to the event only cursorily involved which hand signals to use when; mostly he talked about what he called “charades skills” – like, how being good at charades is about being a good communicator, and a good listener, and requires imagination, and sympathy, and understanding – all of which are, more truly than charades skills, life skills.

        And so his students or audience or whatever you’d call them – if they’re no good at playing charades – can only assume one thing. Since the terms for “goodness” were laid out very clearly at the beginning of class, if you’re not good at playing charades, you are forced to conclude that it’s not because you don’t know the hand gestures, it’s not because you’re not a good actor, but rather it’s because you can’t listen, or you’re not sympathetic, or you don’t have sufficient (as he put it at the beginning of class) “intellectual-analytical skills, motor-expressive skills, creative skills, and emotional-inter-personal skills.”

        The secret lesson of his charades class is: if you’re not good at being a charades player, maybe it’s actually because you’re not an entirely good at being a person. This is called being tortured with fun.

        Yes. I’ve come to the conclusion that what my friend is trying to do is organize events that capture and crystallize and reproduce the effects of ordinary socializing – which is not quite about fun, or about learning how to be good at having fun, but, more distinctly, about learning how to be good at being a person, and, the unfortunate corollary of this, seeing how far from good at being a person you are.

        Why go out? Because if what we want more than anything is to attain self-confidence, health, energy, and peace of mind, we should stay in. We could be like little Buddhas, meditating and masturbating and watching TV. And we could imagine ourselves to be brilliant, and kind, and good lecturers, and good listeners, and utterly loving – and there’d be no way to prove it otherwise.

        One final story: For the first six months of 2005 I lived alone in Montreal; I went because I was overwhelmed and I picked Montreal because I had no friends there, and for the first few weeks all I experienced were pangs of withdrawal for everyone I loved. It was awful and all-consuming… and then it passed. And once it passed, I was in heaven. There I sat in my lovely, cheap apartment – no distractions, no email, surrounded by books. There was a grocery store across the street. The mountain was two blocks away, and I could climb it whenever I wanted. Self-confidence, health, happiness, the equanimity of the non-smoker – all were mine.

        And then… I destroyed it. I met someone and then another person and before I knew it, all of the chaos of life came back, along with all my self-doubt and anxiety and fear.

        But perhaps that’s what it’s for – self-confidence and courage and energy and peace – perhaps it’s to be used in the world. Perhaps there’s only one thing to do with it: spend it.

        I’m always super-conscious of how whenever I go out into the world, whenever I get involved in a relationship, my idea of who I think I am utterly collides with the reality of who I actually am. And I continue to go out even though who I am always comes up short. I always prove myself to be less generous, less charming, less considerate, not as bold or energetic or intelligent or courageous as I imagined in my solitude. And I’m always being insulted, or snubbed, or disappointed. And I’m never in my pyjamas.

        And yet, in some way, maybe this is better. Each of us in this room could suffer the pangs of withdrawal and gain the serenity of the non-smoker. We could be demi-gods in our little castles, all alone, but perhaps, at heart, none of us here wants that. Maybe the only cure for self-confidence and courage is humility. Maybe we go out in order to fall short… because we want to learn how to be good at being people… and moreover, because we want to be people.

        And so, to return to Alan Carr’s final question to the would-be quitter: “Do I really need to go through life paying through the nose, just to stick these things in my mouth and suffocate myself?”

        Yes, Mr. Carr, yes!`
    },
    {
        title:'The Women\'s Movement',
        author:'Joan Didion',
        article:`To make an omelette you need not only those broken eggs but someone “oppressed” to break them: every revolutionist is presumed to under stand that, and also every woman, which either does or does not make 51 per cent of the population of the United States a potentially revolu tionary class. The creation of this revolutionary class was from the vir tual beginning the “idea” of the women's movement, and the ten dency for popular discussion of the movement still to center around day care centers is yet another instance of that studied resistance to the pos sibility of political ideas which char acterizes national life.

        “The new feminism is not just the revival of a serious political move ment for social equality,” the femin ist theorist Shulamith Firestone an nounced flatly in 1970. “It is the sec ond wave of the most important revolution in history.” This was scarcely a statement of purpose any one could find cryptic, and it was scarcely the only statement of its kind in the literature of the move ment. Nonetheless, in 1972, in a “special issue” on women, Time was still musing genially that the move ment might well succeed in bringing about “fewer diapers and more Dante.”
        
        Joan Didion is author of “Play It As It Lays,” “Slouching Towards Bethlehem” and “Run River.”
        
        That was a very pretty image, the idle ladies sitting in the gazebo and murmuring lasciate ogni speranza, but it depended entirely upon the popular view of the movement as some kind of collective inchoate yearning for “fulfillment” or “self expression,” a yearning absolutely devoid of ideas and therefore of any but the most pro forma benevolent interest. In fact there was an idea, and the idea was Marxist, and it was precisely to the extent that there was this Marxist idea that the curious historical anomaly known as the women's movement would have seemed to have any interest at all.

        Marxism in this country had ever been an eccentric and quixotic pas sion. One oppressed class after an other had seemed finally to miss the point. The have‐nots, it turned out, aspired mainly to having. The minori ties seemed to promise more, but finally disappointed: it devloped that they actually cared about the issues, that they tended to see the integra tion of the luncheonette and the seat in the front of the bus as real goals, and only rarely as ploys, counters in a larger game. They resisted that es sential inductive leap from the im mediate reform to the social ideal, and, just as disappointingly, they failed to perceive their common cause with other minorities, con tinued to exhibit a self‐interest disconcerting in the extreme to organ izers steeped in the rhetoric of “brotherhood.”

        And then, at that exact dispirited moment when there seemed no one at all willing to play the proletariat, along came the women's movement, and the invention of women as a “class.” One could not help admiring the radical simplicity of this instant transfiguration. The notion that, in the absence of a cooperative prole tariat, a revolutionary class might simply be invented, made up, “named” and so brought into existence, seemed at once so pragmatic and so vision ary, so precisely Emersonian, that it took the breath away, exactly con firmed one's idea of where 19th‐cen tury transcendental instincts crossed with a late reading of Engels and Marx might lead. To read the theo rists of the women's movement was to think not of Mary Wollstonecraft but of Margaret Fuller at her most high‐minded, of rushing position pa pers off to mimeo and drinking tea from paper cups in lieu of eating lunch; of thin raincoats on bitter nights. If the family was the last fortress of capitalism, then let us abolish the family. If the necessity for conventional reproduction of the species seemed unfair to women, then let us transcend, via technology, “the very organization of nature,” the oppression, as Shulamith Fire stone saw it, “that goes back through recorded history to the animal king dom itself.” I accept the universe, Margaret Fuller had finally allowed: Shulamith Firestone did not.

        It seemed very New England, this febrile and cerebral passion. The solemn a priori idealism in the guise of radical materialism somehow be spoke old‐fashioned self‐reliance and prudent sacrifice. The clumsy tor rents of words became a principle, a renunciation of style as unserious. The rhetorical willingness to break eggs became, in practice, only a thrifty capacity for finding the sermon in every stone. Burn the literature, Ti‐Grace Atkinson said in effect when it was suggested that, even come the revolution, there would still be left the whole body of “sexist” Western literature.
        
        But of course no books would be burned: the women of this move ment were perfectly capable of craft ing didactic revisions of whatever apparently intractable material came to hand. “As a parent you should become an interpreter of myths,” advised Letty Cottin Pogrebin in the preview issue of Ms. magazine. “Por tions of any fairy tale or children's story can be salvaged during a cri tique session with your child.” Other literary analysts devised ways to sal vage other books: Isabel Archer in “The Portrait of a Lady” need no longer be the victim of her own idealism. She could be, instead, the victim of a sexist society, a woman who had “internalized the conven tional definition of wife.” The narra tor of Mary McCarthy's “The Com pany She Keeps” could be seen as “enslaved because she persists in looking for her identity in a man.” Similarly, Miss McCarthy's “The Group” could serve to illustrate “what happens to women who have been educated at first‐rate women's colleges—taught philosophy and his tory—and then are consigned to breast‐feeding and gourmet cooking.”

        That fiction has certain irreducible ambiguities seemed never to occur to these women, nor should it have, for fiction is in most ways hostile to ideology. They had invented a class; now they had only to make that class conscious. They seized as a political technique a kind of shared testimony at first called a “rap session,” then called “consciousness‐raising,” and in any case a therapeutically‐oriented American reinterpretation, according to the British feminist Juliet Mitchell, of a Chinese revolutionary practice known as “speaking bitterness.” They purged and regrouped and purged again, worried out one another's er rors and deviations, the “élitism” here, the “careerism” there.
        
        It would have been merely senten tious to call some of their thinking Stalinist: of course it was. It would have been pointless even to speak of whether one considered these women “right” or “wrong,” meaningless to dwell upon the obvious, upon the coarsening of moral imagination to which such social idealism so often leads. To believe in “the greater good” is to operate, necessarily, in a certain ethical suspension. Ask any one committed to Marxist analysis how many angels dance on the head of a pin, and you will be asked in return to never mind the angels, tell me who controls the production of pins.

        To those of us who remained com mitted mainly to the exploration of moral distinctions and ambiguities, the feminist analysis may have seemed a particularly narrow and cracked determinism. Nonetheless it was serious, and for these high strung idealists to find themselves out of the mimeo room and onto the Cavett Show must have been in cer tain ways more unsettling to them than it ever was to the viewers. They were being heard, and yet not really. Attention was finally being paid, and yet that attention was mired in the trivial. Even the brightest movement women found themselves engaged in sullen public colloquies about the in equities of dishwashing and the in tolerable humiliations of being ob served by construction workers on Sixth Avenue. (This grievance was not atypic in that discussion of it seemed always to take on unexplored Ms. Scarlett overtones, suggestions of fragile cultivated flowers being “spoken to,” and therefore violated, by uppity proles.)

        They totted up the pans scoured, the towels picked off the bathroom floor, the loads of laundry done in a lifetime. Cooking a meal could only be “dogwork,” and to claim any pleasure from it was evidence of craven acquiescence in one's own forced labor. Small children could only be odious mechanisms for the spilling and digesting of food, for robbing women of their “freedom.” It was a long way from Simone de Beauvoir's grave and awesome recog nition of woman's role as “the Other” to the notion that the first step in changing that role was Alix Kates Shulman's marriage contract (“wife strips beds, husband remakes them”) reproduced in Ms.; but it was to ward just such trivialization that the women's movement seemed to be heading.

        Of course this litany of trivia was crucial to the movement in the be ginning, a key technique in the politi cizing of women who perhaps had been conditioned to obscure their re sentments even from themselves. Mrs. Shulman's discovery that she had less time than her husband seemed to have was precisely the kind of chord the movement had hoped to strike in all women (the “click of recognition,” as Jane O'Reilly described it), but such dis coveries could be of no use at all one refused to perceive the larger point, failed to make that inductive leap from the personal to the political.

        Splitting up the week into hours during which the children were di rected to address their “personal questions” to either one parent or another might or might not have im proved the quality of Mr. and Mrs. Shulman's marriage, but the improve ment of marriages would not a revo lution make. It could be very useful to call housework, as Lenin did, “the most unproductive, the most barbar ous and the most arduous work a woman can do,” but it could be use ful only as the first step in a political process, only in the “awakening” of a class to its position, useful only as a metaphor to believe, during the late 1960's and early 1970's in the United. States of America, that the words had literal meaning was not only to stall the movement in the personal but to seriously delude one's self.
        
        More and more, as the literature of the movement began to reflect the thinking of women who did not really understand the movement's ideolog ical base, one had the sense of this stall, this delusion, the sense that the drilling of the theorists had struck only some psychic hardpan dense with superstitions and little sophis tries, wish‐fulfillment, self‐loathing and bitter fancies. To read even desultorily in this literature was to recognize instantly a certain dolorous phantasm, an imagined Everywoman with whom the authors seemed to identify all too entirely.

        This ubiquitous construct was everyone's victim but her own. She was persecuted even by her gynecolo gist, who made her beg in vain for contraceptives. She particularly need ed contraceptives because she was raped on every date, raped by her husband, and raped finally on the abortionist's table. During the fashion for shoes with pointed toes, she, like “many women,” had her toes ampu tated. She was so intimidated by cosmetic advertising that she would sleep “huge portions” of her day in order to forestall wrinkling, and when awake she was enslaved by de tergent commercials on television. She sent her child to a nursery school where the little girls huddled in a “doll corner,” and were forcibly re strained from playing with building blocks. Should she work, she was paid “three to ten times less” than an (always) unqualified man holding the same job, was prevented from attend ing business lunches because she would be “embarrassed” to appear in public with a man not her husband, and, when she traveled alone, faced a choice between humiliation in a res taurant and “eating a doughnut” in her hotel room.

        The half‐truths, repeated, authenti cated themselves. The bitter fancies assumed their own logic. To ask the obvious—why she did not get herself another gynecologist, another job, why she did not get out of bed and turn off the television set, or why, the most eccentric detail, she stayed in hotels where only doughnuts could be obtained from room service—was to join this argument at its own spooky level, a level which had only the most tenuous and unfortunate relationship to the actual condition of being a woman. That many women are victims of condescension and ex ploitation and sex‐role stereotyping was scarcely news, but neither was it news that other women are not: nobody forces women to buy the package.

        But of course something other than an objection to being “discrimi nated against” was at work here, something other than an aversion to being “stereotyped” in one's sex role. Increasingly it seemed that the aversion was to adult sexual life itself: how much cleaner to stay forever children. One is constantly struck, in the accounts of les bian relationships which appear from time to time in the move ment literature, by the empha sis on the superior “tenderness” of the relationship, the “gen tleness” of the sexual con nection, as if the participants were wounded birds. The dero gation of assertiveness as “ma chismo” has achieved such currency that one imagines several million women too delicate to deal with a man more overtly sexual than, say, David Cassidy. Just as one had gotten the unintended but in escapable suggestion, when told about the “terror and revul sion” experienced by women in the vicinity of construction sites, of creatures too “tender” for the abrasiveness of daily life, too fragile for the streets, so now one was getting, in the later literature of the move ment, the impression of women too “sensitive” for the difficul ties and ambiguities of adult life, women unequipped for re ality and grasping at the move ment as a rationale for denying that reality.

        The transient stab of dread and loss which accompanies menstruation simply never hap pens: we only thought it hap pened because a male‐chauvin ist psychiatrist told us so. No woman need have bad dreams after an abortion: she has only been told she should. The power of sex is just an oppressive myth, no longer to be feared, because what the sexual con nection really amounts to, we learn in one liberated woman's account of a postmarital affair, is “wisecracking and laughing” and “lying together and then leaping up to play and sing the entire Sesame Street Song book.” All one's actual appre hension of what it is like to be a woman, the irreconcilable dif ference of it—that sense of liv ing one's deepest life under water, that dark involvement with blood and birth and death —could now be declared in valid, unnecessary, one never felt it at all.
        One was only told it, and now one is to be re‐programed, fixed up, rendered again as in violate and unstained as the “modern” little girls in the Tampax advertisements. More and more we have been hear ing the wishful voices of just such perpetual adolescents, the voices of women scarred by re sentment not of their class posi tion as women but at the failure of their childhood expectations and misapprehensions. “Nobody ever so much as mentioned” to Susan Edmiston “that when you say ‘I do,’ what you are doing is not, as you thought, vowing your eternal love, but rather subscribing to a whole system of rights, obligations and responsibilities that may well be anathema to your most cherished beliefs.”

        To Ellen Peck “the birth of children too often means the dissolution of romance, the loss of freedom, the abandonment of ideals to economics.” A young woman described on the cover of a recent issue of New York magazine as “the Suburban Housewife Who Bought the Promises of Women's Lib and Came to the City to Live Them” tells us what promises she bought: “The chance to respond to the bright lights and civiliza tion of the Big Apple, yes. The chance to compete, yes. But most of all, the chance to have some fun. Fun is what's been missing.”

        Eternal love, romance, fun. The Big Apple. These are rela tively rare expectations in the arrangements of consenting adults, although not in those of children, and it wrenches the heart to read about these women in their brave new lives. An ex‐wife and mother of three speaks of her plan “to play out my college girl's dream. I am going to New York to become this famous writer. Or this working writer. Failing that, I will get a job in publishing.” She mentions a friend, another young woman who “had never had any other life than as a daughter or wife or mother” but who is “just discovering herself to be a gifted potter.” The childlike resourcefulness— to get a job in publishing, to be a gifted potter—bewilders the imagination. The astral discon tent with actual lives, actual men, the denial of the real am biguities and the real genera tive or malignant possibilities of adult sexual life, somehow touches beyond words.

        “It is the right of the op pressed to organize around their oppression as they see and de fine it,” the movement theorists insist doggedly in an effort to solve the question of these women, to convince themselves that what is going on is still a political process; but the hand writing is already on the wall. These are converts who want not a revolution but “romance,” who believe not in the oppres sion of women but in their own chances for a new life in ex actly the mold of their old life. In certain ways they tell us sad der things about what the cul ture has done to them than the theorists ever did, and they also tell us, I suspect, that the women's movement is no longer a cause but a sympton ■`
    },
    {
        title:'How Innovative Ideas Arise',
        aurhor:'James Clear',
        article:`In 2010, Thomas Thwaites decided he wanted to build a toaster from scratch. He walked into a shop, purchased the cheapest toaster he could find, and promptly went home and broke it down piece by piece.

        Thwaites had assumed the toaster would be a relatively simple machine. By the time he was finished deconstructing it, however, there were more than 400 components laid out on his floor. The toaster contained over 100 different materials with three of the primary ones being plastic, nickel, and steel.
        
        He decided to create the steel components first. After discovering that iron ore was required to make steel, Thwaites called up an iron mine in his region and asked if they would let him use some for the project.
        
        Surprisingly, they agreed.

        - The Toaster Project
        The victory was short-lived.

        When it came time to create the plastic case for his toaster, Thwaites realized he would need crude oil to make the plastic. This time, he called up BP and asked if they would fly him out to an oil rig and lend him some oil for the project. They immediately refused. It seems oil companies aren't nearly as generous as iron mines.

        Thwaites had to settle for collecting plastic scraps and melting them into the shape of his toaster case. This is not as easy as it sounds. The homemade toaster ended up looking more like a melted cake than a kitchen appliance.

        This pattern continued for the entire span of The Toaster Project. It was nearly impossible to move forward without the help of some previous process. To create the nickel components, for example, he had to resort to melting old coins. He would later say, “I realized that if you started absolutely from scratch you could easily spend your life making a toaster.”
        
        - Don't Start From Scratch
        Starting from scratch is usually a bad idea.

        Too often, we assume innovative ideas and meaningful changes require a blank slate. When business projects fail, we say things like, “Let's go back to the drawing board.” When we consider the habits we would like to change, we think, “I just need a fresh start.” However, creative progress is rarely the result of throwing out all previous ideas and innovations and completely re-imagining of the world.

        Consider an example from nature:

        Some experts believe the feathers of birds evolved from reptilian scales. Through the forces of evolution, scales gradually became small feathers, which were used for warmth and insulation at first. Eventually, these small fluffs developed into larger feathers capable of flight.

        There wasn't a magical moment when the animal kingdom said, “Let's start from scratch and create an animal that can fly.” The development of flying birds was a gradual process of iterating and expanding upon ideas that already worked.

        The process of human flight followed a similar path. We typically credit Orville and Wilbur Wright as the inventors of modern flight. However, we seldom discuss the aviation pioneers who preceded them like Otto Lilienthal, Samuel Langley, and Octave Chanute. The Wright brothers learned from and built upon the work of these people during their quest to create the world's first flying machine.

        The most creative innovations are often new combinations of old ideas. Innovative thinkers don't create, they connect. Furthermore, the most effective way to make progress is usually by making 1 percent improvements to what already works rather than breaking down the whole system and starting over.

        Iterate, Don't Originate
        
        The Toaster Project is an example of how we often fail to notice the complexity of our modern world. When you buy a toaster, you don't think about everything that has to happen before it appears in the store. You aren't aware of the iron being carved out of the mountain or the oil being drawn up from the earth.

        We are mostly blind to the remarkable interconnectedness of things. This is important to understand because in a complex world it is hard to see which forces are working for you as well as which forces are working against you. Similar to buying a toaster, we tend to focus on the final product and fail to recognize the many processes leading up to it.

        When you are dealing with a complex problem, it is usually better to build upon what already works. Any idea that is currently working has passed a lot of tests. Old ideas are a secret weapon because they have already managed to survive in a complex world.

        Iterate, don't originate.`
    },
    {
        title:'Masters of Love',
        author:'Emily Esfahani Smith',
        article:`Science says lasting relationships come down to—you guessed it—kindness and generosity.

        Every day in June, the most popular wedding month of the year, about 13,000 American couples will say “I do,” committing to a lifelong relationship that will be full of friendship, joy, and love that will carry them forward to their final days on this earth.

        Except, of course, it doesn’t work out that way for most people. The majority of marriages fail, either ending in divorce and separation or devolving into bitterness and dysfunction. Of all the people who get married, only three in 10 marriages remain healthy and happy, as the psychologist Ty Tashiro points out in his book The Science of Happily Ever After, which was published earlier this year.

        Social scientists first started studying marriages by observing them in action in the 1970s in response to a crisis: Married couples were divorcing at unprecedented rates. Worried about the impact these divorces would have on the children of the broken marriages, psychologists decided to cast their scientific net on couples, bringing them into the lab to observe them and determine what the ingredients of a healthy, lasting relationship were. Was each unhappy family unhappy in its own way, as Tolstoy claimed, or did the miserable marriages all share something toxic in common?

        The psychologist John Gottman was one of those researchers. For the past four decades, he has studied thousands of couples in a quest to figure out what makes relationships work. I recently had the chance to interview Gottman and his wife, Julie, also a psychologist, in New York City. Together, the renowned experts on marital stability run the Gottman Institute, which is devoted to helping couples build and maintain loving, healthy relationships based on scientific studies.

        John Gottman began gathering his most crucial findings in 1986, when he set up the “Love Lab” with his colleague Robert Levenson at the University of Washington. Gottman and Levenson brought newlyweds into the lab and watched them interact with each other. With a team of researchers, they hooked the couples up to electrodes and asked the couples to speak about their relationship, including details such as how they met, a major conflict they were facing together, and a positive memory they had. As they spoke, the electrodes measured the subjects’ blood flow, heart rates, and how much sweat they produced. Then the researchers sent the couples home and followed up with them six years later to see if they were still together.

        From the data they gathered, Gottman separated the couples into two major groups: the masters and the disasters. The masters were still happily together after six years. The disasters had either broken up or were chronically unhappy in their marriages. When the researchers analyzed the data they gathered on the couples, they saw clear differences between the masters and disasters. The disasters looked calm during the interviews, but their physiology, measured by the electrodes, told a different story. Their heart rates were quick, their sweat glands were active, and their blood flow was fast. Following thousands of couples longitudinally, Gottman found that the more physiologically active the couples were in the lab, the quicker their relationships deteriorated over time.

        But what does physiology have to do with anything? The problem was that the disasters showed all the signs of arousal—of being in fight-or-flight mode—in their relationships. Having a conversation sitting next to their spouse was, to their bodies, like facing off with a saber-toothed tiger. Even when they were talking about pleasant or mundane facets of their relationships, they were prepared to attack and be attacked. This sent their heart rates soaring and made them more aggressive toward each other. For example, each member of a couple could be talking about how their days had gone, and a highly aroused husband might say to his wife, “Why don’t you start talking about your day. It won’t take you very long.”

        The masters, by contrast, showed low physiological arousal. They felt calm and connected together, which translated into warm and affectionate behavior, even when they fought. It’s not that the masters had, by default, a better physiological makeup than the disasters; it’s that masters had created a climate of trust and intimacy that made both of them more emotionally and thus physically comfortable.

        Gottman wanted to know more about how the masters created that culture of love and intimacy, and how the disasters squashed it. In a follow-up study in 1990, he designed a lab on the University of Washington campus to look like a beautiful bed-and-breakfast retreat. He invited 130 newlywed couples to spend the day at this retreat and watched them as they did what couples normally do on vacation: cook, clean, listen to music, eat, chat, and hang out. And Gottman made a crucial discovery in this study—one that gets at the heart of why some relationships thrive while others languish.

        Throughout the day, partners would make requests for connection, what Gottman calls “bids.” For example, say that the husband is a bird enthusiast and notices a goldfinch fly across the yard. He might say to his wife, “Look at that beautiful bird outside!” He’s not just commenting on the bird here: He’s requesting a response from his wife—a sign of interest or support—hoping they’ll connect, however momentarily, over the bird.

        The wife now has a choice. She can respond by either “turning toward” or “turning away” from her husband, as Gottman puts it. Though the bird-bid might seem minor and silly, it can actually reveal a lot about the health of the relationship. The husband thought the bird was important enough to bring it up in conversation and the question is whether his wife recognizes and respects that.

        People who turned toward their partners in the study responded by engaging the bidder, showing interest and support in the bid. Those who didn’t—those who turned away—would not respond or respond minimally and continue doing whatever they were doing, like watching TV or reading the paper. Sometimes they would respond with overt hostility, saying something like, “Stop interrupting me, I’m reading.”

        These bidding interactions had profound effects on marital well-being. Couples who had divorced after a six-year follow-up had “turn-toward bids” 33 percent of the time. Only three in 10 of their bids for emotional connection were met with intimacy. The couples who were still together after six years had “turn-toward bids” 87 percent of the time. Nine times out of 10, they were meeting their partner’s emotional needs.

        * * *

        By observing these types of interactions, Gottman can predict with up to 94 percent certainty whether couples—straight or gay, rich or poor, childless or not—will be broken up, together and unhappy, or together and happy several years later. Much of it comes down to the spirit couples bring to the relationship. Do they bring kindness and generosity or contempt, criticism, and hostility?
        “There’s a habit of mind that the masters have,” Gottman explained in an interview, “which is this: They are scanning social environments for things they can appreciate and say ‘thank you’ for. They are building this culture of respect and appreciation very purposefully. Disasters are scanning the social environment for partners’ mistakes.”

        “It’s not just scanning environment,” chimed in Julie Gottman. “It’s scanning the partner for what the partner is doing right or scanning him for what he’s doing wrong and criticizing versus respecting him and expressing appreciation.”

        Contempt, they have found, is the No. 1 factor that tears couples apart. People who are focused on criticizing their partners miss a whopping 50 percent of positive things their partners are doing, and they see negativity when it’s not there. People who give their partner the cold shoulder—deliberately ignoring the partner or responding minimally—damage the relationship by making their partner feel worthless and invisible, as if they’re not there, not valued. And people who treat their partners with contempt and criticize them kill not only the love in the relationship but also their partner’s ability to fight off viruses and cancers. Being mean is the death knell of relationships.

        Kindness, on the other hand, glues couples together. Research independent from theirs has shown that kindness (along with emotional stability) is the most important predictor of satisfaction and stability in a marriage. Kindness makes each partner feel cared for, understood, and validated—loved. “My bounty is as boundless as the sea,” says Shakespeare’s Juliet. “My love as deep; the more I give to thee, / The more I have, for both are infinite.” That’s how kindness works too: A great deal of evidence shows that the more someone receives or witnesses kindness, the more they will be kind themselves, which leads to upward spirals of love and generosity in a relationship.

        There are two ways to think about kindness. You can think about it as a fixed trait: Either you have it or you don’t. Or you could think of kindness as a muscle. In some people, that muscle is naturally stronger than in others, but it can grow stronger in everyone with exercise. Masters tend to think about kindness as a muscle. They know that they have to exercise it to keep it in shape. They know, in other words, that a good relationship requires sustained hard work.

        “If your partner expresses a need,” explained Julie Gottman, “and you are tired, stressed, or distracted, then the generous spirit comes in when a partner makes a bid, and you still turn toward your partner.”
        In that moment, the easy response may be to turn away from your partner and focus on your iPad or your book or the television, to mumble “Uh-huh” and move on with your life, but neglecting small moments of emotional connection will slowly wear away at your relationship. Neglect creates distance between partners and breeds resentment in the one who is being ignored.

        The hardest time to practice kindness is, of course, during a fight—but this is also the most important time to be kind. Letting contempt and aggression spiral out of control during a conflict can inflict irrevocable damage on a relationship.

        “Kindness doesn’t mean that we don’t express our anger,” Julie Gottman explained, “but the kindness informs how we choose to express the anger. You can throw spears at your partner. Or you can explain why you’re hurt and angry, and that’s the kinder path.”

        John Gottman elaborated on those spears: “Disasters will say things differently in a fight. Disasters will say ‘You’re late. What’s wrong with you? You’re just like your mom.’ Masters will say ‘I feel bad for picking on you about your lateness, and I know it’s not your fault, but it’s really annoying that you’re late again.’”
        * * *

        For the hundreds of thousands of couples getting married this month—and for the millions of couples currently together, married or not—the lesson from the research is clear: If you want to have a stable, healthy relationship, exercise kindness early and often.

        When people think about practicing kindness, they often think about small acts of generosity, such as buying each other little gifts or giving one another back rubs every now and then. While those are great examples of generosity, kindness can also be built into the very backbone of a relationship through the way partners interact with each other on a day-to-day basis, whether or not there are back rubs and chocolates involved.

        One way to practice kindness is by being generous about your partner’s intentions. From the research of the Gottmans, we know that disasters see negativity in their relationship even when it is not there. An angry wife may assume, for example, that when her husband left the toilet seat up, he was deliberately trying to annoy her. But he may have just absent-mindedly forgotten to put the seat down.

        Or say a wife is running late to dinner (again), and the husband assumes that she doesn’t value him enough to show up to their date on time after he took the trouble to make a reservation and leave work early so that they could spend a romantic evening together. But it turns out that the wife was running late because she stopped by a store to pick him up a gift for their special night out. Imagine her joining him for dinner, excited to deliver her gift, only to realize that he’s in a sour mood because he misinterpreted what was motivating her behavior. The ability to interpret your partner’s actions and intentions charitably can soften the sharp edge of conflict.

        “Even in relationships where people are frustrated, it’s almost always the case that there are positive things going on and people trying to do the right thing,” Tashiro, the psychologist, told me. “A lot of times, a partner is trying to do the right thing even if it’s executed poorly. So appreciate the intent.”

        Another powerful kindness strategy revolves around shared joy. One of the telltale signs of the disaster couples Gottman studied was their inability to connect over each other’s good news. When one person in the relationship shared the good news of, say, a promotion at work with excitement, the other would respond with wooden disinterest by checking his watch or shutting the conversation down with a comment like “That’s nice.”

        We’ve all heard that partners should be there for each other when the going gets rough. But research shows that being there for each other when things go right is actually more important for relationship quality. How someone responds to a partner’s good news can have dramatic consequences for the relationship.

        In one study from 2006, the psychological researcher Shelly Gable and her colleagues brought young-adult couples into the lab to discuss recent positive events from their lives. They psychologists wanted to know how partners would respond to each other’s good news. They found that, in general, couples responded to each other’s good news in four different ways that they called passive destructive, active destructive, passive constructive, and active constructive.

        Let’s say that one partner had recently received the excellent news that she got into medical school. She would say something like “I got into my top-choice med school!”

        If her partner responded in a passive destructive manner, he would ignore the event. For example, he might say something like “You wouldn’t believe the great news I got yesterday! I won a free T-shirt!”

        If her partner responded in a passive constructive way, he would acknowledge the good news, but in a half-hearted, understated way. A typical passive-constructive response is saying “That’s great, babe” as he texts his buddy on his phone.

        In the third kind of response, active destructive, the partner would diminish the good news his partner just got: “Are you sure you can handle all the studying? And what about the cost? Med school is so expensive!”

        Finally, there’s active constructive responding. If her partner responded in this way, he stopped what he was doing and engaged wholeheartedly with her: “That’s great! Congratulations! When did you find out? Did they call you? What classes will you take first semester?”

        Among the four response styles, active-constructive responding is the kindest. While the other response styles are joy killers, active-constructive responding allows the partner to savor her joy and gives the couple an opportunity to bond over the good news. In the parlance of the Gottmans, active-constructive responding is a way of “turning toward” your partner’s bid (sharing the good news) rather than “turning away” from it.

        Active-constructive responding is crucial for healthy relationships. In the 2006 study, Gable and her colleagues followed up with the couples two months later to see if they were still together. The psychologists found that the only difference between the couples who were together and those who broke up was active-constructive responding. Those who showed genuine interest in their partner’s joys were more likely to be together. In an earlier study, Gable found that active-constructive responding was also associated with higher relationship quality and more intimacy between partners.

        There are many reasons why relationships fail, but if you look at what drives the deterioration of many relationships, it’s often a breakdown of kindness. As the normal stresses of a life together pile up—with children, careers, friends, in-laws, and other distractions crowding out the time for romance and intimacy—couples may put less effort into their relationship and let the petty grievances they hold against each other tear them apart. In most marriages, levels of satisfaction drop dramatically within the first few years together. But among couples who not only endure but live happily together for years and years, the spirit of kindness and generosity guides them forward.`
    },
    {
        title:'WE ARE ALL CONFIDENT IDIOTS',
        author:'David Dunning',
        article:`The trouble with ignorance is that it feels so much like expertise. A leading researcher on the psychology of human wrongness sets us straight.
        ast March, during the enormous South by Southwest music festival in Austin, Texas, the late-night talk show Jimmy Kimmel Live! sent a camera crew out into the streets to catch hipsters bluffing. "People who go to music festivals pride themselves on knowing who the next acts are," Kimmel said to his studio audience, "even if they don't actually know who the new acts are." So the host had his crew ask festival-goers for their thoughts about bands that don't exist.

        “The big buzz on the street,” said one of Kimmel’s interviewers to a man wearing thick-framed glasses and a whimsical T-shirt, “is Contact Dermatitis. Do you think he has what it takes to really make it to the big time?”

        “Absolutely,” came the dazed fan’s reply.

        The prank was an installment of Kimmel’s recurring “Lie Witness News” feature, which involves asking pedestrians a variety of questions with false premises. In another episode, Kimmel’s crew asked people on Hollywood Boulevard whether they thought the 2014 film Godzilla was insensitive to survivors of the 1954 giant lizard attack on Tokyo; in a third, they asked whether Bill Clinton gets enough credit for ending the Korean War, and whether his appearance as a judge on America’s Got Talent would damage his legacy. “No,” said one woman to this last question. “It will make him even more popular.”
        One can’t help but feel for the people who fall into Kimmel’s trap. Some appear willing to say just about anything on camera to hide their cluelessness about the subject at hand (which, of course, has the opposite effect). Others seem eager to please, not wanting to let the interviewer down by giving the most boringly appropriate response: I don’t know. But for some of these interviewees, the trap may be an even deeper one. The most confident-sounding respondents often seem to think they do have some clue—as if there is some fact, some memory, or some intuition that assures them their answer is reasonable.

        At one point during South by Southwest, Kimmel’s crew approached a poised young woman with brown hair. “What have you heard about Tonya and the Hardings?” the interviewer asked. “Have you heard they’re kind of hard-hitting?” Failing to pick up on this verbal wink, the woman launched into an elaborate response about the fictitious band. “Yeah, a lot of men have been talking about them, saying they’re really impressed,” she replied. “They’re usually not fans of female groups, but they’re really making a statement.” From some mental gossamer, she was able to spin an authoritative review of Tonya and the Hardings incorporating certain detailed facts: that they’re real; that they’re female (never mind that, say, Marilyn Manson and Alice Cooper aren’t); and that they’re a tough, boundary-breaking group.
        
        IN MANY CASES, INCOMPETENCE DOES NOT LEAVE PEOPLE DISORIENTED, PERPLEXED, OR CAUTIOUS. INSTEAD, THE INCOMPETENT ARE OFTEN BLESSED WITH AN INAPPROPRIATE CONFIDENCE, BUOYED BY SOMETHING THAT FEELS TO THEM LIKE KNOWLEDGE.
        
        To be sure, Kimmel’s producers must cherry-pick the most laughable interviews to put the air. But late-night TV is not the only place where one can catch people extemporizing on topics they know nothing about. In the more solemn confines of a research lab at Cornell University, the psychologists Stav Atir, Emily Rosenzweig, and I carry out ongoing research that amounts to a carefully controlled, less flamboyant version of Jimmy Kimmel’s bit. In our work, we ask survey respondents if they are familiar with certain technical concepts from physics, biology, politics, and geography. A fair number claim familiarity with genuine terms like centripetal force and photon. But interestingly, they also claim some familiarity with concepts that are entirely made up, such as the plates of parallax, ultra-lipid, and cholarine. In one study, roughly 90 percent claimed some knowledge of at least one of the nine fictitious concepts we asked them about. In fact, the more well versed respondents considered themselves in a general topic, the more familiarity they claimed with the meaningless terms associated with it in the survey.

        It’s odd to see people who claim political expertise assert their knowledge of both Susan Rice (the national security adviser to President Barack Obama) and Michael Merrington (a pleasant-sounding string of syllables). But it’s not that surprising. For more than 20 years, I have researched people’s understanding of their own expertise—formally known as the study of metacognition, the processes by which human beings evaluate and regulate their knowledge, reasoning, and learning—and the results have been consistently sobering, occasionally comical, and never dull.

        The American author and aphorist William Feather once wrote that being educated means “being able to differentiate between what you know and what you don’t.” As it turns out, this simple ideal is extremely hard to achieve. Although what we know is often perceptible to us, even the broad outlines of what we don’t know are all too often completely invisible. To a great degree, we fail to recognize the frequency and scope of our ignorance.

        In 1999, in the Journal of Personality and Social Psychology, my then graduate student Justin Kruger and I published a paper that documented how, in many areas of life, incompetent people do not recognize—scratch that, cannot recognize—just how incompetent they are, a phenomenon that has come to be known as the Dunning-Kruger effect. Logic itself almost demands this lack of self-insight: For poor performers to recognize their ineptitude would require them to possess the very expertise they lack. To know how skilled or unskilled you are at using the rules of grammar, for instance, you must have a good working knowledge of those rules, an impossibility among the incompetent. Poor performers—and we are all poor performers at some things—fail to see the flaws in their thinking or the answers they lack.

        What’s curious is that, in many cases, incompetence does not leave people disoriented, perplexed, or cautious. Instead, the incompetent are often blessed with an inappropriate confidence, buoyed by something that feels to them like knowledge.

        This isn’t just an armchair theory. A whole battery of studies conducted by myself and others have confirmed that people who don’t know much about a given set of cognitive, technical, or social skills tend to grossly overestimate their prowess and performance, whether it’s grammar, emotional intelligence, logical reasoning, firearm care and safety, debating, or financial knowledge. College students who hand in exams that will earn them Ds and Fs tend to think their efforts will be worthy of far higher grades; low-performing chess players, bridge players, and medical students, and elderly people applying for a renewed driver’s license, similarly overestimate their competence by a long shot.
        
        Occasionally, one can even see this tendency at work in the broad movements of history. Among its many causes, the 2008 financial meltdown was precipitated by the collapse of an epic housing bubble stoked by the machinations of financiers and the ignorance of consumers. And recent research suggests that many Americans’ financial ignorance is of the inappropriately confident variety. In 2012, the National Financial Capability Study, conducted by the Financial Industry Regulatory Authority (with the U.S. Treasury), asked roughly 25,000 respondents to rate their own financial knowledge, and then went on to measure their actual financial literacy
        
        The roughly 800 respondents who said they had filed bankruptcy within the previous two years performed fairly dismally on the test—in the 37th percentile, on average. But they rated their overall financial knowledge more, not less, positively than other respondents did. The difference was slight, but it was beyond a statistical doubt: 23 percent of the recently bankrupted respondents gave themselves the highest possible self-rating; among the rest, only 13 percent did so. Why the self-confidence? Like Jimmy Kimmel’s victims, bankrupted respondents were particularly allergic to saying “I don’t know.” Pointedly, when getting a question wrong, they were 67 percent more likely to endorse a falsehood than their peers were. Thus, with a head full of “knowledge,” they considered their financial literacy to be just fine.

        Because it’s so easy to judge the idiocy of others, it may be sorely tempting to think this doesn’t apply to you. But the problem of unrecognized ignorance is one that visits us all. And over the years, I’ve become convinced of one key, overarching fact about the ignorant mind. One should not think of it as uninformed. Rather, one should think of it as misinformed.

        An ignorant mind is precisely not a spotless, empty vessel, but one that’s filled with the clutter of irrelevant or misleading life experiences, theories, facts, intuitions, strategies, algorithms, heuristics, metaphors, and hunches that regrettably have the look and feel of useful and accurate knowledge. This clutter is an unfortunate by-product of one of our greatest strengths as a species. We are unbridled pattern recognizers and profligate theorizers. Often, our theories are good enough to get us through the day, or at least to an age when we can procreate. But our genius for creative storytelling, combined with our inability to detect our own ignorance, can sometimes lead to situations that are embarrassing, unfortunate, or downright dangerous—especially in a technologically advanced, complex democratic society that occasionally invests mistaken popular beliefs with immense destructive power (See: crisis, financial; war, Iraq). As the humorist Josh Billings once put it, “It ain’t what you don’t know that gets you into trouble. It’s what you know for sure that just ain’t so.” (Ironically, one thing many people “know” about this quote is that it was first uttered by Mark Twain or Will Rogers—which just ain’t so.)

        Because of the way we are built, and because of the way we learn from our environment, we are all engines of misbelief. And the better we understand how our wonderful yet kludge-ridden, Rube Goldberg engine works, the better we—as individuals and as a society—can harness it to navigate toward a more objective understanding of the truth.

        BORN WRONG

        Some of our deepest intuitions about the world go all the way back to our cradles. Before their second birthday, babies know that two solid objects cannot co-exist in the same space. They know that objects continue to exist when out of sight, and fall if left unsupported. They know that people can get up and move around as autonomous beings, but that the computer sitting on the desk cannot. But not all of our earliest intuitions are so sound.
        
        Very young children also carry misbeliefs that they will harbor, to some degree, for the rest of their lives. Their thinking, for example, is marked by a strong tendency to falsely ascribe intentions, functions, and purposes to organisms. In a child’s mind, the most important biological aspect of a living thing is the role it plays in the realm of all life. Asked why tigers exist, children will emphasize that they were “made for being in a zoo.” Asked why trees produce oxygen, children say they do so to allow animals to breathe.
        
        Any conventional biology or natural science education will attempt to curb this propensity for purpose-driven reasoning. But it never really leaves us. Adults with little formal education show a similar bias. And, when rushed, even professional scientists start making purpose-driven mistakes. The Boston University psychologist Deborah Kelemen and some colleagues demonstrated this in a study that involved asking 80 scientists—people with university jobs in geoscience, chemistry, and physics—to evaluate 100 different statements about “why things happen” in the natural world as true or false. Sprinkled among the explanations were false purpose-driven ones, such as “Moss forms around rocks in order to stop soil erosion” and “The Earth has an ozone layer in order to protect it from UV light.” Study participants were allowed either to work through the task at their own speed, or given only 3.2 seconds to respond to each item. Rushing the scientists caused them to double their endorsements of false purpose-driven explanations, from 15 to 29 percent.

        This purpose-driven misconception wreaks particular havoc on attempts to teach one of the most important concepts in modern science: evolutionary theory. Even laypeople who endorse the theory often believe a false version of it. They ascribe a level of agency and organization to evolution that is just not there. If you ask many laypeople their understanding of why, say, cheetahs can run so fast, they will explain it’s because the cats surmised, almost as a group, that they could catch more prey if they could just run faster, and so they acquired the attribute and passed it along to their cubs. Evolution, in this view, is essentially a game of species-level strategy.

        This idea of evolution misses the essential role played by individual differences and competition between members of a species in response to environmental pressures: Individual cheetahs who can run faster catch more prey, live longer, and reproduce more successfully; slower cheetahs lose out, and die out—leaving the species to drift toward becoming faster overall. Evolution is the result of random differences and natural selection, not agency or choice.

        But belief in the “agency” model of evolution is hard to beat back. While educating people about evolution can indeed lead them from being uninformed to being well informed, in some stubborn instances it also moves them into the confidently misinformed category. In 2014, Tony Yates and Edmund Marek published a study that tracked the effect of high school biology classes on 536 Oklahoma high school students’ understanding of evolutionary theory. The students were rigorously quizzed on their knowledge of evolution before taking introductory biology, and then again just afterward. Not surprisingly, the students’ confidence in their knowledge of evolutionary theory shot up after instruction, and they endorsed a greater number of accurate statements. So far, so good.

        The trouble is that the number of misconceptions the group endorsed also shot up. For example, instruction caused the percentage of students strongly agreeing with the true statement “Evolution cannot cause an organism’s traits to change during its lifetime” to rise from 17 to 20 percent—but it also caused those strongly disagreeing to rise from 16 to 19 percent. In response to the likewise true statement “Variation among individuals is important for evolution to occur,” exposure to instruction produced an increase in strong agreement from 11 to 22 percent, but strong disagreement also rose from nine to 12 percent. Tellingly, the only response that uniformly went down after instruction was “I don’t know.”

        And it’s not just evolution that bedevils students. Again and again, research has found that conventional educational practices largely fail to eradicate a number of our cradle-born misbeliefs. Education fails to correct people who believe that vision is made possible only because the eye emits some energy or substance into the environment. It fails to correct common intuitions about the trajectory of falling objects. And it fails to disabuse students of the idea that light and heat act under the same laws as material substances. What education often does appear to do, however, is imbue us with confidence in the errors we retain.

        MISAPPLIED RULES

        Imagine that the illustration below represents a curved tube lying horizontally on a table:

        In a study of intuitive physics in 2013, Elanor Williams, Justin Kruger, and I presented people with several variations on this curved-tube image and asked them to identify the trajectory a ball would take (marked A, B, or C in the illustration) after it had traveled through each. Some people got perfect scores, and seemed to know it, being quite confident in their answers. Some people did a bit less well—and, again, seemed to know it, as their confidence was much more muted.

        But something curious started happening as we began to look at the people who did extremely badly on our little quiz. By now, you may be able to predict it: These people expressed more, not less, confidence in their performance. In fact, people who got none of the items right often expressed confidence that matched that of the top performers. Indeed, this study produced the most dramatic example of the Dunning-Kruger effect we had ever seen: When looking only at the confidence of people getting 100 percent versus zero percent right, it was often impossible to tell who was in which group.
        
        Why? Because both groups “knew something.” They knew there was a rigorous, consistent rule that a person should follow to predict the balls’ trajectories. One group knew the right Newtonian principle: that the ball would continue in the direction it was going the instant it left the tube—Path B. Freed of the tube’s constraint, it would just go straight.

        People who got every item wrong typically answered that the ball would follow Path A. Essentially, their rule was that the tube would impart some curving impetus to the trajectory of the ball, which it would continue to follow upon its exit. This answer is demonstrably incorrect—but a plurality of people endorse it.

        These people are in good company. In 1500 A.D., Path A would have been the accepted answer among sophisticates with an interest in physics. Both Leonardo da Vinci and French philosopher Jean Buridan endorsed it. And it does make some sense. A theory of curved impetus would explain common, everyday puzzles, such as why wheels continue to rotate even after someone stops pushing the cart, or why the planets continue their tight and regular orbits around the sun. With those problems “explained,” it’s an easy step to transfer this explanation to other problems like those involving tubes.

        What this study illustrates is another general way—in addition to our cradle-born errors—in which humans frequently generate misbeliefs: We import knowledge from appropriate settings into ones where it is inappropriate.

        Here’s another example: According to Pauline Kim, a professor at Washington University Law School, people tend to make inferences about the law based on what they know about more informal social norms. This frequently leads them to misunderstand their rights—and in areas like employment law, to wildly overestimate them. In 1997, Kim presented roughly 300 residents of Buffalo, New York, with a series of morally abhorrent workplace scenarios—for example, an employee is fired for reporting that a co-worker has been stealing from the company—that were nonetheless legal under the state’s “at-will” employment regime. Eighty to 90 percent of the Buffalonians incorrectly identified each of these distasteful scenarios as illegal, revealing how little they understood about how much freedom employers actually enjoy to fire employees. (Why does this matter? Legal scholars had long defended “at-will” employment rules on the grounds that employees consent to them in droves without seeking better terms of employment. What Kim showed was that employees seldom understand what they’re consenting to.)

        Doctors, too, are quite familiar with the problem of inappropriately transferred knowledge in their dealings with patients. Often, it’s not the medical condition itself that a physician needs to defeat as much as patient misconceptions that protect it. Elderly patients, for example, frequently refuse to follow a doctor’s advice to exercise to alleviate pain—one of the most effective strategies available—because the physical soreness and discomfort they feel when they exercise is something they associate with injury and deterioration. Research by the behavioral economist Sendhil Mullainathan has found that mothers in India often withhold water from infants with diarrhea because they mistakenly conceive of their children as leaky buckets—rather than as increasingly dehydrated creatures in desperate need of water.

        MOTIVATED REASONING
        
        Some of our most stubborn misbeliefs arise not from primitive childlike intuitions or careless category errors, but from the very values and philosophies that define who we are as individuals. Each of us possesses certain foundational beliefs—narratives about the self, ideas about the social order—that essentially cannot be violated: To contradict them would call into question our very self-worth. As such, these views demand fealty from other opinions. And any information that we glean from the world is amended, distorted, diminished, or forgotten in order to make sure that these sacrosanct beliefs remain whole and unharmed.

        THE WAY WE TRADITIONALLY CONCEIVE OF IGNORANCE—AS AN ABSENCE OF KNOWLEDGE—LEADS US TO THINK OF EDUCATION AS ITS NATURAL ANTIDOTE. BUT EDUCATION CAN PRODUCE ILLUSORY CONFIDENCE.
        
        One very commonly held sacrosanct belief, for example, goes something like this: I am a capable, good, and caring person. Any information that contradicts this premise is liable to meet serious mental resistance. Political and ideological beliefs, too, often cross over into the realm of the sacrosanct. The anthropological theory of cultural cognition suggests that people everywhere tend to sort ideologically into cultural worldviews diverging along a couple of axes: They are either individualist (favoring autonomy, freedom, and self-reliance) or communitarian (giving more weight to benefits and costs borne by the entire community); and they are either hierarchist (favoring the distribution of social duties and resources along a fixed ranking of status) or egalitarian (dismissing the very idea of ranking people according to status). According to the theory of cultural cognition, humans process information in a way that not only reflects these organizing principles, but also reinforces them. These ideological anchor points can have a profound and wide-ranging impact on what people believe, and even on what they “know” to be true.
        
        It is perhaps not so surprising to hear that facts, logic, and knowledge can be bent to accord with a person’s subjective worldview; after all, we accuse our political opponents of this kind of “motivated reasoning” all the time. But the extent of this bending can be remarkable. In ongoing work with the political scientist Peter Enns, my lab has found that a person’s politics can warp other sets of logical or factual beliefs so much that they come into direct contradiction with one another. In a survey of roughly 500 Americans conducted in late 2010, we found that over a quarter of liberals (but only six percent of conservatives) endorsed both the statement “President Obama’s policies have already created a strong revival in the economy” and “Statutes and regulations enacted by the previous Republican presidential administration have made a strong economic recovery impossible.” Both statements are pleasing to the liberal eye and honor a liberal ideology, but how can Obama have already created a strong recovery that Republican policies have rendered impossible? Among conservatives, 27 percent (relative to just 10 percent of liberals) agreed both that “President Obama’s rhetorical skills are elegant but are insufficient to influence major international issues” and that “President Obama has not done enough to use his rhetorical skills to effect regime change in Iraq.” But if Obama’s skills are insufficient, why should he be criticized for not using them to influence the Iraqi government?

        Sacrosanct ideological commitments can also drive us to develop quick, intense opinions on topics we know virtually nothing about—topics that, on their face, have nothing to do with ideology. Consider the emerging field of nanotechnology. Nanotech, loosely defined, involves the fabrication of products at the atomic or molecular level that have applications in medicine, energy production, biomaterials, and electronics. Like pretty much any new technology, nanotech carries the promise of great benefit (antibacterial food containers!) and the risk of serious downsides (nano-surveillance technology!).

        In 2006, Daniel Kahan, a professor at Yale Law School, performed a study together with some colleagues on public perceptions of nanotechnology. They found, as other surveys had before, that most people knew little to nothing about the field. They also found that ignorance didn’t stop people from opining about whether nanotechnology’s risks outweighed its benefits.

        When Kahan surveyed uninformed respondents, their opinions were all over the map. But when he gave another group of respondents a very brief, meticulously balanced description of the promises and perils of nanotech, the remarkable gravitational pull of deeply held sacrosanct beliefs became apparent. With just two paragraphs of scant (though accurate) information to go on, people’s views of nanotechnology split markedly—and aligned with their overall worldviews. Hierarchics/individualists found themselves viewing nanotechnology more favorably. Egalitarians/collectivists took the opposite stance, insisting that nanotechnology has more potential for harm than good.

        Why would this be so? Because of underlying beliefs. Hierarchists, who are favorably disposed to people in authority, may respect industry and scientific leaders who trumpet the unproven promise of nanotechnology. Egalitarians, on the other hand, may fear that the new technology could present an advantage that conveys to only a few people. And collectivists might worry that nanotechnology firms will pay insufficient heed to their industry’s effects on the environment and public health. Kahan’s conclusion: If two paragraphs of text are enough to send people on a glide path to polarization, simply giving members of the public more information probably won’t help them arrive at a shared, neutral understanding of the facts; it will just reinforce their biased views.
        
        One might think that opinions about an esoteric technology would be hard to come by. Surely, to know whether nanotech is a boon to humankind or a step toward doomsday would require some sort of knowledge about materials science, engineering, industry structure, regulatory issues, organic chemistry, surface science, semiconductor physics, microfabrication, and molecular biology. Every day, however, people rely on the cognitive clutter in their minds—whether it’s an ideological reflex, a misapplied theory, or a cradle-born intuition—to answer technical, political, and social questions they have little or no direct expertise in. We are never all that far from Tonya and the Hardings.

        SEEING THROUGH THE CLUTTER
       
        Unfortunately for all of us, policies and decisions that are founded on ignorance have a strong tendency, sooner or later, to blow up in one’s face. So how can policymakers, teachers, and the rest of us cut through all the counterfeit knowledge—our own and our neighbors’—that stands in the way of our ability to make truly informed judgments?

        The way we traditionally conceive of ignorance—as an absence of knowledge—leads us to think of education as its natural antidote. But education, even when done skillfully, can produce illusory confidence. Here’s a particularly frightful example: Driver’s education courses, particularly those aimed at handling emergency maneuvers, tend to increase, rather than decrease, accident rates. They do so because training people to handle, say, snow and ice leaves them with the lasting impression that they’re permanent experts on the subject. In fact, their skills usually erode rapidly after they leave the course. And so, months or even decades later, they have confidence but little leftover competence when their wheels begin to spin.

        In cases like this, the most enlightened approach, as proposed by Swedish researcher Nils Petter Gregersen, may be to avoid teaching such skills at all. Instead of training drivers how to negotiate icy conditions, Gregersen suggests, perhaps classes should just convey their inherent danger—they should scare inexperienced students away from driving in winter conditions in the first place, and leave it at that.

        But, of course, guarding people from their own ignorance by sheltering them from the risks of life is seldom an option. Actually getting people to part with their misbeliefs is a far trickier, far more important task. Luckily, a science is emerging, led by such scholars as Stephan Lewandowsky at the University of Bristol and Ullrich Ecker of the University of Western Australia, that could help.

        In the classroom, some of best techniques for disarming misconceptions are essentially variations on the Socratic method. To eliminate the most common misbeliefs, the instructor can open a lesson with them—and then show students the explanatory gaps those misbeliefs leave yawning or the implausible conclusions they lead to. For example, an instructor might start a discussion of evolution by laying out the purpose-driven evolutionary fallacy, prompting the class to question it. (How do species just magically know what advantages they should develop to confer to their offspring? How do they manage to decide to work as a group?) Such an approach can make the correct theory more memorable when it’s unveiled, and can prompt general improvements in analytical skills.
        
        Then, of course, there is the problem of rampant misinformation in places that, unlike classrooms, are hard to control—like the Internet and news media. In these Wild West settings, it’s best not to repeat common misbeliefs at all. Telling people that Barack Obama is not a Muslim fails to change many people’s minds, because they frequently remember everything that was said—except for the crucial qualifier “not.” Rather, to successfully eradicate a misbelief requires not only removing the misbelief, but filling the void left behind (“Obama was baptized in 1988 as a member of the United Church of Christ”). If repeating the misbelief is absolutely necessary, researchers have found it helps to provide clear and repeated warnings that the misbelief is false. I repeat, false.

        The most difficult misconceptions to dispel, of course, are those that reflect sacrosanct beliefs. And the truth is that often these notions can’t be changed. Calling a sacrosanct belief into question calls the entire self into question, and people will actively defend views they hold dear. This kind of threat to a core belief, however, can sometimes be alleviated by giving people the chance to shore up their identity elsewhere. Researchers have found that asking people to describe aspects of themselves that make them proud, or report on values they hold dear, can make any incoming threat seem, well, less threatening.

        For example, in a study conducted by Geoffrey Cohen, David Sherman, and other colleagues, self-described American patriots were more receptive to the claims of a report critical of U.S. foreign policy if, beforehand, they wrote an essay about an important aspect of themselves, such as their creativity, sense of humor, or family, and explained why this aspect was particularly meaningful to them. In a second study, in which pro-choice college students negotiated over what federal abortion policy should look like, participants made more concessions to restrictions on abortion after writing similar self-affirmative essays.

        Sometimes, too, researchers have found that sacrosanct beliefs themselves can be harnessed to persuade a subject to reconsider a set of facts with less prejudice. For example, conservatives tend not to endorse policies that preserve the environment as much as liberals do. But conservatives do care about issues that involve “purity” in thought, deed, and reality. Casting environmental protection as a chance to preserve the purity of the Earth causes conservatives to favor those policies much more, as research by Matthew Feinberg and Robb Willer of Stanford University suggests. In a similar vein, liberals can be persuaded to raise military spending if such a policy is linked to progressive values like fairness and equity beforehand—by, for instance, noting that the military offers recruits a way out of poverty, or that military promotion standards apply equally to all.

        But here is the real challenge: How can we learn to recognize our own ignorance and misbeliefs? To begin with, imagine that you are part of a small group that needs to make a decision about some matter of importance. Behavioral scientists often recommend that small groups appoint someone to serve as a devil’s advocate—a person whose job is to question and criticize the group’s logic. While this approach can prolong group discussions, irritate the group, and be uncomfortable, the decisions that groups ultimately reach are usually more accurate and more solidly grounded than they otherwise would be.

        For individuals, the trick is to be your own devil’s advocate: to think through how your favored conclusions might be misguided; to ask yourself how you might be wrong, or how things might turn out differently from what you expect. It helps to try practicing what the psychologist Charles Lord calls “considering the opposite.” To do this, I often imagine myself in a future in which I have turned out to be wrong in a decision, and then consider what the likeliest path was that led to my failure. And lastly: Seek advice. Other people may have their own misbeliefs, but a discussion can often be sufficient to rid a serious person of his or her most egregious misconceptions.

        CIVICS FOR ENLIGHTENED DUMMIES

        In an edition of “Lie Witness News” last January, Jimmy Kimmel’s cameras decamped to the streets of Los Angeles the day before President Barack Obama was scheduled to give his annual State of the Union address. Interviewees were asked about John Boehner’s nap during the speech and the moment at the end when Obama faked a heart attack. Reviews of the fictitious speech ranged from “awesome” to “powerful” to just “all right.” As usual, the producers had no trouble finding people who were willing to hold forth on events they couldn’t know anything about.

        American comedians like Kimmel and Jay Leno have a long history of lampooning their countrymen’s ignorance, and American scolds have a long history of lamenting it. Every few years, for at least the past century, various groups of serious-minded citizens have conducted studies of civic literacy—asking members of the public about the nation’s history and governance—and held up the results as cause for grave concern over cultural decline and decay. In 1943, after a survey of 7,000 college freshmen found that only six percent could identify the original 13 colonies (with some believing that Abraham Lincoln, “our first president,” “emaciated the slaves”), the New York Times lamented the nation’s “appallingly ignorant” youth. In 2002, after a national test of fourth, eighth, and 12th graders produced similar results, the Weekly Standard pronounced America’s students “dumb as rocks.”

        BECAUSE IT’S SO EASY TO JUDGE THE IDIOCY OF OTHERS, IT MAY BE SORELY TEMPTING TO THINK THIS DOESN’T APPLY TO YOU. BUT THE PROBLEM OF UNRECOGNIZED IGNORANCE IS ONE THAT VISITS US ALL.
        
        In 2008, the Intercollegiate Studies Institute surveyed 2,508 Americans and found that 20 percent of them think the electoral college “trains those aspiring for higher political office” or “was established to supervise the first televised presidential debates.” Alarms were again raised about the decline of civic literacy. Ironically, as Stanford historian Sam Wineburg has written, people who lament America’s worsening ignorance of its own history are themselves often blind to how many before them have made the exact same lament; a look back suggests not a falling off from some baseline of American greatness, but a fairly constant level of clumsiness with the facts.

        The impulse to worry over all these flubbed answers does make a certain amount of sense given that the subject is civics. “The questions that stumped so many students,” lamented Secretary of Education Rod Paige after a 2001 test, “involve the most fundamental concepts of our democracy, our growth as a nation, and our role in the world.” One implicit, shame-faced question seems to be: What would the Founding Fathers think of these benighted descendants?

        But I believe we already know what the Founding Fathers would think. As good citizens of the Enlightenment, they valued recognizing the limits of one’s knowledge at least as much as they valued retaining a bunch of facts. Thomas Jefferson, lamenting the quality of political journalism in his day, once observed that a person who avoided newspapers would be better informed than a daily reader, in that someone “who knows nothing is closer to the truth than he whose mind is filled with falsehoods and errors.” Benjamin Franklin wrote that “a learned blockhead is a greater blockhead than an ignorant one.” Another quote sometimes attributed to Franklin has it that “the doorstep to the temple of wisdom is a knowledge of our own ignorance.”

        The built-in features of our brains, and the life experiences we accumulate, do in fact fill our heads with immense knowledge; what they do not confer is insight into the dimensions of our ignorance. As such, wisdom may not involve facts and formulas so much as the ability to recognize when a limit has been reached. Stumbling through all our cognitive clutter just to recognize a true “I don’t know” may not constitute failure as much as it does an enviable success, a crucial signpost that shows us we are traveling in the right direction toward the truth.`
    },
    {
        title:'Truth is Ruthless',
        author:'Abhishek Chakraborty',
        article:`There is a scene in the movie Sherlock Holmes starring Robert Downey Jr. where Holmes analyses Mary Morstan, Watson’s soon-to-be, at dinner. Even though he hesitates in the beginning, he relents after Mary “insists”.

        Holmes deduces some rather embarrassing details about Mary which makes her throw a drink on his face. Even though Holmes said the truth, it “hurt” Mary’s feelings.
        
        There are two fundamental ethical orientations that guide human behaviour: deontological ethics and consequentialist ethics.
        
        Deontological ethics is an absolutist view of ethical standards. The rule is final. It dictates everything. There are no exceptions. For example, it’s never correct to lie, no matter what the circumstances.
        
        Consequentialist ethics, on the other hand, evaluates actions based on consequences. For example, It is at times acceptable to lie to spare someone’s feelings.
        
        Humans operate under both systems. If your wife asks you if she looks overweight, you will utter “no” without flinching, whatever you actually think. On the other hand, all of us consider it morally wrong (under all circumstances) to make sexual advances on children.
        
        There are however no clear rules about when to operate under which system. This fuzziness creates an interesting dilemma.
        
        Say a professor from a reputed Indian university announces she wants to study the relationship between religion and intelligence. Say she decides to see how being a Hindu or a Muslim or a Christian or a Sikh in India affects our intelligence, would you be able to keep a straight face?
        
        Let’s take a bit more extreme example: if a politician criticises a religion, say Islam, and its growing influence in India, isn’t there a good chance you’ll call him Islamophobic?
        
        But if you think about it, people should have the right to criticise a religion in a free society. They should have the right to do so, and of course their criticisms are themselves open to criticism. Isn’t that the essence of freedom of speech and thought?
        
        In fact, the above examples aren’t completely fictitious.
        
        In 2010, Geert Wilders, a Dutch parliamentarian, was charged with a slew of crimes for criticising Islam and its influence in the Netherlands. When Mr. Wilders sought to call on expert witnesses to show that his concerns weren’t unfounded, the response from the prosecutor’s office was: “It is irrelevant whether Wilders’s witnesses might prove Wilders’s observations to be correct. What’s relevant is that his observations are illegal.”
        
        In 2018, British sociologist Noah Carl was investigated and subsequently dismissed from his position as a Toby Jackman Newton Trust Research Fellow at St Edmund’s College, Cambridge after over 500 academics signed a letter refusing to accept his research on race and intelligence.
        
        A deontological view asserts that it is never justified to suppress the truth. A consequentialist perspective asserts that the truth must at times be altered, fudged, or suppressed to avert bad consequences.
        
        A consequentialist perspective forces you to be “politically correct” which is important in a lot of circumstances but is a fundamental problem when it comes to the pursuit of truth.
        
        Any human endeavour rooted in the pursuit of truth must rely on facts and not feelings.
        
        Legal proceedings are a good example. We do not establish the innocence or guilt of defendants using feelings, do we? Instead we rely on a broad range of available facts to make a case. The threshold for establishing guilt is set purposely high: the cumulative evidence must be beyond a reasonable doubt to convict someone.
        
        It is important to be concerned about feelings, especially when it comes to sharing the truth, but this concern should never prevent us from finding the truth!
        
        For example, if gender or race really affects intelligence, it’s natural to be concerned about longterm adverse consequences if this information is shared publically, but this concern shouldn’t stop somebody from pursuing the answer.
        
        Otherwise it’s a slippery slope. If we prohibit somebody from studying the relationship between gender and intelligence today because we are afraid of “public sentiments”, tomorrow we might prohibit them from studying the relationship between lack of education and religious fanaticism, or the authenticity of palmistry, or the effectiveness of gau mutra (cow urine). Soon all we’ll care about is public sentiment (which loosely translates to: make sure people like us) and have clue about what the truth really is.
        
        While it might have been okay if Holmes would have saved Mary from the truth and didn’t hurt her feelings, it wouldn’t be okay if he didn’t catch a criminal because it would hurt somebody’s feelings. The nickname of Bengali detective Byomkesh Bakshi (created by writer Sharadindu Bandyopadhyay) is Satyanweshi or the pursuer of truth, and this isn’t without reason. It’s their job; their only job.
        
        The pursuit of truth should always be deontological. There’s no room for exceptions or feelings or public sentiment or political correctness in the quest for truth.
        
        Feelings are related to humans; truth is related to the universe. The universe doesn’t care about humans. Truth ignores feelings.
        
        Truth is relentless. Truth is ruthless.`
        
    },
    {
        title:'Want To Make Better Decisions? Do This',
        author:'Darius Foroux',
        article:`Do you ever look back on your decisions and think, “Why I on earth did I do that?”

        We all make bad decisions.

        Buying an SUV that sucks up all your cash
        Starting a relationship without being in love
        Saying yes to a job that you’re not passionate about
        Creating products that no one needs

        Shit happens (the above examples are all about me). But the funny thing is that bad decisions never seem like bad decisions at the moment.

        I’ve been reading about the decision-making process of Warren Buffet and Charlie Munger, two of the most successful investors of all time.

        In Alice Schroeder’s biography of Warren Buffett, I read that Buffet and Munger have a learning strategy that’s based on what you should avoid doing. They identify mistakes and do their best to avoid those mistakes. But as Charlie Munger says:

        “Smart people do dumb things.”

        You can never avoid making a mistake. However, you can do your best to avoid making dumb decisions.

        Plus, by learning from other people’s mistakes, you can make their mistakes your own. You’ll learn faster that way.

        Don’t Overthink.
        Smart people are way too preoccupied with doing the right things. They want to have a perfect life, career, house, business, car, holiday, etc.

        When you put too much pressure on yourself to make the right decisions, you get analysis paralysis.

        I recently spoke to a friend who wanted to make a career move. I asked him to walk me through his thinking process:

        “I like the company I work for, but my job isn’t engaging anymore. I’ve been doing this work for four years. And sure, I’ve been promoted twice, but it’s still the same work. So I’ve been looking at other companies. But what if I go somewhere else and that doesn’t work out? I’ll have to move on quick. And that won’t look good on my resume.”

        I remained silent.

        “Just hearing myself talk leads me to another thing: Overthinking it.”

        We both laughed our asses off. I can do the same with overthinking. And I bet you’ve been there too.

        When you overanalyze every single decision, you become paralyzed. Result? Nothing. Now, that’s a bad outcome!

        That’s how people end up wasting their lives.

        The only way you can stop overthinking is by making yourself aware of your thinking process. When I asked my friend about his thinking, he became aware of how irrational his process was.

        You can’t control the future. So stop thinking about it.

        Do This Instead: Make Small Decisions. Decide Often.
        I recently read Seeking Wisdom by Peter Bevelin. It’s about the way Charlie Munger thinks. One of his decision-making strategies is to avoid mistakes. But that can be interpreted in different ways.

        You can fear decisions altogether because you might make mistakes. What happens is that you don’t make decisions at all. As Munger says:

        “The difference between a good business and a bad business is that good businesses throw up one easy decision after another. The bad businesses throw up painful decisions time after time.”

        You can interpret that Munger quote in different ways. I interpret it as follows:

        When you make small decisions early, before they become big — it’s easy. When you put off decisions, they become big — and painful.

        For example, I’m not happy with the email provider I use to send out my newsletter. Their support is slow, there’s no good integration with my online courses platform, and readers have complained about not getting my newsletter.

        This is something that’s on my radar for more than 1.5 years. At the time my list was less than half of the size it’s today. I also had only one online course. Now, I have three.

        The hassle of moving to another provider gets bigger every day. Had I moved early, it was easy. By now, it’s a painful process.

        In life, it’s exactly the same. The longer you stay in a bad relationship, the harder it gets to leave. It’s also true for your job.

        Earlier Decisions Lead To Better Decisions
        The earlier and more you decide, the more chance that you make better decisions.

        I often say that there are no right or wrong decisions — only decisions. That’s not entirely accurate. Of course, there’s a difference in the quality of our choices. That’s the topic of another article.

        But here’s the thing: NOT making a decision is also a decision. If that’s a conscious move, that’s okay. You think about something, and you decide that doing nothing is the best option.

        However, I’m referring to not making a decision as in “I’ll put it off until another time.”

        No matter what, you’re making decisions all the time. Instead of making fewer conscious decisions, we need to make them earlier.

        Because all you need are a few good choices in your lifetime anyway. What will be your best? You only find out after, you guessed it, you’ve made a decision.`

    },
    {
        title:'Death by Harry Potter',
        author:' Chuck Klosterman',
        article:`Ignoring a cultural phenomenon today may render you completely irrelevant in a few years. Just so you know.

        Here is what I know about Harry Potter: nothing.

        I haven't read any of the books about him, nor have I seen any of the movies. I know the novels were written by a rich middle-aged British woman named J.K. Rowling with semi-lush hair, but I have no idea what the letters J and K represent. I don't know the name of the actor who portrays Harry Potter in the films, although I think he has eyeglasses. I don't know the names of any minor characters and I don't know the narrative arc of the plot. I don't know where the stories take place or if they are set in the past or the future. Somebody at a steakhouse recently told me that Harry Potter doesn't die at the conclusion of the seventh book (and that this detail was important), but I wasn't even aware he was sick. Christopher Hitchens wrote something I didn't read about this series in The New York Times, but I don't think he mentioned Nixon. I assume there are dragons and griffins and werewolves and homosexual Frankensteins throughout these novels, but I honestly don't give a shit if my assumption is true or false. In fact, if somebody told me that the final Harry Potter novel was a coded interpretation of the Koran that instructed its readers how to read my thoughts, I could only respond by saying, "Well, maybe so." For whatever reason, this is one phenomenon that I have missed completely (and mostly, I suppose, on purpose).

        Now, do not take this to mean that I dislike these books. I do not. I have a colleague who feels that anyone over the age of twenty-one caught reading a Harry Potter novel should be executed without trial, but that strikes me as unreasonable; the fact that they're written for British thirteen-year-olds probably means they're precisely the right speed for 90 percent of American adults. I don't hate these novels at all -- in fact, I suspect they're quite good. Moreover, I find it astounding that the unifying cultural currency for modern teenagers are five-hundred-page literary works about a wizard. We are all collectively underestimating how unusual this is. Right now, there is no rock guitarist or film starlet as popular as J.K. Rowling. Over time, these novels (and whatever ideas lie within them) will come to represent the mainstream ethos of our future popular culture. Harry Potter will be the only triviality that most of that coming culture will unilaterally share.

        And I have no interest in any of it.

        And I wonder how much of a problem this is going to become.

        The bookish kids reading Harry Potter novels may not go on to control the world, but they will almost certainly go on to control the mass media. In fifteen years, they will be publishing books and directing films and writing broad jokes for unfunny situation comedies that will undoubtedly be downloaded directly into our brains. And like all generations of artists, they will traffic in their own nostalgia. They will use their shared knowledge and experiences as the foundation for discourse. So I wonder: Because I don't understand Harry Potter, am I doomed to misunderstand everything else?

        I have a female friend who has never seen any of the Star Wars movies; if someone on The Office makes a joke about a Wookiee, she knows that it's supposed to be funny, but it never makes her laugh. I also know a guy from college who (under pressure) cannot name three Beatles songs unless you allow him to include their cover of "Twist and Shout," and that's only because it was used in Ferris Bueller's Day Off. On a practical level, those specific knowledge chasms do not hinder either of their lives; I'm sure some would argue they're better off not caring about such matters. But part of me knows that there's an intangible downside to having complete intellectual detachment from whatever most Americans consider to be common knowledge. It's not just that someone who hasn't seen Star Wars won't appreciate Kevin Smith films or that any person who doesn't know about the Beatles won't appreciate the Apples in Stereo; those connections are obvious (and usually meaningless). What's less clear -- and much more important -- is the degree to which all of culture is imperceptibly defined by whichever of its entities happens to be the most popular at any given time.(1)

        Within any complex scenario, there are three basic kinds of information:

        1) Information that you know you know.

        2) Information that you know you don't know.

        3) Information that you don't know you don't know.

        I'd like to believe that my relationship with Harry Potter fits into that second category; I'd like to view the information in Rowling's books as something I consciously realize that I don't understand. But this is not the case. The phenomenon around these books is so large that I can't isolate the consequence of my unawareness. My relationship to Harry Potter actually falls into the third category: I cannot even pretend to predict what the social impact of 325 million books will eventually embody. As the years pass, the influence of these teenage-wizard stories will be so vast that it will become invisible. In two decades, I will not be alienated or confused by passing references to Harry Potter; very often, I will be unaware that any reference has even been made. I will not know what I am missing. I'll just feel bored, and I won't know why.

        Here is what I imagine the seven Harry Potter novels are about: I imagine that Harry is an orphan who had a bad relationship with his father (kind of like Tom Cruise in Top Gun or Days of Thunder or A Few Good Men or any of his movies that didn't involve Ireland). He escapes some sort of abstract slavery and decides to become a wizard, so he attends Wizard College and meets a bunch of anachronistic magic-using weirdos and perhaps a love interest that he never has sex with. There is probably a good teacher and a bad teacher at this school and (I'm sure) they eventually fight each other, and then some previously theoretical villain tries to destroy the world, and all the wizard kids have to unite and protect the universe by boiling black cats in a cauldron and throwing lightning bolts at pterodactyls. Harry learns about life and loss and leadership, and then he doesn't die. The end.

        Now, I realize I don't have to guess at these details. I'm sure I could read the entire four-thousand-page plot summarized in four hundred words on Wikipedia, or I could simply walk into any high school and ask a few questions of the first kid I find who isn't smoking crystal meth. I could just as easily buy and read the books themselves, which, as stated previously, I assume are engaging. But I am not going to do this. It doesn't seem worth it, even though I know it probably is. It's an interior paradox. I mean, is it my obligation to "study" these novels, even if I don't want to? Perhaps it is. In many ways, I am paid by Esquire to contextualize this sort of phenomenon, and I assume that will still be the case in the future. It is probably to my long-term financial benefit to read Harry Potter books; ignoring them is like not investing in my 401(k). Were I a more responsible citizen, I would force myself to consume everything I could about this goddamn teenage wizard, simply for economic self-preservation. Yet I still cannot make myself do it. At the end of the day (or at the beginning of the day, or whenever), I don't care if I don't understand this.

        Which, I realize, is a dangerous position to publicly adopt. I am constructing my own generation gap on purpose. By making this decision in the present, I will be less able to manage the future. My thoughts about entertainment aesthetics will be outdated, and I will not grasp the fundamental lingua franca of the 2025 hipster. I will not only be old but old for my age. I will be the pterodactyl, and I will be slain. It is only a matter of time.`

    },
{
        title:'Dear Single Women of NYC: It’s Not Them, It’s You.',
        author:'Jen Doll',
        article:`My years of New York City dating—if you’re counting, there have been 12—have involved a lot of guys, short- and long- and mid-term. My longest relationship lasted two years. My shortest—minus the one-off hookups that we all know aren’t “dates” at all—was somewhere in the range of two weeks. There have been certifiable crazies, like the Eastern European fellow who broke my bedroom window in a fit of rage and told me not to complain that he’d broken my “fucking window.” There was the Jersey boy who worked in women’s handbags; fond memories involve him drunk-puking at the Hilton, then giggling hysterically, running, and “hiding” our soiled comforter in front of someone else’s door down the hall. There was the super-successful corporate honcho with a cardboard box for a nightstand. The best friend with whom I had zero sexual attraction. The self-described “bi-coastal but not in a gay way” guy who didn’t come home one night because he’d passed out in a planter underneath the Manhattan Bridge. (We continued to date for at least a month after that.)

        Their ages have ranged from nearly 15 years younger than me to going on 15 years older. There were Peter Pan Syndrome–afflicted man-children, full-fledged adult males with zero desire to grow up, maybe ever. There were drunks and drug addicts and maybe once a teetotaler. There were Christians and atheists and Jews. There was a clammer from Cape Cod—a real, live clammer, with his very own waders. There was a man who shaved everything . . . down there . . . every single day. There was the dashing Argentinean only in town for a week; the Ronkonkoma deli worker barely old enough to drink; the beleaguered i-banker who came over regularly just to pass out on my couch. And I can’t forget the “totally eligible” magazine editor who moved to the suburbs while we were dating, convinced me to take a bus to visit him, showed off his two-story brick house with granite kitchen counters and an actual backyard, as if knowing it was exactly what I aspired to—and then promptly married someone else. There were men who have dropped me on my head, literally and figuratively. I could show you bruises.
        
        At some point, I yelled at almost all of these men for not being “what I wanted,” and, as we all do, turned to my female friends for consolation and support. “He doesn’t deserve you,” they would say, my own Greek chorus. “You’re so much better than him.” Then, inevitably: “Why are New York men such assholes?”
        
        If you’re a single, heterosexual woman of a certain age living in New York City, you’ve surely heard some version of the lament more times than you can count: “There are no good single men living in New York City! They’re all gay or taken!” It’s followed by various tales of woe regarding “typical NYC jerks” and the evils they have inflicted upon amazing, upstanding, attractive, intelligent, high-powered New York City women who are so much better than the men they date.
        
        You’ve probably met more than a few aesthetically, shall we say, “uneven” couples, in which the man is short, pudgy, bald—or distractingly hirsute—with one of those pudding faces only a mother (or gold-digger) could love. He’s impossibly rich, and his lady-friend could model for a living, and possibly does. Also, he cheats on her. Only in New York!
        
        And you’ve probably heard, and maybe retold, the modern-day relationship folk tale of that friend of a friend who, after “unsuccessfully” dating in New York for years, met her amazing husband while living or vacationing in Austin, or Boston, or Paris, or Rio, and then brought him back—or moved there herself. Because, you know, you just can’t find a decent dude in this city. It’s impossible. Those who do it are the exception, not the rule. Ask anyone.
        
        Maybe saying and hearing this makes single women feel better. It enforces the belief that there is such a thing as a “plight” of the single lady, and that women can’t be blamed for our lack of success in the New York City relationship game. It’s them, not us.
        
        The problem is, it’s patently untrue. Worse, it’s a cop-out.
        
        New York City, to be fair, suffers its share of problems for the female dater. There are more women than men, which everyone loves to bemoan as the cold, hard cornerstone of this city’s relationship difficulties. According to statistics collected by Richard Florida, author of The Great Reset and director of the Martin Prosperity Institute at the University of Toronto, single women currently outnumber single men in New York by 149,219. This is based on data from the U.S. Census, which, it bears mentioning, does not ask to identify sexual orientation. The good news: This number has actually decreased from 2008’s woman-surplus of 210,000, a gap that caused Lysandra Ohrstrom, writing for the Observer, to unleash the ominous decree that “savvy, well-educated women hoping to find a mate and settle down are out of luck.”
        
        Meanwhile, our fine city was recently ranked the top spot for single men to find a willing lady to smooch, and whatever else, on New Year’s Eve, according to more numbers from Mr. Florida. We were named number one of 2010’s top 29 cities for dudes to live in: a/k/a “paradise for men,” according to gratuitous macho website AskMen.com. Luisita Lopez Torregrosa, writing in Politics Daily, called the ratio of men to women “scarily in favor of men,” and advised ladies to “go West—San Diego, Dallas, and Seattle. It’s where the boys are.”
        
        As Tamsen Fadal, relationship expert and the female member of “America’s only husband-wife matchmaking team” told us, “New York is like a candy store to men. If they think, ‘This girl’s not giving me what I want, or pushing things too quickly,’ they find someone else. It’s an unlevel playing field.”
        
        Of course, love is inherently not a level playing field—its terrain is rocky, uncharted, completely unfair. The beautiful, the smart, the successful, and the young will attract more than their allotment of admirers, while the ugly, the desperate, the “too old,” and the socially unfit for whatever reason are just not going to have the same dating opportunities. If you’re a die-hard optimist, maybe you believe that there’s someone for everyone, but there are far more somebodies for some, male or female.
        
        If you’re a single man who has moved to New York City, chances are it has to do with being good—even the best—at something. Hence the workaholics, status-aholics, power-aholics, and whatever else ambition breeds. Meanwhile, the streets are plentiful with ever more attractive women. Amid all that, there is a sense of perpetual youth, a staving off of the trappings of adulthood—like “settling down and getting married”—far into our 30s and even 40s because, frankly, we can get away with it. And there’s so much to do! Why get married when you’re having so much fun? As one man admitted, “Guys in New York have unrealistic standards for what their lives should be.”
        
        But it’s hardly fair to say that New York City women haven’t come here for much the same reasons that men have, or that they don’t have similarly unrealistic expectations. “I think there are a couple of different problems in New York,” says Fadal. “People who live in New York are successful in their field or want to be. We’re not someplace where so much of our time is devoted to relationships. We then realize our years sort of went by.”
        
        This is true of all of us, men and women. Yet somehow, helped along by rom-coms and self-help books and chick lit, at some point we learn to ignore the simple fact that there are two people in every relationship, and that they both have a hand in whether it succeeds or fails. And something else: that the success or failure of most relationships can, if we look at them with open eyes, probably be predicted from the very beginning based on some simple indicators.
        
        Take a “concept” like “He’s Just Not That Into You,” which puts blame squarely on the man’s shoulders. How freeing: He is just not that into you! But at what point did we lose the capacity to be as “Just Not That Into You” as the men? If we’re to expect a society in which men and women are truly considered equals, women have to accept their portion of the responsibility, and the blame.
        
        Here’s the deal, women of New York City: The so-called plight of the single lady? It’s not about him. It’s about you.
        
        Some years ago, having lived in New York City since graduating from college, I was visiting my parents for Thanksgiving. An older male neighbor who had been invited to dinner took one look at me across the table and said to my mother, “She’s single? She’s pretty. What’s wrong with her?”
        
        You can probably imagine the indignant response that ensued, in which I (and my mom) defended my choice not to be married and not even be dating anyone at the ripe old age of, say, 26, because it’s New York and that’s how the kids do things there, and plus I’d just broken up with someone, and who are you to tell me I should already be paired off and shuffled down the aisle for a life of tedium and domesticity anyway, old neighbor man?
        
        But, really, the question hit home because there was truth to it. There was (and still is) something wrong with me. And it’s the same thing that’s “wrong” with pretty much every single woman in New York complaining she can’t find a decent man, or who has perhaps even given up in pursuit of her own continued drama and mini-amusements with the kind of guys she’d never want to settle down with anyway (safer that way): We don’t know what we want. And so we want a little bit of everything, over and over again.
        
        Auntie Mame said famously that “Life is a banquet, and most poor bastards are starving to death!” But those poor bastards don’t live in New York City, where the banquet is 24 hours a day and everybody wants a piece of everybody else, if just for a little amuse-bouche. We’re free and “grown up” and independent; we can do what we want, sexually and otherwise. Which is part of the problem, if you’re going to call it that.
        
        When asked what he thought about the “plight of the single lady”—and women who blame men for the state of dating in the city, a single New Yorker in his twenties admitted, “I see where they’re coming from, but, in a lot of ways, they bring it upon themselves. I think if girls were more withholding, boys would be more likely to commit, but because boys can get most of what they want without having to commit, they do. That implies that all boys want is to hook up, which I don’t think is true, but I think that is a lot of it. That’s why when a girl says, ‘Oh, sure, we can hook up and I won’t be weird about it,’ they end up yelling at you a week later.”
        
        For every loser I’ve screamed at, there have been nice, normal single guys with perfectly acceptable ZIP codes and ages and jobs and habits who never did a thing wrong but for some reason were chucked after the first or second, or maybe even third, date for being boring, predictable, too nice, too normal, not successful enough, or . . . admitted to no one, perhaps not even myself: too available. The scariest of scary words.
        
        If you’re like me (and I think a lot of us are), you might say you can’t stand drama and that all you want is a nice, stable relationship with someone who loves and treats you well, but “nice” and “stable” have hardly the appeal of words like “exciting” or “passionate” or, well, “drama.” Our status as single, independent, financially solvent New York City women in the year 2011 has us sitting on a mountain of unprecedented options. Options: Those are exciting. So we want all the options, bigger and better and faster and shinier, or taller or sexier or stronger or smarter, and yet somehow also different and completely our own. We want the tippy-top of what we can get—why shouldn’t we? And we want to push those boundaries.
        
        That, to a large extent, is why we live here. It’s not because we wanted to settle down with the patient and reliable plod-along schmo, and have babies and live in a three-bedroom house with a two-car garage where we peaceably grill in the summer and make casseroles in winter until we die. It’s not because we wanted our lives charted out before we lived them.
        
        My high school boyfriend was probably the best man I’ve ever dated. One time, for no reason whatsoever, he printed out a dictionary definition of “beautiful,” circled the word, drew an arrow to it, and wrote “THIS IS YOU.” He left it for me somewhere I would find it, as a surprise. He told me he loved me. But at the end of high school, when I knew I was going away to bigger, brighter things while he stayed in town and continued at the local community college, I tried to dump him over and over again, eventually making out with a random guy in a band on high school graduation night and telling the would-be ex about it the next day. The ex has a little boy, a dog, and a wife now; I don’t even own a cat. But I have options! I wanted them then; I still want them now.
        
        Yet these never-ending options wreak havoc with us, as does the idea that we can dally with each of them without ever deciding on any and just hope it will all fall where it may—that someday our prince will come, and he better be fucking good. As a married friend mused, “Holding out for everything we want—maybe it’s a delusional expectation. Maybe it’s more about self-reflection, an exercise in goals. It’s more you-centered soul-searching than about the guy, necessarily. In most relationships, there’s a huge, huge focus on timing. A lot of it is just a matter of reaching the point where you’ve figured out what you want.”
        
        Florida, the man behind those male-female NYC dating stats, writes on his website that “one reason ladies in the prime marriage years flock to big cities is to compete for the most eligible men,” and intelligent women who gravitate to “vibrant cities are more likely to stay single—for longer, at least—because they rightly refuse to settle for someone who can’t keep up with them intellectually or otherwise.”
        
        “Rightly refusing to settle,” especially for someone who’s boring, otherwise uninspired, or just a bad choice, sounds pretty good—even empowering. Somewhere along the way, “settling” became a dirty word, evoking visceral reactions of distaste and even disgust, particularly for the strivers among us. Take the negative reactions to Lori Gottlieb’s book Marry Him: The Case for Settling for Mr. Good Enough, which suggests that women who are still single after 35 are just too damn picky.
        
        But I’d argue that it’s not about being picky. It’s about having all of these options, and not knowing how to choose from among them, or whether we even want to. It’s about the years of being told we can have it all, and suddenly being deeply afraid to admit that that house of cards has been a sham all along because no one really gets to have it all. (And so, the self-professed adamantly anti-marriage Elizabeth Gilbert—who ate, prayed, and loved her options into a bestseller and a Julia Roberts movie—ultimately “caved” to marrying her foreign-born partner so that he could live in the U.S.)
        
        Everyone has to make choices. This isn’t to say that if you want a successful career and to be a wife and a mom, you can’t do it. Nor that you can’t do it fairly well. But inevitably, you’ll have to give up one thing for something else. Why should you settle? Because that’s what all humans do when they make choices.
        
        If Carrie Bradshaw were here and an actual person, she would say, “But what about the ‘za-za-zoo’?” And after berating her for that corny terminology, I’d grudgingly agree that, yes, there needs to be something—call it magic, or a spark, or a connection—with regard to our romantic relationships. But the magic pales in comparison to the simplest, and yet most difficult, of things. Knowing what you want. It’s timing, but it’s more than that, because you dictate your own timing. You hold the cards.
        
        If Carrie had wanted marriage and kids back in Season 4, she would have stuck with Aidan. Instead, she got panicked and neurotic and self-destructive and Carrie Bradshaw–esque, and started to have an affair with Big, who was clearly (until the unbelievable ending of the series) never going to marry her. Why do that to yourself? Because you aren’t quite sure you want to get married, either. Because the grass is ever so mysteriously greener in the yard (does he even have a yard?) of the guy who doesn’t want to marry you. And because it makes for good drama, or, at the very least, tragicomedy.
        
        Still, at the end of the movie, or the TV series, everything gets wrapped up neatly and tied with a Tiffany-box bow. In the film version of Breakfast at Tiffany’s, Holly Golightly is eventually tamed by the love of a good man who has been there all along. In Working Girl, the girl gets her career-with-corner-office and Harrison Ford to pack her lunchbox. In The Apartment, Shirley MacLaine’s character attempts suicide on account of Mr. Wrong, but in surviving finds her Mr. Right. Harry and Sally run through the relationship ropes course as enemies, friends, lovers, and enemies again, only to end up an old married couple. As do, of course, Carrie and Big. It all just seems to unfold, without anybody doing too much soul-searching or goal-plotting, much like a movie. A movie set in New York! This is what we’re supposed to want.
        
        People who have been married will tell you that it’s not all butterflies and lying in the grass together clutching hands. It’s actually work—not magic, and not the movies. Which means the dream we expect for ourselves drastically needs to be tempered with a dash of reality, a dose of self-reflection. As a thirtysomething New York woman said, “Ultimately, marriage has more to do with knowing what you’re looking for. Sure, there are a lot of guys out there that suck, but I don’t think that’s a New York–specific issue. There are all of these successful, smart, workaholic women who have their shit together and strong views and senses of who they are. Their expectations are a bit higher. And in New York, there’s not this worry about being the only single person; we all have friends who are married, married with kids, divorced, single.”
        
        Fewer people are getting married than ever. According to a Pew Research poll published at the end of last year, about half of all adults in the U.S. are married, down from 72 percent in 1960. Four in 10 people consider marriage obsolete. At the same time that fewer of us are getting married, more people are doing it for love—93 percent said it was the most important reason to tie the knot. Love is not something that used to factor into marriages; it’s a relatively modern concept. You might say we’re spoiled by even expecting it, and that it’s entirely unrelated to a social “institution” that was really about property and taxes and making sure you had enough kids to work the farm or protect the homestead way back when—not to mention one of the only socially acceptable ways for women to have sex.
        
        But if you confessed to someone today that you’d married without “being in love,” because you’d simply wanted to get married or have the financial foundation to start a family (or buy more shoes), or maybe because you just didn’t want to spend Sundays alone anymore, they would look at you with a horror akin to what you might bestow upon a person admitting to murder.
        
        If there is a real and current plight of the single lady in New York City, it’s not that New York men are so horrible. It’s figuring out how to balance what you want and what you can get—in terms of love, marriage, and what each guy has to offer—against all of the options, including the imminent biological reality of your decreasing fertility. It’s figuring out if you care about your fertility at all, and if you care about it in light of being—or not being—married. Because at some point, it will simply be too late to have kids.
        
        At the same time, if you don’t want children, then maybe you don’t really want a husband, or as one happily unmarried New Yorker explained, “I’d never been really hung up on having kids. It certainly made dating easier, because I didn’t have the same timeline some of my friends did. No urgency. The same holds true now that I am dating someone. Whether we get married or not is almost immaterial since we don’t plan on having kids. Unless, of course, one of us gets hooked up with really good health insurance. Then we’d get married for sure.”
        
        The fertility question is often a tipping point, and definitely “a challenge for women,” says Fadal. “Men here are very motivated, and their career comes first. They’re not under any age restriction, nor do they face the fertility reality. If that weren’t an issue, I think women would keep playing the field, too. I would. But all the technology in the world isn’t going to change that.” Another married New Yorker agreed: “If you could have babies easily into your 50s, I think you’d go on being single forever,” she said. But we can’t. This is just a biological fact.
        
        It’s also a fact that, at least in the non-romantic portions of life, understanding and expressing what you want makes achieving it far easier, whatever the “it” is. Yet, by and large, New York City women fail to be specific with men about what they really want and instead just go along with things hoping for the best and getting angry when it doesn’t work out that way. Or they’re so specific, with such intricately wrought lists of requirements for what they will and won’t date, that they miss the point altogether—if the criteria is that complicated, maybe they don’t actually want to be with someone at all yet.
        
        Perhaps this is changing. I’ve heard of at least two single New York women who have set their own wedding dates for themselves—minus even a potential boyfriend. Say what you will about the “method,” but I think they should be congratulated for having at least acknowledged what they want while so many of us wait aimlessly for a nebulous “Mr. Right” with whom we will fall deeply and madly in love in the kind of fantasy relationship promoted by romantic comedies. When that doesn’t happen, because it can’t happen—it never happens—we blame the men. But ladies, we are so much smarter than that!
        
        There is nothing wrong with taking your time and sampling liberally from the buffet. As Fadal says, “I caution against trying to settle down before you’re ready. Every guy has his purpose. There’s the guy who takes you great places, the guy you’re sleeping with, etc. If you’re enjoying yourself, and if you do it in the right way, there’s nothing wrong with that.”
        
        And so, the wild and crazy kisser who actually broke your front tooth, which then required dental work; the guy who taught you to always ask for Sriracha in your deviled eggs; the man who introduced you to Wolf Parade; the man who introduced you to really good bourbon; the guy with kids who helped you remember why you do, or don’t, want them for yourself; the bisexual co-worker; the “poonhound”; the one that got away; and the one you let get away on purpose—they all have a place in your dating life. Don’t regret them.
        
        Once you know what you want, narrow the options, make your choices, and go for it. But until you do, embrace not knowing. Make New York your playground and stop complaining about how single ladies have it so hard in this city. Along the way, remember that men are not the enemy. Many of them are reasonable and good and not at all the brutes we’ve made them out to be, even if they don’t want to marry us (and some of them do). One recently confessed that he’d like to get married in the next few years because “I don’t want to be 34 and doing that thing that sketchy New York guys do where they go out and act as though they’re 24. I’ve seen too much of it. . . . It’s a real cautionary tale.” When I told him that was refreshing, he said, “I think most guys feel that way.”
        
        The other night, I had drinks with the ex who’d passed out in that planter underneath the Manhattan Bridge. We hadn’t talked in about three months. He bought me two glasses of wine, touched me on the shoulder, and told me I looked “unbelievable.” I knew I could do it all again if I wanted to. Options. Drama. Will I? I’m not narrowing them yet.`
    },
    {
        title:'What the World Will Speak in 2115',
        author:'John H. McWhorter',
        article:`A century from now, expect fewer but simpler languages on every continent.

        In 1880 a Bavarian priest created a language that he hoped the whole world could use. He mixed words from French, German and English and gave his creation the name Volapük, which didn’t do it any favors. Worse, Volapük was hard to use, sprinkled with odd sounds and case endings like Latin.

        It made a splash for a few years but was soon pushed aside by another invented language, Esperanto, which had a lyrical name and was much easier to master. A game learner could pick up its rules of usage in an afternoon.

        But it didn’t matter. By the time Esperanto got out of the gate, another language was already emerging as an international medium: English. Two thousand years ago, English was the unwritten tongue of Iron Age tribes in Denmark. A thousand years after that, it was living in the shadow of French-speaking overlords on a dampish little island. No one then living could have dreamed that English would be spoken today, to some degree, by almost two billion people, on its way to being spoken by every third person on the planet.

        Science fiction often presents us with whole planets that speak a single language, but that fantasy seems more menacing here in real life on this planet we call home—that is, in a world where some worry that English might eradicate every other language. That humans can express themselves in several thousand languages is a delight in countless ways; few would welcome the loss of this variety.

        But the existence of so many languages can also create problems: It isn’t an accident that the Bible’s tale of the Tower of Babel presents multilingualism as a divine curse meant to hinder our understanding. One might even ask: If all humans had always spoken a single language, would anyone wish we were instead separated now by thousands of different ones?

        Thankfully, fears that English will become the world’s only language are premature. Few are so pessimistic as to suppose that there will not continue to be a multiplicity of nations and cultures on our planet and, along with them, various languages besides English. It is difficult, after all, to interrupt something as intimate and spontaneous as what language people speak to their children. Who truly imagines a Japan with no Japanese or a Greece with no Greek? The spread of English just means that earthlings will tend to use a local language in their own orbit and English for communication beyond.
        But the days when English shared the planet with thousands of other languages are numbered. A traveler to the future, a century from now, is likely to notice two things about the language landscape of Earth. One, there will be vastly fewer languages. Two, languages will often be less complicated than they are today—especially in how they are spoken as opposed to how they are written.

        Some may protest that it is not English but Mandarin Chinese that will eventually become the world’s language, because of the size of the Chinese population and the increasing economic might of their nation. But that’s unlikely. For one, English happens to have gotten there first. It is now so deeply entrenched in print, education and media that switching to anything else would entail an enormous effort. We retain the QWERTY keyboard and AC current for similar reasons.

        Also, the tones of Chinese are extremely difficult to learn beyond childhood, and truly mastering the writing system virtually requires having been born to it. In the past, of course, notoriously challenging languages such as Greek, Latin, Aramaic, Arabic, Russian and even Chinese have been embraced by vast numbers of people. But now that English has settled in, its approachability as compared with Chinese will discourage its replacement. Many a world power has ruled without spreading its language, and just as the Mongols and Manchus once ruled China while leaving Chinese intact, if the Chinese rule the world, they will likely do so in English.

        Yet more to the point, by 2115, it’s possible that only about 600 languages will be left on the planet as opposed to today’s 6,000. Japanese will be fine, but languages spoken by smaller groups will have a hard time of it. Too often, colonialization has led to the disappearance of languages: Native speakers have been exterminated or punished for using their languages. This has rendered extinct or moribund, for example, most of the languages of Native Americans in North America and Aboriginal peoples of Australia. Urbanization has only furthered the destruction, by bringing people away from their homelands to cities where a single lingua franca reigns.

        Even literacy, despite its benefits, can threaten linguistic diversity. To the modern mind, languages used in writing, with its permanence and formality, seem legitimate and “real,” while those that are only spoken—that is, all but a couple hundred of them today—can seem evanescent and parochial. Few illusions are harder to shed than the idea that only writing makes something “a language.” Consider that Yiddish is often described as a “dying” language at a time when hundreds of thousands of people are living and raising children in it—just not writing it much—every day in the U.S. and Israel.

        It is easy for speakers to associate larger languages with opportunity and smaller ones with backwardness, and therefore to stop speaking smaller ones to their children. But unless the language is written, once a single generation no longer passes it on to children whose minds are maximally plastic, it is all but lost. We all know how much harder it is to learn a language well as adults.

        In a community where only older people now speak a language fluently, the task is vastly more difficult than just passing on some expressions, words and word endings. The Navajo language made news recently when a politician named Chris Deschene was barred from leading the Navajo nation because his Navajo isn’t fluent. One wishes Mr. Deschene well in improving his Navajo, but he has a mountain to climb. In Navajo there is no such thing as a regular verb: You have to learn by heart each variation of every verb. Plus it has tones.

        That’s what indigenous languages tend to be like in one way or another. Languages “grow” in complexity the way that people pick up habits and cars pick up rust. One minute the way you mark a verb in the future tense is to use will: I will buy it. The next minute, an idiom kicks in where people say I am going to buy it, because if you are going with the purpose of doing something, it follows that you will. Pretty soon that gels into a new way of putting a verb in the future tense with what a Martian would hear as a new “word,” gonna.

        In any language that kind of thing is happening all the time in countless ways, far past what is necessary even for nuanced communication. A distinction between he and she is a frill that most languages do without, and English would be fine without gonna alongside will, irregular verbs and much else.

        These features, like he versus she, certainly don’t hurt anything. A language isn’t something that can be trimmed like a bush, and children have no trouble picking up even the weirdest of linguistic frills. A “click” language of southern Africa typically has not just two or three but as many as dozens of different clicks to master (native speakers have a bump on their larynx from producing them 24/7). For English speakers, it seems hard enough that Mandarin Chinese requires you to distinguish four tones to get meaning across, but in the Hmong languages of Southeast Asia, any syllable means different things according to as many as eight tones.
        But the very things that make these languages so fabulously rich also makes it hard to revive them once lost—it’s tough to learn hard stuff when you’re grown, busy and self-conscious. There are diligent efforts to keep various endangered languages from dying, but the sad fact is that few are likely to lead to communities raising children in the language, which is the only way a language exists as its full self.

        Instead, many communities, passing their ancestral language along by teaching it in school and to adults, will create new versions of the languages, with smaller vocabularies and more streamlined grammars. The Irish Gaelic proudly spoken by today’s English-Gaelic bilinguals is an example, something one might call a “New Gaelic.” New versions of languages like this will be part of a larger trend, growing over the past few millennia in particular: the birth of languages less baroquely complicated than the linguistic norm of the premodern world.

        The first wave in this development occurred when technology began to allow massive, abrupt population transfers. Once large numbers of people could cross an ocean at one time, or be imported by force into a territory, a new language could end up being learned by hordes of adults instead of by children. As we know from our experiences in the classroom, adults aren’t as good at mastering the details of a language as toddlers are, and the result was simpler languages.

        Vikings, for example, invaded England starting in the eighth century and married into the society. Children in England, hearing their fathers’ “broken” Old English in a time when schooling was limited to elites and there was no media, grew up speaking that kind of English, and the result was what I am writing now. Old English bristled with three genders, five cases and the same sort of complex grammar that makes modern German so difficult for us, but after the Vikings, it morphed into modern English, one of the few languages in Europe that doesn’t assign gender to inanimate objects. Mandarin, Persian, Indonesian and other languages went through similar processes and are therefore much less “cluttered” than a normal language is.

        The second wave of simplification happened when a few European powers transported African slaves to plantations or subjected other people to similarly radical displacements. Adults had to learn a language fast, and they learned even less of it than Vikings did of English—often just a few hundred words and some scraps of sentence structure. But that won’t do as a language to fully live in, and so they expanded these fundamentals into brand-new languages. Now these languages can express any nuance of human thought, but they haven’t existed long enough to also dangle unnecessary things like willfully irregular verbs. These are called Creole languages.

        It’s far easier to manage a basic conversation in a Creole than in an older language. Haitian Creole, for example, is a language low on the complications that make learning Navajo or Hmong so tough. It spares a student from having to know that boats are male and tables are female, which is one of the reasons that it’s so hard to master French, the language from which it got most of its words.

        Creole languages were created world-wide during the era that the textbooks call Western “exploration.” African soldiers created an Arabic Creole in Sudan; orphans created a German one in New Guinea. Aboriginal Australians created an English Creole, which was passed on to surrounding locations such as, again, New Guinea, where under the name Tok Pisin it is today the language of government for people speaking hundreds of different native languages. Jamaican patois, South Carolina’s Gullah and Cape Verdean are other examples.
        Modern population movements are now creating a third wave of language streamlining. In cities world-wide, children of immigrants speaking many different languages are growing up speaking among themselves a version of their new country’s language that nibbles away at such arbitrary features as irregular verbs and gendered objects. It’s a kind of compromise between the original version of the language and the way their parents speak it.

        Linguists have no single term yet for these new speech varieties, but from Kiezdeutsch in Germany to “Kebob Norsk” in Norway, from the urban Wolof of Senegal to Singapore’s “Singlish,” the world is witnessing the birth of lightly optimized versions of old languages. These will remain ways of speaking that are rarely committed to the page. Yet as we know from languages like Yiddish, this will hardly disqualify them as thriving human languages.

        This streamlining should not be taken as a sign of decline. All of the “optimized” languages remain full languages in every sense of the term, as we know from the fact that I’m writing in one: An Old English speaker who heard modern English would consider it confounding and “broken.” That any language has all irregular verbs, eight tones or female tables is ultimately a matter of accident, not design.

        Hopefully, the languages lost amid all of this change will at least be described and, with modern tools, recorded for posterity. We may regret the eclipse of a world where 6,000 different languages were spoken as opposed to just 600, but there is a silver lining in the fact that ever more people will be able to communicate in one language that they use alongside their native one.

        After all, what’s peculiar about the Babel tale is the idea of linguistic diversity as a curse, not the idea of universal comprehension as a blessing. The future promises both a goodly amount of this diversity and ever more mutual comprehension, as many languages become easier to pick up, in their spoken versions, than they once were. A future dominated by English won’t be a linguistic paradise, in short, but it won’t be a linguistic Armageddon either.`

    },
    {
        title:'What Kind of Genius Are You?',
        Author:'Daniel H. Pink',
        article:`A new theory suggests that creativity comes in two distinct types – quick and dramatic, or careful and quiet.

        David Galenson in front of Georges Seurat’s Sunday on La Grande Jatte at the Art Institute of Chicago. A classic conceptualist, Seurat reinvented painting at the age of 25.


        When Galenson plotted the average auction price of an artist’s work against age, two distinct patterns emerged. Conceptualists did their breakthrough work early in life and then declined steadily. Experimental innovators, on the other hand, plodded along, peaking late in their careers.

        In the fall of 1972, when David Galenson was a senior economics major at Harvard, he took what he describes as a “gut” course in 17th-century Dutch art. On the first day of class, the professor displayed a stunning image of a Renaissance Madonna and child. “Pablo Picasso did this copy of a Raphael drawing when he was 17 years old,” the professor told the students. “What have you people done lately?” It’s a question we all ask ourselves. What have we done lately? It rattles us each birthday. It surfaces whenever an upstart twentysomething pens a game-changing novel or a 30-year-old tech entrepreneur becomes a billionaire. The question nagged at Galenson for years. In graduate school, he watched brash colleagues write dissertations that earned them quick acclaim and instant tenure, while he sat in the library meticulously tabulating 17th- and 18th-century indentured-servitude records. He eventually found a spot on the University of Chicago’s Nobelist-studded economics faculty, but not as a big-name theorist. He was a colonial economic historian – a utility infielder on a team of Hall of Famers.

        Now, however, Galenson might have done something at last, something that could provide hope for legions of late bloomers everywhere. Beavering away in his sunny second-floor office on campus, he has scoured the records of art auctions, counted entries in poetry anthologies, tallied images in art history textbooks – and then sliced and diced the numbers with his econometric ginsu knife. Applying the fiercely analytic, quantitative tools of modern economics, he has reverse engineered ingenuity to reveal the source code of the creative mind.

        What he has found is that genius – whether in art or architecture or even business – is not the sole province of 17-year-old Picassos and 22-year-old Andreessens. Instead, it comes in two very different forms, embodied by two very different types of people. “Conceptual innovators,” as Galenson calls them, make bold, dramatic leaps in their disciplines. They do their breakthrough work when they are young. Think Edvard Munch, Herman Melville, and Orson Welles. They make the rest of us feel like also-rans. Then there’s a second character type, someone who’s just as significant but trudging by comparison. Galenson calls this group “experimental innovators.” Geniuses like Auguste Rodin, Mark Twain, and Alfred Hitchcock proceed by a lifetime of trial and error and thus do their important work much later in their careers. Galenson maintains that this duality – conceptualists are from Mars, experimentalists are from Venus – is the core of the creative process. And it applies to virtually every field of intellectual endeavor, from painters and poets to economists.

        After a decade of number crunching, Galenson, at the not-so-tender age of 55, has fashioned something audacious and controversial: a unified field theory of creativity. Not bad for a middle-aged guy. What have you done lately?

        Galenson’s quest to unlock the secret of innovation began almost by accident. In the spring of 1997, he decided to buy a painting, a small gouache by the American artist Sol LeWitt. But before he put down his money, he called a friend in the art world, who told him that the price was too high. We’re selling that size for less, she said.

        “I thought, this is like carpet,” Galenson tells me one afternoon in his office. Size determines price? His friend hadn’t even seen the painting. What about when the piece was created, what stage it represented in the artist’s career? His friend said that didn’t matter. “I thought, it has to matter.”

        Galenson was right, of course. Art isn’t carpet. And age does matter. The relationship between age and other economic variables was at the foundation of Galenson’s academic work. His first book examined the relationship of age to productivity among indentured servants in colonial America. His second book looked at the relationship of age to the price of slaves. “It was the same regression,” Galenson says, still amazed years after the discovery. “A hedonic wage regression!”

        So he bought the painting and set out to answer questions about art the way any LeWitt-loving economist would.

        Galenson collected data, ran the numbers, and drew conclusions. He selected 42 contemporary American artists and researched the auction prices for their works. Then, controlling for size, materials, and other variables, he plotted the relationship between each artist’s age and the value of his or her paintings. On the vertical axis, he put the price each painting fetched at auction; on the horizontal axis, he noted the age at which the artist created the work. When he tacked all 42 charts to his office wall, he saw two distinct shapes.

        For some artists, the curve hit an early peak followed by a gradual decline. People in this group created their most valuable works in their youth – Andy Warhol at 33, Frank Stella at 24, Jasper Johns at 27. Nothing they made later ever reached those prices. For others, the curve was more of a steady rise with a peak near the end. Artists in this group produced their most valuable pieces later in their careers – Willem de Kooning at 43, Mark Rothko at 54, Robert Motherwell at 72. But their early work wasn’t worth much.

        Galenson decided to test the robustness of his conclusions about artists’ life cycles by looking at variables other than price. Art history textbooks presumably reflect the consensus among scholars about which works are important. So he and his research assistants gathered up textbooks and began tabulating the illustrations as a way of inferring importance. (The methodology is analogous to Google’s PageRank system: The more books that “linked” to a particular piece of art, the more important it was assumed to be.)

        When Galenson’s team correlated the frequency of an image with the age at which the artist created it, the same two contrasting graphs reappeared. Some artists were represented by dozens of pieces created in their twenties and thirties but relatively few thereafter. For other artists, the reverse was true.

        Galenson, a classic library rat, began reading biographies of the artists and accounts by art critics to add some qualitative meat to these quantitative bones. And then the theory came alive. These two patterns represented two types of artists – indeed, two types of humans.

        The insight was so powerful that Galenson soon turned his full attention to the subject. He elaborated his theory in 24 additional papers and set down his findings in a pair of books, Painting Outside the Lines: Patterns of Creativity in Modern Art, published in 2001, and Old Masters and Young Geniuses: The Two Life Cycles of Artistic Creativity, published earlier this year.

        Pablo Picasso and Paul Cézanne are the archetypes of the Galensonian universe. Picasso was a conceptual innovator. He broke with the past to invent a revolutionary style, Cubism, that jolted art in a new direction. His Demoiselles d’Avignon, regarded by critics as the most important painting of the past 100 years, appears in more art history textbooks than any other 20th-century piece. Picasso completed Demoiselles when he was 26. He lived into his nineties and produced many other well-known works, of course, but Galenson’s analysis shows that of all the Picassos that appear in textbooks, nearly 40 percent are those he completed before he turned 30.

        Cézanne was an experimental innovator. He progressed in fits and starts. Working endlessly to perfect his technique, he moved slowly toward a goal that he never fully understood. As a result, he bloomed late. The highest-priced Cézannes are paintings he made in the year he died, at age 67. Cézanne is well represented in art history textbooks; he’s the third-most-illustrated French artist of the 20th century. But of all his reproduced images, just 2 percent are from his twenties. Sixty percent were completed after he turned 50, and he painted more than one-third during his sixties.

        Picasso and Cézanne represent radically different approaches to creation. Picasso thought through his works carefully before he put brush to paper. Like most conceptualists, he figured out in advance what he was trying to create. The underlying idea was what mattered; the rest was mere execution. The hallmark of conceptualists is certainty. They know what they want. And they know when they’ve created it. Cézanne was different. He rarely preconceived a work. He figured out what he was painting by actually painting it. “Picasso signed virtually everything he ever did immediately,” Galenson says. “Cézanne signed less than 10 percent.”

        Experimentalists never know when their work is finished. As one critic wrote of Cézanne, the realization of his goal “was an asymptote toward which he was forever approaching without ever quite reaching.”

        Galenson later applied his methodology to poetry. He counted the poems that appear in major anthologies and recorded the age at which the poet wrote each entry. Once again, conceptual poets like T. S. Eliot, Ezra Pound, and Sylvia Plath, each of whom made sudden breaks from convention and emphasized abstract ideas over visual observations, were early achievers. Eliot wrote “The Love Song of J. Alfred Prufrock” at 23 and “The Wasteland” at 34. Pound published five volumes of poetry before he turned 30. On the other hand, experimental poets like Wallace Stevens, Robert Frost, and William Carlos Williams, whose work is grounded in concrete images and everyday language, took years to mature. For example, both Pound and Frost lived into their eighties. But by the time Pound turned 40, he had essentially exhausted his creative output. Of his anthologized poems, 85 percent are from his twenties and thirties. By comparison, Frost got a late start. He has more poems in anthologies than any other American poet, but he wrote 92 percent of them after his 40th birthday.

        On and on it goes. Conceptualist F. Scott Fitzgerald wrote The Great Gatsby – light on character development, heavy on symbolism – when he was 29. Experimentalist Mark Twain frobbed around with different writing styles and formats and wrote The Adventures of Huckleberry Finn at 50. Conceptualist Maya Lin redefined our notion of national monuments while still a college student; experimentalist Frank Lloyd Wright created Fallingwater when he was 70.

        The theory even applies to economists. Over lunch at the University of Chicago’s faculty club, Galenson tells me the story of Paul Samuelson, one of the most renowned economists of the last century. No shrinking violet, Samuelson titled his dissertation “Foundations of Economic Analysis.” As a 25-year- old, he sought to reinvent the entire field – and later won a Nobel Prize for ideas he came up with as a grad student. Swift, deductive, certain. That’s a conceptual economist.

        An experimental economist is someone like … Galenson. He progresses more quietly, more inductively, step- by-careful-step. And he often sails into the winds of indifference – from the art world, which believes that creativity is too elusive for econometric analysis, and from colleagues who can’t comprehend why he’s wasting his time with picture books. At one point, he leans over his chicken sandwich and tells me quietly and in mild horror, “I don’t have a colleague who knows a Manet from a Monet.”

        Yet Galenson, whose parents were both economists, pushes on, ever approaching the asymptote. “Most people in economics do their best work before the age of 35. And I was constantly irritated that these guys were getting ahead of me,” Galenson says. “But from very early in my career, I knew I could do really good work. I didn’t know exactly how, and I didn’t know when. I just had this vague feeling that my work was going to improve.”

        The most-reproduced 19th-century work in US and European art history texts is Georges Seurat’s Sunday on La Grande Jatte. The painting, completed in Paris in 1886, now hangs on the second floor of the Art Institute of Chicago. One morning in April, I visit the museum with Galenson to look at this and other masterpieces.

        Walking the floors of a museum with David Galenson is a treat. He is astonishingly well informed about art. For nearly every painting I point to, he accurately pinpoints the year it was made, tells me its backstory, and describes something my pedestrian eyes haven’t noticed. He is an erudite, insightful guide who keeps things entertaining with salty asides. “Monet had a lot of balls,” he explains in one gallery. “Renoir was a very peculiar guy,” he says later. Several times during our four-hour journey through the museum, tourists and schoolteachers sidle up to eavesdrop on his commentary.

        Galenson threads his small frame through the swarm of visitors gathered in front of La Grande Jatte, considers it for a moment, and then launches into an explanation of why this artist was the quintessential conceptual innovator. “Seurat starts off at the official academy,” Galenson says. “He goes and finds the Impressionists, and he works with them. But he’s a very nerdy guy. He’s sort of a proto-scientist, and he wants to be systematic.” Seurat knew about recent discoveries on optical perception – including that people perceive a hue more vividly when it’s paired with its opposite on the color wheel. So he broke from the Impressionists to study the science. He made dozens of preparatory studies for the painting, then executed it with scientific precision.

        As Galenson explains, “This guy comes along and says, ‘Look, Impressionism has been all the rage. But these guys are unsystematic, they’re casual. I’m going to make a scientific, progressive art. And this is going to be the prototype of the new art. In the future, everyone will paint scientifically.’” Seurat was 25. “This is his dissertation, basically. This is like ‘Foundations of Economic Analysis,’” Galenson tells me. “It’s like Samuelson saying, ‘I’m going to unite all of economics.’ Seurat is saying, ‘We’re discovering the underlying principles of representation.’ One of them is the systematic use of color. And this is the masterpiece.” La Grande Jatte changed the practice of nearly every painter of its time.

        Alas, this is the only painting for which Seurat is remembered – in part, because he died five years after completing it. But that would be the case even had he lived far longer, Galenson maintains. “He did the most important work of his generation; he couldn’t have done it again. There’s no law you can’t do it again. But once you’ve written Gatsby, it’s very unlikely you’re going to outdo it.” (Indeed, Fitzgerald went on to write two more novels, one published posthumously, but neither approached the importance of The Great Gatsby.)

        We meander through the museum and stop awhile in Gallery 238, which includes two paintings by Jackson Pollock. Galenson gestures toward the first, The Key, done in 1946, when Pollock was 34 years old. It looks like a child’s drawing – thick lines, crayony colors, underwhelming. “Pollock was a really bad artist at this point,” Galenson says.

        Nearby is another Pollock, Greyed Rainbow, a large and explosive work done in 1953. It’s spectacular. Pollock was an experimental innovator who spent two decades tinkering, and this painting is a triumph of that process. To paint it, he laid the canvas on the floor, splattered it with paint, walked around it, tacked it to the wall, looked at it, put it back on the ground, splattered it with more paint, and so on. “This painting is full of innovations,” Galenson says, “but Pollock arrived here by trial and error. He was a slow developer.”

        “Take a few steps back,” Galenson directs me. “If you were to describe this to somebody and see the jagged edges, you might say this is a really agitated painting. If you had this in your house, would it make you nervous?”

        No, I answer.

        “No. It’s perfectly resolved. This is a great visual artist making a great work,” Galenson says. “He didn’t start this way.”

        We walk back to The Key. “Look at this thing,” Galenson says. “It’s a piece of crap. If that weren’t by a famous artist, it wouldn’t be here.”

        “Seurat died at 31,” Galenson reminds me. “If Pollock had died at 31, you never would have heard of him.”

        Galenson’s theory of artistic life cycles is hardly bulletproof. Picasso, the marquee youthful innovator, painted his incomparable condemnation of the Spanish Civil War, Guernica, at the creaky age of 56. Is that somehow an exception? Sylvia Plath, a prolific conceptualist poet, did extraordinary work in her twenties but committed suicide in her early thirties. Couldn’t she have continued innovating if she’d lived? Philip Roth won a National Book Award for Goodbye, Columbus in his twenties and a Pulitzer Prize for American Pastoral in his sixties. Where does he belong?
        Galenson recognizes the limits of dogmatic duality. In his later papers, as well as in the book he published this year, he has refined his theory to make it less binary. He now talks of a continuum – with extreme conceptual innovators at one end, extreme experimental innovators at the other, and moderates in the middle. He allows that people can change camps over the course of a career, but he thinks it’s difficult. And he acknowledges that he’s charting tendencies, not fixed laws.

        Just because a theory isn’t perfect, though, doesn’t mean it’s not valuable. What Galenson has done – and what might deliver the recognition that bypassed him in his youth – is to identify two significant gaps in our understanding of the world and of ourselves.

        The first gap exists within his own field. Galenson mentions that his professional colleagues scratch their heads over his research. “It doesn’t fit immediately into what economists do,” he tells me. “The word creativity won’t appear in the index of an economics textbook.” Then, ever the empiricist, he rises from his chair, grabs a textbook off a shelf, and shows me the lacuna in the end pages.

        That’s a serious omission. Although Galenson has limited his analysis mostly to artists, he believes the pattern he’s uncovered also applies to science, technology, and business. Economic activity is all about creation – even more so today, as advanced economies shed routine work and gain advantage through innovation and ingenuity.

        If the link between age and creative capacity applies outside the bounds of the arts, then every economic institution – universities, companies, governments – should take note. Galenson’s ideas may yield clues about how to foster fresh thinking in a wide range of organizations, industries, and disciplines. If nurturing innovators is an economic imperative, the real peculiarity isn’t that Galenson is studying creativity; it’s that other economists aren’t.

        Which leads to the second gap. Consider the word genius. “Since the Renaissance, genius has been associated with virtuosos who are young.

        The idea is that you’re born that way – it’s innate and it manifests itself very young,” Galenson says. But that leaves the vocabulary of human possibility incomplete. “Who’s to say that Virginia Woolf or Cézanne didn’t have an innate quality that simply had to be nourished for 40 or 50 years before it bloomed?” The world exalts the young turks – the Larrys and the Sergeys, the Picassos and the Samuelsons. And it should. We need those brash, certain, paradigm-busting youthful conceptualists. We should give them free rein to do bold work and avoid saddling them with rules and bureaucracy.

        But we should also leave room for those of us who have, er, avoided peaking too early, whose most innovative days may lie ahead. Nobody would have heard of Jackson Pollock had he died at 31. But the same would be true had Pollock given up at 31. He didn’t. He kept at it. We need to look at that more halting, less certain fellow and perhaps not write him off too early, give him a chance to ride the upward curve of middle age.

        Of course, not every unaccomplished 65-year-old is some undiscovered experimental innovator. This is a universal theory of creativity, not a Viagra for sagging baby boomer self-esteem. It’s no justification for laziness or procrastination or indifference. But it might bolster the resolve of the relentlessly curious, the constantly tinkering, the dedicated tortoises undaunted by the blur of the hares. Just ask David Galenson.`
    },
    {
        title:'Your Lifestyle Has Already Been Designed',
        author:'David Cain',
        article:`Well I’m in the working world again. I’ve found myself a well-paying gig in the engineering industry, and life finally feels like it’s returning to normal after my nine months of traveling.

        Because I had been living quite a different lifestyle while I was away, this sudden transition to 9-to-5 existence has exposed something about it that I overlooked before.
        
        Since the moment I was offered the job, I’ve been markedly more careless with my money. Not stupid, just a little quick to pull out my wallet. As a small example, I’m buying expensive coffees again, even though they aren’t nearly as good as New Zealand’s exceptional flat whites, and I don’t get to savor the experience of drinking them on a sunny café patio. When I was away these purchases were less off-handed, and I enjoyed them more.
        
        I’m not talking about big, extravagant purchases. I’m talking about small-scale, casual, promiscuous spending on stuff that doesn’t really add a whole lot to my life. And I won’t actually get paid for another two weeks.
        
        In hindsight I think I’ve always done this when I’ve been well-employed — spending happily during the “flush times.” Having spent nine months living a no-income backpacking lifestyle, I can’t help but be a little more aware of this phenomenon as it happens.
        
        I suppose I do it because I feel I’ve regained a certain stature, now that I am again an amply-paid professional, which seems to entitle me to a certain level of wastefulness. There is a curious feeling of power you get when you drop a couple of twenties without a trace of critical thinking. It feels good to exercise that power of the dollar when you know it will “grow back” pretty quickly anyway.
        
        What I’m doing isn’t unusual at all. Everyone else seems to do this. In fact, I think I’ve only returned to the normal consumer mentality after having spent some time away from it.
        
        One of the most surprising discoveries I made during my trip was that I spent much less per month traveling foreign counties (including countries more expensive than Canada) than I did as a regular working joe back home. I had much more free time, I was visiting some of the most beautiful places in the world, I was meeting new people left and right, I was calm and peaceful and otherwise having an unforgettable time, and somehow it cost me much less than my humble 9-5 lifestyle here in one of Canada’s least expensive cities.
        
        It seems I got much more for my dollar when I was traveling. Why?
        
        A Culture of Unnecessaries

        Here in the West, a lifestyle of unnecessary spending has been deliberately cultivated and nurtured in the public by big business. Companies in all kinds of industries have a huge stake in the public’s penchant to be careless with their money. They will seek to encourage the public’s habit of casual or non-essential spending whenever they can.
        
        In the documentary The Corporation, a marketing psychologist discussed one of the methods she used to increase sales. Her staff carried out a study on what effect the nagging of children had on their parents’ likelihood of buying a toy for them. They found out that 20% to 40% of the purchases of their toys would not have occurred if the child didn’t nag its parents. One in four visits to theme parks would not have taken place. They used these studies to market their products directly to children, encouraging them to nag their parents to buy.
        
        This marketing campaign alone represents many millions of dollars that were spent because of demand that was completely manufactured.
        
        “You can manipulate consumers into wanting, and therefore buying, your products. It’s a game.” ~ Lucy Hughes, co-creator of “The Nag Factor”
        
        This is only one small example of something that has been going on for a very long time. Big companies didn’t make their millions by earnestly promoting the virtues of their products, they made it by creating a culture of hundreds of millions of people that buy way more than they need and try to chase away dissatisfaction with money.
        
        We buy stuff to cheer ourselves up, to keep up with the Joneses, to fulfill our childhood vision of what our adulthood would be like, to broadcast our status to the world, and for a lot of other psychological reasons that have very little to do with how useful the product really is. How much stuff is in your basement or garage that you haven’t used in the past year?
        
        The real reason for the forty-hour workweek

        The ultimate tool for corporations to sustain a culture of this sort is to develop the 40-hour workweek as the normal lifestyle. Under these working conditions people have to build a life in the evenings and on weekends. This arrangement makes us naturally more inclined to spend heavily on entertainment and conveniences because our free time is so scarce.
        
        I’ve only been back at work for a few days, but already I’m noticing that the more wholesome activities are quickly dropping out of my life: walking, exercising, reading, meditating, and extra writing.
        
        The one conspicuous similarity between these activities is that they cost little or no money, but they take time.
        
        Suddenly I have a lot more money and a lot less time, which means I have a lot more in common with the typical working North American than I did a few months ago. While I was abroad I wouldn’t have thought twice about spending the day wandering through a national park or reading my book on the beach for a few hours. Now that kind of stuff feels like it’s out of the question. Doing either one would take most of one of my precious weekend days!
        
        The last thing I want to do when I get home from work is exercise. It’s also the last thing I want to do after dinner or before bed or as soon as I wake, and that’s really all the time I have on a weekday.
        
        This seems like a problem with a simple answer: work less so I’d have more free time. I’ve already proven to myself that I can live a fulfilling lifestyle with less than I make right now. Unfortunately, this is close to impossible in my industry, and most others. You work 40-plus hours or you work zero. My clients and contractors are all firmly entrenched in the standard-workday culture, so it isn’t practical to ask them not to ask anything of me after 1pm, even if I could convince my employer not to.
        
        The eight-hour workday developed during the industrial revolution in Britain in the 19th century, as a respite for factory workers who were being exploited with 14- or 16-hour workdays.
        
        As technologies and methods advanced, workers in all industries became able to produce much more value in a shorter amount of time. You’d think this would lead to shorter workdays.
        
        But the 8-hour workday is too profitable for big business, not because of the amount of work people get done in eight hours (the average office worker gets less than three hours of actual work done in 8 hours) but because it makes for such a purchase-happy public. Keeping free time scarce means people pay a lot more for convenience, gratification, and any other relief they can buy. It keeps them watching television, and its commercials. It keeps them unambitious outside of work.
        
        We’ve been led into a culture that has been engineered to leave us tired, hungry for indulgence, willing to pay a lot for convenience and entertainment, and most importantly, vaguely dissatisfied with our lives so that we continue wanting things we don’t have. We buy so much because it always seems like something is still missing.
        
        Western economies, particularly that of the United States, have been built in a very calculated manner on gratification, addiction, and unnecessary spending. We spend to cheer ourselves up, to reward ourselves, to celebrate, to fix problems, to elevate our status, and to alleviate boredom.
        
        Can you imagine what would happen if all of America stopped buying so much unnecessary fluff that doesn’t add a lot of lasting value to our lives?
        
        The economy would collapse and never recover.
        
        All of America’s well-publicized problems, including obesity, depression, pollution and corruption are what it costs to create and sustain a trillion-dollar economy. For the economy to be “healthy”, America has to remain unhealthy. Healthy, happy people don’t feel like they need much they don’t already have, and that means they don’t buy a lot of junk, don’t need to be entertained as much, and they don’t end up watching a lot of commercials.
        
        The culture of the eight-hour workday is big business’ most powerful tool for keeping people in this same dissatisfied state where the answer to every problem is to buy something.
        
        You may have heard of Parkinson’s Law. It is often used in reference to time usage: the more time you’ve been given to do something, the more time it will take you to do it. It’s amazing how much you can get done in twenty minutes if twenty minutes is all you have. But if you have all afternoon, it would probably take way longer.
        
        Most of us treat our money this way. The more we make, the more we spend. It’s not that we suddenly need to buy more just because we make more, only that we can, so we do. In fact, it’s quite difficult for us to avoid increasing our standard of living (or at least our rate of spending) every time we get a raise.
        
        I don’t think it’s necessary to shun the whole ugly system and go live in the woods, pretending to be a deaf-mute, as Holden Caulfield often fantasized. But we could certainly do well to understand what big commerce really wants us to be. They’ve been working for decades to create millions of ideal consumers, and they have succeeded. Unless you’re a real anomaly, your lifestyle has already been designed.
        
        The perfect customer is dissatisfied but hopeful, uninterested in serious personal development, highly habituated to the television, working full-time, earning a fair amount, indulging during their free time, and somehow just getting by.
        
        Is this you?
        
        Two weeks ago I would have said hell no, that’s not me, but if all my weeks were like this one has been, that might be wishful thinking.`
    },
    {
        title:'You can do it, baby!',
        author:'Leslie Garrett',
        article:`Our culture is rich with esteem-boosting platitudes for young dreamers, but the assurances are dishonest and dangerous.

        Twelve-year-old Gwenyth has dark brown eyes and a fierce desire to change people’s negative perception of sharks. She attends West Oaks French Immersion school in Ontario, where she and about a dozen other kids who test as gifted, spend three days every other month exploring topics outside their usual curriculum. Most recently, they studied forensics, searching for clues, avoiding red herrings, and learning how scientists test for DNA evidence.

        But it’s sharks that fascinate her. She’s determined to be a marine biologist some day and has given considerable thought to what she’ll need to achieve this. Her teacher, Mrs Ensing, who is optimistic about Gwenyth’s prospects, routinely tells her elite group that they can be anything they want to be.

        Gwenyth likes her teacher but is troubled by this philosophy. ‘You can’t be anything,’ she says, ‘if you don’t manage to get the marks good enough, or if you have the wrong idea about it. There was a guy on YouTube who wanted to be a veterinarian and they made him watch a video of something happening to an animal and he fainted, so he didn’t get the job.’

        Her skepticism is well-founded. A 2012 LinkedIn survey showed that roughly one in three adults are working at their ‘dream job’, which means that two in three are not. Gallup’s most recent State of the American Workplace poll came up with similar results when it concluded that 30 per cent of employees are ‘engaged’ in their work, while 52 per cent are ‘not engaged’ and 18 per cent are ‘actively disengaged’.

        ‘When you tell somebody: You can be anything,’ says Jean Twenge, professor of psychology at San Diego State University and author of Generation Me (2014), ‘that “anything” they’re thinking of is rarely a plumber or an accountant.’

        Indeed, a 2011 survey of more than 5,000 children around the world revealed that while almost half of children in developing countries dreamed of becoming doctors and teachers, more than a quarter of American children aspire to such careers as professional athletes, singers and actors. When a grown‑up asks the inevitable: What do you want to be when you grow up?, most kids have an answer: video‑game developer; astronaut; back-up dancer for Rihanna. And many grown-ups will congratulate them for dreaming big, assuring them that, with hard work and a can-do attitude, they can be anything they want.

        When your child is four or five, barring intellectual disabilities or severe behavioural diagnoses, anything does seem possible. A child shows an interest in art and we imagine his work eventually hanging in galleries. A talented runner, we think, might make the Olympics. Kids who love science are given microscopes and we begin to wonder if we should start saving up for college fees at the Massachusetts Institute of Technology. Backing our hopes and theirs are the culture’s cheerleaders, led by viral convocation speeches and a steady stream of ‘overnight’ successes unveiled on reality shows and YouTube, all urging us to dream big and never give up.

        Consider Steve Jobs’s commencement address to Stanford graduates in 2005. He was, of course, talking to the high achievers who had already earned a degree from a prestigious university. But with more than 22 million views on YouTube, his advice – ‘find what you love… Don’t settle’ – has resonated with the masses. Oprah Winfrey, whose own rags-to-riches tale gives her moral authority, insists that if we follow our passion, achievement will follow. Even Dr Seuss left us with Oh, the Places You’ll Go! (1990), which has become the go-to graduation gift for millions, assuring kids that their imminent success is ‘98 and ¾ per cent guaranteed’.

        As long ago as the fourth century BCE, the Greek philosopher Aristotle celebrated the value of a meaningful goal when he coined the term eudaimonia (‘human flourishing’). The concept re‑emerged in the 16th-century Protestant concept of a ‘calling’. More recently, in the 1960s, a whole generation of young people brought up at the height of an economic boom began asking whether work could amount to more than just paying the bills. Couldn’t it have something to do with meaning and life, talents and passions?

        It was then that the episcopal clergyman Richard Bolles in California noticed people grappling with how to choose that special, meaningful career, and responded by publishing What Color is Your Parachute? (1970), which has sold more than 10 million copies, encouraging job‑hunters and career-changers to inventory their skills and talents. Bolles bristles at the suggestion that he’s telling people to be ‘anything’ they want to be. ‘I hate the phrase,’ he says. ‘We need to say to people: Go for your dreams. Figure out what it is you most like to do, and then let’s talk about how realistically you can find some of that, or most of that, but maybe not all of that.’

        But in this culture of entitlement, gratification and intensive hyper-parenting, Bolles’s cautionary note has barely made a dent. At Florida State University, the sociologist John Reynolds reported that the gap between goals and actual achievements grew significantly over the period from 1976 to 2000. His study, published in Social Problems in 2006, found that just 26 per cent of high‑school seniors in 1976 planned to get an advanced degree and 41 per cent planned to work as professionals, compared with 50 per cent and 63 per cent, respectively, in 2000. Despite the soaring change in ambition, there was no increase in advanced degrees. What did increase was disappointment; the gap between expectation of earning an advanced degree and actually getting one grew from 22 percentage points in 1976 to 41 percentage points in 2000.

        Having lofty dreams can be a wonderful thing. It’s a natural part of childhood to imagine great things for ourselves, says Laura Berk, a professor emerita of economics at Illinois State University and one of the world’s experts on play. And, as kids grow and try, and succeed and fail, the world will shape those dreams.

        Behind the bromides is the wish that we not be bound by our talents, by our genetics, by our temperament, by our character

        The problem arises when we counter the world’s feedback with platitudes such as ‘you can be anything you want’ or ‘don’t give up.’ Tracey Cleantis, a psychotherapist in California and the author of The Next Happy (2015), says that behind such bromides ‘is a kind of wish of parents or ourselves that we’re not bound by our talents, by our genetics, by our temperament, by our character. I think it really creates shame and guilt and feelings of failure.’

        ‘What it essentially says to our children,’ adds Penelope Trunk, author of Brazen Careerist: The New Rules for Success (2007), ‘is that, if they don’t achieve their dreams, they have no one to blame but themselves.’ Indeed, the transition to adulthood is already overwrought, and it’s made only more difficult when you think you can do anything and then feel completely incompetent when you can’t.

        The dangers are legion. Unrealistic plans lead to a waste of time and money. When a C‑student spins her wheels planning on medical school, other, more lucrative and realistic careers – say in business or education – fall by the wayside. And the ambition gap has led to increased dissatisfaction across working life. Deloitte’s 2010 Shift Index revealed that 80 per cent of workers were dissatisfied in their jobs. By 2013, the figure had jumped to 89 per cent.

        The situation even endangers health. In 2007, psychologists from the US and Canada followed 81 university undergraduates for a semester and concluded that those persisting in unattainable goals had higher concentrations of cortisol, an inflammatory hormone associated with adverse medical outcomes, and were more prone to colds.

        In many ways, where work is concerned, we live in a gilded age. Ask octogenarians if they believed they could do anything, and they’d be incredulous: wars, the Great Depression, out-of-reach education, as well as widespread discrimination made the idea of a dream career laughable for most. For generations, work was a means to an end, that end being food on the table, a roof overhead, and clothes on their backs.

        Roman Krznaric, founding faculty member of The School of Life in London, says that his father, a refugee who left Poland for Australia, was a talented musician and linguist, but would have never expected that to be part of his working life. We’ve seen ‘rising expectations among everybody for work that’s more than just a salary,’ says Krznaric. ‘You see this among people who are highly educated or [those who] don’t have very many qualifications, and that helps explain why job dissatisfaction tends to rise over the past couple of decades, because people are asking … to use their talents or passions in their work.’

        The shift in expectation has resulted in tremendous anxiety over achieving these goals and, paradoxically, sheer delusion. Point out to parents or teens just how difficult it can be to achieve such a high-level career and you’ll likely be treated to a catalogue of all the people who bucked the odds. Not just the Oprahs or the Steve Jobses who achieved super-status but the friend of a friend who just bought a modern all-glass home overlooking San Francisco Bay with his signing bonus, or the daughter of a work colleague who was cast in a new TV show, or the neighbour’s son who got a scholarship to Harvard.

        And there’s the rub. If it’s possible for them, we want to believe that it’s possible for our children too. Besides, who wants to be the killjoy who has to remind the struggling math student that medicine likely isn’t an option? Or caution the dancer that, though she might find work in the field, she’ll likely have to supplement her income with waiting tables or working retail? Besides, technically it is possible. At what point do we abandon possible for probable, and encourage our children to do the same?

        ‘This links to the cult of positive thinking,’ says Krznaric, ‘where we’re always wanting to feel up and good and send positive messages… and so we feel that we should only be sending good messages and positive messages to our children and to young people. That it’s somehow wrong or bad or inappropriate to tell them: actually, it isn’t possible.’

        The popular mantra comes from the actor Will Smith, who said: ‘Being realistic is the most commonly travelled road to mediocrity.’

        But ‘mediocrity’ is a loaded term. Who, after all, wants to be ‘mediocre’ or ordinary? And yet, save for a few, aren’t we all? By implying that the only options are superstardom or mediocrity, we ignore where most of us ultimately land – that huge middle ground between anything and nothing much at all.

        ‘People confuse encouraging kids with telling them you can be anything you want to be,’ says Twenge. ‘People think they’re the same thing and they’re not.’

        Someone who’s maybe a taxi driver or a nurse cannot believe that there’s a TV producer who seems to be more miserable than they are

        Instead of emphasising you’re special, you’re great, ‘teach self-control and hard work,’ Twenge says. ‘Those two things are actually connected to success.’ Her eight-year-old recently announced plans for a career as a veterinarian (the career of choice for almost 10 per cent of American children between the ages of 10 and 12). Practising what she preaches, Twenge said it’s ‘a wonderful career… But, just so you know, you have to work really hard at math and science and do very well because it’s really hard to get into veterinary school.’ It might not feel as good as telling our kids they can be anything they want, she says, but it serves them far better.

        Gwenyth has taken it upon herself to figure out what she needs to achieve her dream of becoming a marine biologist – the courses necessary (‘math, science’), the jobs she’ll have to take working toward it (‘taxi driver’), the skills she’ll need (‘how to drive a boat’).

        Bolles suggests we remain curious when children share their career goals. By asking them what, exactly, appeals about a particular job, we – and they – gain insight into their values and perceived talents.

        This self-reflection, which provides the backbone of Bolles’s Parachute model for job-hunters and career-changers, is something Krznaric agrees is crucial. ‘Fifteen- and 16-year-olds have all sorts of interesting things to say about who they are and what they care about.’ He thinks it’s fundamental for kids to be asking not so much what they want to be but who they want to be.

        The answer might be surprising. Krznaric, who teaches his career course around the world and is father to six-year-old twins, is intrigued by something else he’s noticed. The people who take his course are just as likely to already have the so-called dream careers as those seeking them. ‘When I run these classes, it’s really curious that someone who’s maybe a taxi driver or a nurse cannot believe that in the same room there’s a TV producer who seems to be more miserable than they are in their jobs.’

        Julie Lythcott-Haims, the author of How to Raise an Adult (2015) and a former dean of freshmen at Stanford University, routinely counselled students whose dreams were less lofty than what their parents expected – students who wanted to be nurses, not doctors, or high‑school teachers, not university professors. ‘I sat with those students and listened to them going through the motions of doing the work in the fields they felt were legitimate or expected or required, and I was interested in what this human in front of me actually wanted to do with their life, and how can I support them in listening to that voice in their own head?’

        The problem, she says, isn’t telling kids you can be anything, it’s our narrow idea of what ‘anything’ is. ‘We’re equating it with prestige, power, title, money, certain sectors. If we could shift, over the next decade, toward high achievement being the equivalent of knowing your skills and your values and your passion, and living accordingly, imagine what a different world we’d be living in.’

        Cleantis says the issues must be reframed: our dreams are more often about what we hope to feel than what we want to do. ‘There’s a kind of unspoken narrative: if I become this, if I do this, if I achieve this, then I will be loved, I will have self-acceptance,’ she says. By deconstructing what we hope to achieve emotionally, ‘it’s possible to find other ways of achieving that.’

        Cal Newport, the author of So Good They Can’t Ignore You (2012) and a computer science researcher at Georgetown University in Washington, DC, adds that we have got the passion/purpose equation backwards. ‘It misrepresents how people actually end up passionate about their work,’ he says. ‘It assumes that people must have a pre-existing passion, and the only challenge is identifying it and raising the courage to pursue it. But this is nonsense.’ Passion doesn’t lead to purpose but rather, the other way around. People who get really good at something that’s useful and that the world values become passionate about what they’re doing. Finding a great career is a matter of picking something that feels useful and interesting. Not only will you find great meaning in the honing of the craft itself, but having a hard-won skill puts you in a position to dictate how your professional life unfolds.

        Newport’s recommendation begs examination of another aspect of the ‘you-can-be-anything’ framework: should we expect to pursue a passion within our career or is it wiser to try to satisfy it outside of one? Sure, it’s convenient (and nice!) to be paid for something we’d love to do anyway. But is it realistic?

        Marty Nemko, a career counsellor in the San Francisco Bay Area and the public radio host of Work with Marty Nemko, offers up a resounding ‘no’. He’s all for people pursuing their dreams, as a hobby. ‘Do what you love,’ he says, ‘but don’t expect to get paid for it.’ Of course, he says, there will be those who can – and do – make it in fields that are highly competitive. Maybe your passion for computer programming, or for splicing atoms, brushes up against career fields that offer plenty of opportunity. But, if like many, making a career out of your passion is a long-shot, instead of giving it up, incorporate it into your free time.

        Lythcott-Haims encouraged her students to look at three things: what am I good at; what am I passionate about; and what are my values? Then, she told them to ask: ‘How can I spend a meaningful part of my week – whether career or hobby – living at the intersection of those things?’

        ‘You can be anything? It’s not true. You can’t be a dinosaur’

        Maybe our parents and grandparents had it right when they pursued their passions and hobbies – which offered up meaning and mastery – in their free time. Like Krznaric’s father, who made music outside his job. Or like Nemko himself who gave up working as a professional pianist for psychology.

        Krznaric suggests a slightly different model – that of the ‘wide achiever’ who does several jobs at the same time, such as someone who works as an accountant for three days a week and a photographer for two. It’s a smart approach in an unstable economy where, he says, ‘the average job lasts four years’. It also recognises that ‘who we are changes throughout our lives. We’re really bad judges of our future selves.’

        ‘You can be anything you want to be’ is pithy advice that isn’t helping most of the young launch careers or find satisfaction in life. If we really think about it, few of us mean it literally. Twenge has told her daughter that ‘when people say you can be anything, it’s not true. For example, you can’t be a dinosaur.’ Perhaps what we’re really trying to say to our children is that we trust in their ability to build a meaningful life.

        ‘[Adults] should say: be what you’re capable of,’ says Gwenyth, ‘not you could be anything. I’m not very good in dance. That’s like telling me I could be a professional dancer. No. No, I couldn’t be.’`

    },
    {
        title:'Online Dating',
        author:'Emily Witt',
        article:`I am not usually comfortable in a bar by myself, but I had been in San Francisco for a week and the apartment I sublet had no chairs in it, just a bed and a couch. My friends in town were married or worked nights. One Tuesday I had lentil soup for supper standing up at the kitchen counter. After I finished, I moved to the couch in the empty living room and sat under the flat overhead light refreshing feeds on my laptop. This was not a way to live. A man would go to a bar alone, I told myself. So I went to a bar alone.

        I sat on a stool at the centre of the bar, ordered a beer, and refreshed the feeds on my mobile. I waited for something to happen. A basketball game played on several monitors at once. The bar had red fake leather booths, Christmas lights and a female bartender. A lesbian couple cuddled at one end of it. At the other end, around the corner from where I sat, a bespectacled man my age watched the game. As the only man and the only woman alone at the bar, we looked at each other. Then I pretended to watch the game on a monitor that allowed me to look the other way. He turned his back to me to watch the monitor over the pool tables, where the pool players now applauded some exploit.
        
        I waited to be approached. A few stools down, two men broke into laughter. One came over to show me why they were laughing. He handed me his mobile and pointed to a Facebook post. I read the post and smiled obligingly. The man returned to his seat. I drank my beer.
        
        I allowed myself a moment’s longing for my living room and its couch. The couch had a woollen blanket woven in a Navajo-inspired pattern, exemplary of a trend in San Francisco that a friend of mine calls ‘White People Gone Wild’. When I moved in, the receipt for the blanket was on the mantelpiece. It had cost $228. There was a cast-iron gas stove in the fireplace. I had fiddled with the knobs and the gas, but couldn’t figure out how to ignite it. At night the room had the temperature and pallor of a corpse. There was no television.
        
        I returned to my mobile and opened OK Cupid, the free internet dating service. I refreshed the feed that indicated whether other people in the neighbourhood were sitting alone in bars. This service is called OK Cupid Locals. An OK Cupid Locals invitation has to start with the word ‘Let’s’:
        
        Let’s smoke a joint and hang out 
        
        Let’s grab a brunch, lunch, beer or some such for some friendly Saturday revelry.
        
        Let’s get a drink after Koyaanisqatsi at the Castro.
        
        Let’s meet and tickle.
        
        Let’s enjoy a cookie.
        
        Let’s become friends and explore somewhere.
        
        ‘Let’s go now you and I’ always comes into my mind, but I’ve never broadcast an OK Cupid chat signal, I just respond. That night I scrolled until I found a handsome man who had written a benign invitation: ‘Let’s get a drink.’ I looked at his profile. He was Brazilian. I speak Portuguese. He played the drums. ‘Tattoos are a big part of my friends’ and family’s life,’ he wrote. Every era has its own utopian possibilities: ours is the chance to make our lives more bearable through technology.
        
        The man generally held responsible for internet dating as we know it today is a native of Illinois called Gary Kremen, but Kremen was out of the internet dating business altogether by 1997, just around the time people were signing up for the internet en masse. Today he runs a solar energy financing company, is an elected official in Los Altos Hills, California and is better known for his protracted legal battle over the ownership of the pornography website sex.com than he is for inventing internet dating. Like many visionary entrepreneurs, Kremen doesn’t have very good management skills. His life has passed through periods of grave disarray. When I met him, at a conference on the internet dating industry in Miami last January, he asked where I was from. ‘Ah, Minnesota,’ he said: ‘Have you ever been to the Zumbro River?’ The Zumbro flows south of Minneapolis past Rochester, home of the Mayo Clinic. It turned out that Kremen had once driven, or been driven, into the river. He used to be addicted to speed.
        
        In Miami Kremen recounted the genesis of his ideas about internet dating to a room full of matchmakers. In 1992, he was a 29-year-old computer scientist and one of the many graduates of Stanford Business School running software companies in the Bay Area. One afternoon a routine email with a purchase order attached to it arrived in his inbox. But it wasn’t routine: the email was from a woman. At the time, emails from women in his line of work were exceedingly rare. He stared at it. He showed the email to his colleagues. He tried to imagine the woman behind it. ‘I wonder if she would date me?’ Then he had another idea: what if he had a database of all the single women in the world? If he could create such a database and charge a fee to access it, he would most probably turn a profit.
        
        In 1992, that couldn’t be done – modems transmitted information too slowly. Then there was the scarcity of women with online access. Because in its early days the internet was prevalent in worlds that had historically excluded women – the military, finance, mathematics and engineering – women were not online in big numbers. As late as 1996 America Online estimated that of its five million users, 79 per cent were men. In more administrative fields, however, a growing number of women had email.
        
        So Kremen started with email. He left his job, hired some programmers with his credit card, and created an email-based dating service. Subscribers were given anonymous addresses from which to send out their profiles with a photo attached. The photos arrived as hard copy, and Kremen and his employees scanned them in by hand. Interested single people who did not yet have email could participate by fax. By 1994 modems had got faster, so Kremen moved to take his company online. He and four male partners formed Electric Classifieds Inc, a business premised on the idea of re-creating online the classifieds section of newspapers, beginning with the personals. They rented an office in a basement in San Francisco and registered the domain match.com.
        
        ‘ROMANCE – LOVE – SEX – MARRIAGE AND RELATIONSHIPS’ read the headline on an early business plan Electric Classifieds presented to potential investors. ‘American business has long understood that people knock the doors down for dignified and effective services that fulfil these most powerful human needs.’ Kremen eventually removed ‘sex’ from his list of needs, but many of the basic parts of most online dating sites were laid out in this early document. Subscribers completed a questionnaire, indicating the kind of relationship they wanted – ‘marriage partner, steady date, golf partner or travel companion’. Users posted photos: ‘A customer could choose to show himself in various favourite activities and clothing to give the viewing customer a stronger sense of personality and physical character.’
        
        The business plan cited a market forecast that suggested 50 per cent of the adult population would be single by 2000 (a 2008 poll found 48 per cent of American adults were single, compared to 28 per cent in 1960). At the time, single people, particularly those over the age of 30, were still seen as a stigmatised group with which few wanted to associate. But the age at which Americans marry was rising steadily and the divorce rate was high. A more mobile workforce meant that single people often lived in cities they didn’t know and the chummy days when a father might set his daughter up with a junior colleague were over. Since Kremen started his company little has changed in the industry. Niche dating sites have proliferated, new technology has made new ways of meeting people possible and new gimmicks hit the market every day, but as I knew from my own experience, the fundamental characteristics of the online dating profile have remained static.
        
        At the same time big cities have a way of shrinking. In her essay about leaving New York Joan Didion tells a man she’ll take him to a party where he might meet some ‘new faces’, and he laughs at her. ‘It seemed that the last time he had gone to a party where he had been promised “new faces”, there had been 15 people in the room, and he had already slept with five of the women and owed money to all but two of the men.’ Didion doesn’t say, but I’ve always assumed her friend went to the party anyway.
        
        I joined OK Cupid at the age of 30, in late November 2011, with the pseudonym ‘viewfromspace’. When the time came to write the ‘About’ section of my profile, I quoted Didion’s passage, then added: ‘But now we have internet dating. New faces!’ The Didion bit sounded unpleasant, so I replaced it with a more optimistic statement, about internet dating restoring the city’s possibilities to a life that had become stagnant between work, subway and apartment. Then that sounded depressing, so I finally wrote: ‘I like watching nature documentaries and eating pastries.’ From then on I was flooded with suggestions of YouTube videos of endangered species and recommendations for pain au chocolat.
        
        OK Cupid was founded in 2004 by four maths majors from Harvard who were good at giving away things people were used to paying for (study guides, music). In 2011 they sold the company for $50 million to IAC, the corporation that now owns Match. Like Match, OK Cupid has its users fill out a questionnaire. The service then calculates a user’s ‘match percentage’ in relation to other users by collecting three values: the user’s answer to a question, how she would like someone else to answer the same question, and the importance of the question to her. These questions ranged from ‘Does smoking disgust you?’ to ‘How often do you masturbate?’ Many questions are specifically intended to gauge one’s interest in casual sex: ‘Regardless of future plans, what’s more interesting to you right now, sex or true love?’ ‘Would you consider sleeping with someone on the first date?’ ‘Say you’ve started seeing someone you really like. As far as you’re concerned, how long will it take before you have sex?’ I found these algorithms put me in the same area – social class and level of education – as the people I went on dates with, but otherwise did very little to predict whom I would like. One occurrence in both online and real-life dating was an inexplicable talent on my part for attracting vegetarians. I am not a vegetarian.
        
        I should note that I answered all the questions indicating an interest in casual sex in the negative, but that’s fairly common for women. The more an internet-dating site leads with the traditional signifiers of (male) sexual desire – pictures of women in their knickers, open hints about casual sex – the less likely women are to sign up for it. At a 51/49 male to female ratio, OK Cupid has a near parity many sites would envy. It’s not that women are averse to the possibility of a casual encounter (I would have been very happy had the right guy appeared), but they need some sort of alibi before they go looking. Kremen had also noticed this, and set up Match to look neutral and bland, with a heart-shaped logo.
        
        I wanted a boyfriend. I was also badly hung up on someone and wanted to stop thinking about him. People cheerily list their favourite movies and hope for the best, but darkness simmers beneath the chirpy surface. An extensive accrual of regrets lurks behind even the most well-adjusted profile. I read 19th-century novels to remind myself that sunny equanimity in the aftermath of heartbreak was not always the order of the day. On the other hand, online dating sites are the only places I’ve been where there’s no ambiguity of intention. A gradation of subtlety, sure: from the basic ‘You’re cute,’ to the off-putting ‘Hi there, would you like to come over, smoke a joint and let me take nude photos of you in my living room?’
        
        The largest free dating site in America is another algorithm-based service, Plenty of Fish, but in New York everyone I know uses OK Cupid, so that’s where I signed up. I also signed up to Match, but OK Cupid was the one I favoured, mostly because I got such constant and overwhelming attention from men there. The square-jawed bankers who reigned over Match, with their pictures of scuba diving in Bali and skiing in Aspen, paid me so little attention it made me feel sorry for myself. The low point came when I sent a digital wink to a man whose profile read, ‘I have a dimple on my chin,’ and included photos of him playing rugby and standing bare-chested on a deep-sea fishing vessel holding a mahi-mahi the size of a tricycle. He didn’t respond to my wink.
        
        I went to a lecture by the novelist Ned Beauman who compared the OK Cupid experience to Carl Sagan pondering the limits of our ability even to imagine non-carbon-based extraterrestrial life, let alone perceive when it was beaming signals to us. We troll on OK Cupid for what we think we want, but what if we are incapable of seeing the signals being sent to us, let alone interpreting them?
        
        OK Cupid gave the almost awe-inspiring impression of Kremen’s dream database: unlimited choice. There are drawbacks to this. As the sociologist Eva Illouz writes in Cold Intimacies, ‘the experience of romantic love is related to an economy of scarcity, which in turn enables novelty and excitement.’ In contrast, ‘the spirit presiding over the internet is that of an economy of abundance, where the self must choose and maximise its options and is forced to use techniques of cost-benefit and efficiency.’ At first it was exciting but after a couple of months the cracks began to show. What Beauman says about our inability to gauge what might be attractive turned out to be true. Consider the following.
        
        I went on a date with a classical composer who invited me to a John Cage concert at Juilliard. After the concert we looked for the bust of Béla Bartók on 57th Street. We couldn’t find it, but he told me how Bartók had died there of leukaemia. I wanted to like this man, who was excellent on paper, but I didn’t. I gave it another go. We went out for a second time to eat ramen in the East Village. I ended the night early. He next invited me to a concert at Columbia and then to dinner at his house. I said yes but I cancelled at the last minute, claiming illness and adding that I thought our dating had run its course. I was in fact sick, but he was angry with me. My cancellation, he wrote, had cost him a ‘ton of time shopping, cleaning and cooking that I didn’t really have to spare in the first place a few days before a deadline ...’ He punctuated almost exclusively with Pynchonian ellipses.
        
        I apologised, then stopped responding. In the months that followed he continued to write, long emails with updates of his life, and I continued not responding until it came to seem as if he was lobbing his sadness into a black hole, where I absorbed it into my own sadness.
        
        I went on a date with a furniture craftsman. We met at a coffee shop. It was a sunny afternoon in late February, but a strange snowfall began after we arrived, the flakes sparkling in the sun. The coffee shop was below ground, and we sat at a table by a window that put us just below two chihuahuas tied to a bench on the sidewalk outside. They shivered uncontrollably despite their fitted jackets. They looked down at us through the window, chewing on their leashes. The woodworker bought me a coffee and drank tea in a pint glass.
        
        Our conversation was strained. He seemed bored. His blue eyes shifted restlessly and he had a moustache. He had gone to a school for graphic design in Arizona. He showed me photos of furniture he made. He had calloused hands and was tall. He was attractive but dour and I wondered why: was it me, or a generalised posture against the world? We discovered we had been born in the same hospital, Allentown Hospital in Allentown, Pennsylvania, except that I was seven months older. In another era, the era when marriage was dictated by religion, family and the village, we might have had several children by now. Instead my parents had moved halfway across the country when I was three years old, he had stayed in Allentown until adulthood and now we both lived in bleak Bedford-Stuyvesant and were 30. He thought of himself as defiant, and loved being a craftsman only as much as he had hated working in an office. After drinking his tea, he went to the bathroom, came back and wordlessly put on his coat. I stood up and did the same. We walked up the stairs into the February wind. We said goodbye.
        
        I went on a date with a man who turned out to be a hairstylist who had attracted me with his Texas charm: ‘A nod and a bow, Ms Space,’ he had written. He arrived late to our date in Alphabet City, having accommodated some last-minute clients who wanted unscheduled blow-drys for their own dates. On either side of his neck he had tattoos of crossed scimitars. I asked him what the tattoos meant. He said they meant nothing. They were mistakes. He pushed up his sleeves and revealed more mistakes. As a teenager in Dallas he had let his friends use him as a training canvas. To call the tattoos mistakes seemed to be different from regretting them. He didn’t regret them. He said it was just that his 16-year-old self was giving him the finger. ‘You think you’ve changed,’ the 16-year-old version of him was saying through the tattoos: ‘Fuck you, I’m still here.’
        
        OK Cupid had another unintended effect, which was that in posting my profile, however pseudonymously, I had adorned myself with the equivalent of a ‘For Sale’ sign. Those who saw me on OK Cupid whom I knew in real life and who recognised my photo would often contact me: ‘I saw you on OK Cupid and I thought I would write.’ I went for Colombian food in Greenpoint with one of these. When I arrived my date was reading some documents that the National Security Agency had recently declassified to do with John Nash, the schizophrenic genius portrayed in A Beautiful Mind. We ordered arepas and beers. I liked this man. He had a job he loved at a blue-chip art gallery and lived in a spacious, high-ceiling apartment overlooking a tree-filled park with benches that formed a serpentine pattern. We talked about Cascadian black metal bands and the idea of resisting capitalism through unlistenable music and sustainable agriculture. We walked from Cafecito Bogotá back to his impeccable apartment, where he played ambient records and I petted his two cats. We decided to conduct an OK Cupid Locals experiment: he broadcast ‘Let’s lkjdlfjlsjdfijsflsjlj.’ I sat next to him on the couch. I refreshed my phone to see if his broadcast came up. It did. We looked at each other. He walked me to the train.
        
        Around this time I met someone in the real world. It didn’t work out, but it was a vivid enough reminder of what it feels like to want to sleep with someone and not even know what their favourite books are to make internet dating all but impossible for a while. The boredom returned, the ex-boyfriend resumed his place in the halls of memory. I went west and the walls of the all but unfurnished apartment in San Francisco loomed over me.
        
        Like most people I had started internet dating out of loneliness. I soon discovered, as most do, that it can only speed up the rate and increase the number of encounters with other single people, where each encounter is still a chance encounter. Internet dating destroyed my sense of myself as someone I both know and understand and can also put into words. It had a similarly harmful effect on my sense that other people can accurately know and describe themselves. It left me irritated with the whole field of psychology. I began responding only to people with very short profiles, then began forgoing the profiles altogether, using them only to see that people on OK Cupid Locals had a moderate grasp of the English language and didn’t profess rabidly right-wing politics.
        
        Internet dating alerted me to the fact that our notions of human behaviour and achievement, expressed in the agglomerative text of hundreds of internet dating profiles, are all much the same and therefore boring and not a good way to attract other people. The body, I also learned, is not a secondary entity. The mind contains very few truths that the body withholds. There is little of import in an encounter between two bodies that would fail to be revealed rather quickly. Until the bodies are introduced, seduction is only provisional.
        
        In the depths of loneliness, however, internet dating provided me with a lot of opportunities to go to a bar and have a drink with a stranger on nights that would otherwise have been spent unhappy and alone. I met all kinds of people: an X-ray technician, a green tech entrepreneur, a Polish computer programmer with whom I enjoyed a sort of chaste fondness over the course of several weeks. We were both shy and my feelings were tepid (as, I gathered, were his), but we went to the beach, he told me all about mushroom foraging in Poland, he ordered his vegetarian burritos in Spanish, and we shared many mutual dislikes.
        
        As for that night in San Francisco, I responded to an online beacon, and I went for a drink with a stranger. We kissed, he showed me his special collection of marijuana plants, and we talked about Brazil. Then I went home and never spoke to him again.`
    },
    {
        title:'Forty years of the internet: how the world changed for ever',
        author:'Oliver Burkeman',
        article:`In October 1969, a student typed 'LO' on a computer - and the internet was born

        Towards the end of the summer of 1969 – a few weeks after the moon landings, a few days after Woodstock, and a month before the first broadcast of Monty Python's Flying Circus – a large grey metal box was delivered to the office of Leonard Kleinrock, a professor at the University of California in Los Angeles. It was the same size and shape as a household refrigerator, and outwardly, at least, it had about as much charm. But Kleinrock was thrilled: a photograph from the time shows him standing beside it, in requisite late-60s brown tie and brown trousers, beaming like a proud father.
        Had he tried to explain his excitement to anyone but his closest colleagues, they probably wouldn't have understood. The few outsiders who knew of the box's existence couldn't even get its name right: it was an IMP, or "interface message processor", but the year before, when a Boston company had won the contract to build it, its local senator, Ted Kennedy, sent a telegram praising its ecumenical spirit in creating the first "interfaith message processor". Needless to say, though, the box that arrived outside Kleinrock's office wasn't a machine capable of fostering understanding among the great religions of the world. It was much more important than that.

        
        It's impossible to say for certain when the internet began, mainly because nobody can agree on what, precisely, the internet is. (This is only partly a philosophical question: it is also a matter of egos, since several of the people who made key contributions are anxious to claim the credit.) But 29 October 1969 – 40 years ago next week – has a strong claim for being, as Kleinrock puts it today, "the day the infant internet uttered its first words". At 10.30pm, as Kleinrock's fellow professors and students crowded around, a computer was connected to the IMP, which made contact with a second IMP, attached to a second computer, several hundred miles away at the Stanford Research Institute, and an undergraduate named Charley Kline tapped out a message. Samuel Morse, sending the first telegraph message 125 years previously, chose the portentous phrase: "What hath God wrought?" But Kline's task was to log in remotely from LA to the Stanford machine, and there was no opportunity for portentousness: his instructions were to type the command LOGIN.

        To say that the rest is history is the emptiest of cliches – but trying to express the magnitude of what began that day, and what has happened in the decades since, is an undertaking that quickly exposes the limits of language. It's interesting to compare how much has changed in computing and the internet since 1969 with, say, how much has changed in world politics. Consider even the briefest summary of how much has happened on the global stage since 1969: the Vietnam war ended; the cold war escalated then declined; the Berlin Wall fell; communism collapsed; Islamic fundamentalism surged. And yet nothing has quite the power to make people in their 30s, 40s or 50s feel very old indeed as reflecting upon the growth of the internet and the world wide web. Twelve years after Charley Kline's first message on the Arpanet, as it was then known, there were still only 213 computers on the network; but 14 years after that, 16 million people were online, and email was beginning to change the world; the first really usable web browser wasn't launched until 1993, but by 1995 we had Amazon, by 1998 Google, and by 2001, Wikipedia, at which point there were 513 million people online. Today the figure is more like 1.7 billion.

        Unless you are 15 years old or younger, you have lived through the dotcom bubble and bust, the birth of Friends Reunited and Craigslist and eBay and Facebook and Twitter, blogging, the browser wars, Google Earth, filesharing controversies, the transformation of the record industry, political campaigning, activism and campaigning, the media, publishing, consumer banking, the pornography industry, travel agencies, dating and retail; and unless you're a specialist, you've probably only been following the most attention-grabbing developments. Here's one of countless statistics that are liable to induce feelings akin to vertigo: on New Year's Day 1994 – only yesterday, in other words – there were an estimated 623 websites. In total. On the whole internet. "This isn't a matter of ego or crowing," says Steve Crocker, who was present that day at UCLA in 1969, "but there has not been, in the entire history of mankind, anything that has changed so dramatically as computer communications, in terms of the rate of change."

        Looking back now, Kleinrock and Crocker are both struck by how, as young computer scientists, they were simultaneously aware that they were involved in something momentous and, at the same time, merely addressing a fairly mundane technical problem. On the one hand, they were there because of the Russian Sputnik satellite launch, in 1957, which panicked the American defence establishment, prompting Eisenhower to channel millions of dollars into scientific research, and establishing Arpa, the Advanced Research Projects Agency, to try to win the arms technology race. The idea was "that we would not get surprised again," said Robert Taylor, the Arpa scientist who secured the money for the Arpanet, persuading the agency's head to give him a million dollars that had been earmarked for ballistic missile research. With another pioneer of the early internet, JCR Licklider, Taylor co-wrote the paper, "The Computer As A Communication Device", which hinted at what was to come. "In a few years, men will be able to communicate more effectively through a machine than face to face," they declared. "That is rather a startling thing to say, but it is our conclusion."

        On the other hand, the breakthrough accomplished that night in 1969 was a decidedly down-to-earth one. The Arpanet was not, in itself, intended as some kind of secret weapon to put the Soviets in their place: it was simply a way to enable researchers to access computers remotely, because computers were still vast and expensive, and the scientists needed a way to share resources. (The notion that the network was designed so that it would survive a nuclear attack is an urban myth, though some of those involved sometimes used that argument to obtain funding.) The technical problem solved by the IMPs wasn't very exciting, either. It was already possible to link computers by telephone lines, but it was glacially slow, and every computer in the network had to be connected, by a dedicated line, to every other computer, which meant you couldn't connect more than a handful of machines without everything becoming monstrously complex and costly. The solution, called "packet switching" – which owed its existence to the work of a British physicist, Donald Davies – involved breaking data down into blocks that could be routed around any part of the network that happened to be free, before getting reassembled at the other end.

        "I thought this was important, but I didn't really think it was as challenging as what I thought of as the 'real research'," says Crocker, a genial Californian, now 65, who went on to play a key role in the expansion of the internet. "I was particularly fascinated, in those days, by artificial intelligence, and by trying to understand how people think. I thought that was a much more substantial and respectable research topic than merely connecting up a few machines. That was certainly useful, but it wasn't art."

        Still, Kleinrock recalls a tangible sense of excitement that night as Kline sat down at the SDS Sigma 7 computer, connected to the IMP, and at the same time made telephone contact with his opposite number at Stanford. As his colleagues watched, he typed the letter L, to begin the word LOGIN.

        "Have you got the L?" he asked, down the phone line. "Got the L," the voice at Stanford responded.

        Kline typed an O. "Have you got the O?"

        "Got the O," Stanford replied.

        Kline typed a G, at which point the system crashed, and the connection was lost. The G didn't make it through, which meant that, quite by accident, the first message ever transmitted across the nascent internet turned out, after all, to be fittingly biblical:

        "LO."

        Frenzied visions of a global conscious brain

        One of the most intriguing things about the growth of the internet is this: to a select group of technological thinkers, the surprise wasn't how quickly it spread across the world, remaking business, culture and politics – but that it took so long to get off the ground. Even when computers were mainly run on punch-cards and paper tape, there were whispers that it was inevitable that they would one day work collectively, in a network, rather than individually. (Tracing the origins of online culture even further back is some people's idea of an entertaining game: there are those who will tell you that the Talmud, the book of Jewish law, contains a form of hypertext, the linking-and-clicking structure at the heart of the web.) In 1945, the American presidential science adviser, Vannevar Bush, was already imagining the "memex", a device in which "an individual stores all his books, records, and communications", which would be linked to each other by "a mesh of associative trails", like weblinks. Others had frenzied visions of the world's machines turning into a kind of conscious brain. And in 1946, an astonishingly complete vision of the future appeared in the magazine Astounding Science Fiction. In a story entitled A Logic Named Joe, the author Murray Leinster envisioned a world in which every home was equipped with a tabletop box that he called a "logic":

        "You got a logic in your house. It looks like a vision receiver used to, only it's got keys instead of dials and you punch the keys for what you wanna get . . . you punch 'Sally Hancock's Phone' an' the screen blinks an' sputters an' you're hooked up with the logic in her house an' if somebody answers you got a vision-phone connection. But besides that, if you punch for the weather forecast [or] who was mistress of the White House durin' Garfield's administration . . . that comes on the screen too. The relays in the tank do it. The tank is a big buildin' full of all the facts in creation . . . hooked in with all the other tanks all over the country . . . The only thing it won't do is tell you exactly what your wife meant when she said, 'Oh, you think so, do you?' in that peculiar kinda voice "

        Despite all these predictions, though, the arrival of the internet in the shape we know it today was never a matter of inevitability. It was a crucial idiosyncracy of the Arpanet that its funding came from the American defence establishment – but that the millions ended up on university campuses, with researchers who embraced an anti-establishment ethic, and who in many cases were committedly leftwing; one computer scientist took great pleasure in wearing an anti-Vietnam badge to a briefing at the Pentagon. Instead of smothering their research in the utmost secrecy – as you might expect of a cold war project aimed at winning a technological battle against Moscow – they made public every step of their thinking, in documents known as Requests For Comments.

        Deliberately or not, they helped encourage a vibrant culture of hobbyists on the fringes of academia – students and rank amateurs who built their own electronic bulletin-board systems and eventually FidoNet, a network to connect them to each other. An argument can be made that these unofficial tinkerings did as much to create the public internet as did the Arpanet. Well into the 90s, by the time the Arpanet had been replaced by NSFNet, a larger government-funded network, it was still the official position that only academic researchers, and those affiliated to them, were supposed to use the network. It was the hobbyists, making unofficial connections into the main system, who first opened the internet up to allcomers.

        What made all of this possible, on a technical level, was simultaneously the dullest-sounding and most crucial development since Kleinrock's first message. This was the software known as TCP/IP, which made it possible for networks to connect to other networks, creating a "network of networks", capable of expanding virtually infinitely – which is another way of defining what the internet is. It's for this reason that the inventors of TCP/IP, Vint Cerf and Bob Kahn, are contenders for the title of fathers of the internet, although Kleinrock, understandably, disagrees. "Let me use an analogy," he says. "You would certainly not credit the birth of aviation to the invention of the jet engine. The Wright Brothers launched aviation. Jet engines greatly improved things."

        The spread of the internet across the Atlantic, through academia and eventually to the public, is a tale too intricate to recount here, though it bears mentioning that British Telecom and the British government didn't really want the internet at all: along with other European governments, they were in favour of a different networking technology, Open Systems Interconnect. Nevertheless, by July 1992, an Essex-born businessman named Cliff Stanford had opened Demon Internet, Britain's first commercial internet service provider. Officially, the public still wasn't meant to be connecting to the internet. "But it was never a real problem," Stanford says today. "The people trying to enforce that weren't working very hard to make it happen, and the people working to do the opposite were working much harder." The French consulate in London was an early customer, paying Demon £10 a month instead of thousands of pounds to lease a private line to Paris from BT.

        After a year or so, Demon had between 2,000 and 3,000 users, but they weren't always clear why they had signed up: it was as if they had sensed the direction of the future, in some inchoate fashion, but hadn't thought things through any further than that. "The question we always got was: 'OK, I'm connected – what do I do now?'" Stanford recalls. "It was one of the most common questions on our support line. We would answer with 'Well, what do you want to do? Do you want to send an email?' 'Well, I don't know anyone with an email address.' People got connected, but they didn't know what was meant to happen next."

        Fortunately, a couple of years previously, a British scientist based at Cern, the physics laboratory outside Geneva, had begun to answer that question, and by 1993 his answer was beginning to be known to the general public. What happened next was the web.

        The birth of the web

        I sent my first email in 1994, not long after arriving at university, from a small, under-ventilated computer room that smelt strongly of sweat. Email had been in existence for decades by then – the @ symbol was introduced in 1971, and the first message, according to the programmer who sent it, Ray Tomlinson, was "something like QWERTYUIOP". (The test messages, Tomlinson has said, "were entirely forgettable, and I have, therefore, forgotten them".) But according to an unscientific poll of friends, family and colleagues, 1994 seems fairly typical: I was neither an early adopter nor a late one. A couple of years later I got my first mobile phone, which came with two batteries: a very large one, for normal use, and an extremely large one, for those occasions on which you might actually want a few hours of power. By the time I arrived at the Guardian, email was in use, but only as an add-on to the internal messaging system, operated via chunky beige terminals with green-on-black screens. It took for ever to find the @ symbol on the keyboard, and I don't remember anything like an inbox, a sent-mail folder, or attachments. I am 34 years old, but sometimes I feel like Methuselah.

        I have no recollection of when I first used the world wide web, though it was almost certainly when people still called it the world wide web, or even W3, perhaps in the same breath as the phrase "information superhighway", made popular by Al Gore. (Or "infobahn": did any of us really, ever, call the internet the "infobahn"?) For most of us, though, the web is in effect synonymous with the internet, even if we grasp that in technical terms that's inaccurate: the web is simply a system that sits on top of the internet, making it greatly easier to navigate the information there, and to use it as a medium of sharing and communication. But the distinction rarely seems relevant in everyday life now, which is why its inventor, Tim Berners-Lee, has his own legitimate claim to be the progenitor of the internet as we know it. The first ever website was his own, at CERN: info.cern.ch.

        The idea that a network of computers might enable a specific new way of thinking about information, instead of just allowing people to access the data on each other's terminals, had been around for as long as the idea of the network itself: it's there in Vannevar Bush's memex, and Murray Leinster's logics. But the grandest expression of it was Project Xanadu, launched in 1960 by the American philosopher Ted Nelson, who imagined – and started to build – a vast repository for every piece of writing in existence, with everything connected to everything else according to a principle he called "transclusion". It was also, presciently, intended as a method for handling many of the problems that would come to plague the media in the age of the internet, automatically channelling small royalties back to the authors of anything that was linked. Xanadu was a mind-spinning vision – and at least according to an unflattering portrayal by Wired magazine in 1995, over which Nelson threatened to sue, led those attempting to create it into a rabbit-hole of confusion, backbiting and "heart-slashing despair". Nelson continues to develop Xanadu today, arguing that it is a vastly superior alternative to the web. "WE FIGHT ON," the Xanadu website declares, sounding rather beleaguered, not least since the declaration is made on a website.

        Web browsers crossed the border into mainstream use far more rapidly than had been the case with the internet itself: Mosaic launched in 1993 and Netscape followed soon after, though it was an embarrassingly long time before Microsoft realised the commercial necessity of getting involved at all. Amazon and eBay were online by 1995. And in 1998 came Google, offering a powerful new way to search the proliferating mass of information on the web. Until not too long before Google, it had been common for search or directory websites to boast about how much of the web's information they had indexed – the relic of a brief period, hilarious in hindsight, when a user might genuinely have hoped to check all the webpages that mentioned a given subject. Google, and others, saw that the key to the web's future would be helping users exclude almost everything on any given topic, restricting search results to the most relevant pages.

        Without most of us quite noticing when it happened, the web went from being a strange new curiosity to a background condition of everyday life: I have no memory of there being an intermediate stage, when, say, half the information I needed on a particular topic could be found online, while the other half still required visits to libraries. "I remember the first time I saw a web address on the side of a truck, and I thought, huh, OK, something's happening here," says Spike Ilacqua, who years beforehand had helped found The World, the first commercial internet service provider in the US. Finally, he stopped telling acquaintances that he worked in "computers", and started to say that he worked on "the internet", and nobody thought that was strange.

        It is absurd – though also unavoidable here – to compact the whole of what happened from then onwards into a few sentences: the dotcom boom, the historically unprecedented dotcom bust, the growing "digital divide", and then the hugely significant flourishing, over the last seven years, of what became known as Web 2.0. It is only this latter period that has revealed the true capacity of the web for "generativity", for the publishing of blogs by anyone who could type, for podcasting and video-sharing, for the undermining of totalitarian regimes, for the use of sites such as Twitter and Facebook to create (and ruin) friendships, spread fashions and rumours, or organise political resistance. But you almost certainly know all this: it's part of what these days, in many parts of the world, we call "just being alive".

        The most confounding thing of all is that in a few years' time, all this stupendous change will probably seem like not very much change at all. As Crocker points out, when you're dealing with exponential growth, the distance from A to B looks huge until you get to point C, whereupon the distance between A and B looks like almost nothing; when you get to point D, the distance between B and C looks similarly tiny. One day, presumably, everything that has happened in the last 40 years will look like early throat-clearings — mere preparations for whatever the internet is destined to become. We will be the equivalents of the late-60s computer engineers, in their horn-rimmed glasses, brown suits, and brown ties, strange, period-costume characters populating some dimly remembered past.

        Will you remember when the web was something you accessed primarily via a computer? Will you remember when there were places you couldn't get a wireless connection? Will you remember when "being on the web" was still a distinct concept, something that described only a part of your life, instead of permeating all of it? Will you remember Google?`

    },
    {
        title:'Who decides what words mean',
        author:'Lane Greene',
        article:`Bound by rules, yet constantly changing, language might be the ultimate self-regulating system, with nobody in charge

        Decades before the rise of social media, polarisation plagued discussions about language. By and large, it still does. Everyone who cares about the topic is officially required to take one of two stances. Either you smugly preen about the mistakes you find abhorrent – this makes you a so-called prescriptivist – or you show off your knowledge of language change, and poke holes in the prescriptivists’ facts – this makes you a descriptivist. Group membership is mandatory, and the two are mutually exclusive.

        But it doesn’t have to be this way. I have two roles at my workplace: I am an editor and a language columnist. These two jobs more or less require me to be both a prescriptivist and a descriptivist. When people file me copy that has mistakes of grammar or mechanics, I fix them (as well as applying The Economist’s house style). But when it comes time to write my column, I study the weird mess of real language; rather than being a scold about this or that mistake, I try to teach myself (and so the reader) something new. Is this a split personality, or can the two be reconciled into a coherent philosophy? I believe they can.

        Language changes all the time. Some changes really are chaotic, and disruptive. Take decimate, a prescriptivist shibboleth. It comes from the old Roman practice of punishing a mutinous legion by killing every 10th soldier (hence that deci­- root). Now we don’t often need a word for destroying exactly a 10th of something – this is the ‘etymological fallacy’, the idea that a word must mean exactly what its component roots indicate. But it is useful to have a word that means to destroy a sizeable proportion of something. Yet many people have extended the meaning of decimate until now it means something approaching ‘to wipe out utterly’.

        Descriptivists – that is, virtually all academic linguists – will point out that semantic creep is how languages work. It’s just something words do: look up virtually any nontechnical word in the great historical Oxford English Dictionary (OED), which lists a word’s senses in historical order. You’ll see things such as the extension of decimate happening again and again and again. Words won’t sit still. The prescriptivist position, offered one linguist, is like taking a snapshot of the surface of the ocean and insisting that’s how ocean surfaces must look.

        Be that as it may, retort prescriptivists, but that doesn’t make it any less annoying. Decimate doesn’t have a good synonym in its traditional meaning (to destroy a portion of), and it has lots of company in its new meaning: destroy, annihilate, devastate and so on. If decimate eventually settles on this latter meaning, we lose a unique word and gain nothing. People who use it the old way and people who use it the new way can also confuse each other.

        Or take literally, on which I am a traditionalist. It is a delight to be able to use a good literally: when my son fell off a horse on a recent holiday, I was able to reassure my mother that ‘He literally got right back in the saddle,’ and this pleased me no end. So when people use literally to say, for example, We literally walked a million miles, I sigh a little sigh. I know that James Joyce, Vladimir Nabokov and many others used a figurative literally, but as a mere intensifier it’s not particularly useful or lovely, and it is particularly useful and lovely in the traditional sense, where it has no good substitute.

        So I do believe that when change happens in a language it can do harm. Not the end of the world, but harm.

        There is another fact to bear in mind: no language has fallen apart from lack of care. It is just not something that happens – literally. Prescriptivists cannot point to a single language that became unusable or inexpressive as a result of people’s failure to uphold traditional vocabulary and grammar. Every language existing today is fantastically expressive. It would be a miracle, except that it is utterly commonplace, a fact shared not only by all languages but by all the humans who use them.

        How can this be? Why does change of the decimate variety not add up to chaos? If one such ‘error’ is bad, and these kinds of things are happening all the time, how do things manage to hold together?

        The answer is that language is a system. Sounds, words and grammar do not exist in isolation: each of these three levels of language constitutes a system in itself. And, extraordinarily, these systems change as systems. If one change threatens disruption, another change compensates, so that the new system, though different from the old, is still an efficient, expressive and useful whole.

        Begin with sounds. Every language has a characteristic inventory of contrasting sounds, called phonemes. Beet and bit have different vowels; these are two phonemes in English. Italian has only one, which is why Italians tend to make homophones of sheet and shit.

        There is something odd about the vowels of English. Have you ever noticed that every language in Europe seems to use the letter A the same way? From latte to lager to tapas, Italian, German and Spanish all seem to use it for the ah sound. And at some level, this seems natural; if you learn frango is ‘chicken’ in Portuguese, you will probably know to pronounce it with an ah, not an ay. How, then, did English get A to sound like it does in plate, name, face and so on?

        Look around the other ‘long’ vowels in English, and they seem out of whack in similar ways. The letter I has an ee sound from Nice to Nizhni Novgorod; why does it have the sound it does in English write and ride? And why do two Os yield the sound they do in boot and food?

        Nobody in a 15th-century tavern (men carried knives back then) wants to confuse meet, meat and mate

        The answer is the Great Vowel Shift. From the middle English period and continuing into the early modern era, the entire set of English long vowels underwent a radical disruption. Meet used to be pronounced a bit like modern mate. Boot used to sound like boat. (But both vowels were monophthongs, not diphthongs; the modern long A is really pronounced like ay-ee said quickly, but the vowel in medieval meet was a pure single vowel.)

        During the Great Vowel Shift, ee and oo started to move towards the sounds they have today. Nobody knows why. It’s likely that some people noticed at the time and groused about it. In any case, there was really a problem: now ee was too close to the vowel in time, which in that era was pronounced tee-muh. And oo was too close to the vowel in house, which was then pronounced hoose.

        Speakers didn’t passively accept the confusion. What happened next shows the genius of what economists call spontaneous order. In response to their new pushy neighbours in the vowel space, the vowels in time and house started to change, too, becoming something like tuh-eem and huh-oos. Other changes prompted yet more changes, too: the vowel in mate – then pronounced mah-tuh – moved towards the sound of the modern vowel in cat. That made it a little too close to meat, which was pronounced like a drawn-out version of the modern met. So the vowel in meat changed too.

        Throughout the system, vowels were on the move. Nobody in a 15th-century tavern (men carried knives back then) wants to confuse meet, meat and mate. So they responded to a potentially damaging change by changing something else. A few vowels ended up merging. So meet and meat became homophones. But mostly the system just settled down with each vowel in a new place. It was the Great Vowel Shift, not the Great Vowel Pile-Up.

        Such shifts are common enough that they have earned a name: ‘chain shifts’. These are what happens when one change prompts another, which in turn prompts yet another, and so on, until the language arrives at a new equilibrium. There is a chain shift underway now: the Northern Cities Shift, noticed and described in the cities around the Great Lakes of North America by William Labov, the pioneer of sociolinguistics. There is also a California Shift. In other words, these things happen. The local, individual change is chaotic and random, but the system responds to keep things from coming to harm.

        What about words? There are only so many vowels in a language, but many thousands of words. So changes in the meanings of words might not be as orderly as the chain shifts seen in the Great Vowel Shift and others. Nonetheless, despite potential harm done by an individual word’s change in meaning, cultures tend to have all the words they need for all the things they want to talk about.

        In researching Samuel Johnson’s dictionary for my new book, Talk on the Wild Side (2018), I made a startling find. Johnson, in describing his plan for the dictionary to the Earl of Chesterfield in 1747, wrote that

        [B]uxom, which means only obedient, is now made, in familiar phrases, to stand for wanton; because in an ancient form of marriage, before the Reformation, the bride promised complaisance and obedience, in these terms: ‘I will be bonair and buxom in bed and at board.’
        When most people think of buxom today, neither ‘obedient’ nor ‘wanton’ is what comes to mind (To my wife: this is why a Google Images search for buxom is in my search history, I promise.)

        Turning to the OED, I found that buxom had come from a medieval word buhsam, cognate to the modern German biegsam, or ‘bendable’. From physical to metaphorical (the natural extension), it came to mean ‘pliable’ of a person, or – as Johnson put it – obedient. Then buxom kept on moving: a short hop from ‘obedient’ to ‘amiable’, and then another one to ‘lively, gay’. (William Shakespeare describes a soldier of ‘buxom valour’ in Henry V.) From there, it is another short jump to ‘healthy, vigorous’, which seems to have been the current meaning around Johnson’s time. From ‘good health’ it was another logical extension to physical plumpness, then to plumpness specifically on a woman, to big-breasted.

        The leap from ‘obedient’ to ‘busty’ seems extraordinary until we look at it step by step. Nice used to mean ‘foolish’. Silly used to mean ‘holy’. Assassin is from the plural of the Arabic word for ‘hashish(-eater)’, and magazine from the Arabic word for a storehouse. This is just what words do. Prestigious used to be pejorative, meaning glittery but not substantive. These kinds of changes are common.

        I don’t know how we did without hangry so long in English, because I spent about a third of every day hangry

        Two paragraphs ago, I used the words ‘leap’ and ‘jump’. But we see the ‘leaps’ only when lexicographers, looking back, chop up a word’s history into meanings for their dictionaries. Words change meaning gradually, as a small number of speakers use them in a new way, and they in turn cause others to do so. This is how words can change meaning so totally and utterly; mostly, they do so in steps too small to notice.

        Again, no chaos results. Every time buxom changed meaning, it could have theoretically left a hole in the lexicon for the meaning it had left behind. But in each case, another word filled its place: in fact, the ones I have used above (pliable, obedient, amiable, lively, gay, healthy, plump and so on). For useful concepts, it seems, the lexicon abhors a vacuum. (I don’t know how we did without hangry so long in English, because I spent about a third of every day hangry. But sure enough, someone coined it.)

        There are several predictable ways that words change meaning. Some people insist that nauseous means only ‘causing nausea’. But going from cause to experiencer is a common semantic shift, just as many words can be used in both active and agentless constructions (consider I broke the dishwasher and The dishwasher broke). Yet true confusion is rare. For nauseous’s old meaning we have nauseating.

        Words also weaken with frequent use: The Lego Movie (2014) was on to something with its song ‘Everything Is Awesome’, because Americans really do use this word rather a lot. Once powerful, it can now be used for anything even slightly good, as in This burrito is awesome. It can even be near-meaningless, as in Steven Pinker’s lovely example: ‘If you could pass the guacamole, that would be awesome.’

        But do we really lack ways of communicating that we’re impressed by something? No language does, and English-speakers are spoiled for choice from the likes of incredible, fantastic, stupendous and brilliant. (All of which have changed from their etymological meanings of ‘unbelievable’, ‘like a fantasy’, ‘inducing stupor’ and ‘shiny, reflective’, by the way.) When those get overused (and all are in danger of that), people coin new ones still: sick, amazeballs, kick-ass.

        The thousands of words in the language are a swirling mass constantly on the move. Again, when one piece moves, threatening a gap or an overlap, something else moves too. The individual, short-term change is random; the overall, long-term change is systemic.

        At the level of grammar, change might seem the most unsettling, threatening a deeper kind of harm than a simple mispronunciation or new use for an old word. Take the long-term decline of whom, which signals that something in a question or relative clause is an object (direct or indirect), as in That’s the man whom I saw. Most people today would either say That’s the man who I saw or just That’s the man I saw.

        What word is the subject in a clause, and what is the object, is a deeply important fact. And yet, precisely because this is so, even radical grammatical change leaves this distinction intact. Readers of Beowulf are in no doubt that virtually every word in that epic poem is vastly different from its modern counterpart. What those who can’t read Old English might not realise is how different the grammar is. English was a language like Russian or Latin: it had case endings everywhere: on nouns, adjectives and determiners (words such as the and a). In other words, they all behaved like who/whom/whose does (there was even a fourth case).

        Today, just six words (I, he, she, we, they and who) change form when they are direct or indirect objects (me, him, her, us, them and whom). In a longer view, modern Anglophones speak godawful, brokendown Anglo-Saxon, lacking all the communicative power that those endings provided. How, one can imagine Alfred the Great asking, do English-speakers know what is the subject of a sentence and what are the objects without those crucial case endings?

        The answer is boring: word order. English is a subject-verb-object language. In I love her, case is evident by the form of I (a subject, in the nominative case) and her (a direct object, in the objective case). But the meaning of Steve loves Sally is just as clear, despite the lack of case endings. Subject-verb-object order can be violated in special circumstances (Her I love the most) but it is expected; and that expectation, shared by all native speakers, does the work that the case endings once did.

        To my six-year-old, everything is epic, which strikes my ear as awesome must have done my parents’

        Why did the case endings disappear? We don’t know, but it was probably sped up as a result of two waves of conquest: adult Vikings and Normans coming to Britain, and learning Anglo-Saxon imperfectly. Then as now, things such as fiddly inflections are hard for adults to learn in a foreign language. Many adult learners would have neglected all those endings and relied on word order, raising children who heard their parents’ slightly stripped-down version. The children would then have used the endings less than earlier generations, until they disappeared entirely.

        Once again, the grammar responded as a system. No civilisation can afford to leave the distinction between subjects and objects to guesswork. Word order was relatively flexible in the Anglo-Saxon period. Then the loss of case endings fixed it in more rigid form. The gradual disappearance of case signalling resulted in a potential loss of information, but the solidification of word order made up for it.

        We now have a framework in which both the prescriptivists and the descriptivists can have their say. Sound changes can be seen as wrong, understandably, by people who learned an older pronunciation: to my ear, nucular sounds uneducated and expresso is just wrong. But in the long run, sound systems make up for any confusion in a delicate dance of changes that makes sure the language’s necessary distinctions remain. Word meanings change, by both type (a change in meaning) and by force (a change in how powerful a word is). To my six-year-old, everything is epic, which strikes my ear the way awesome must have done to my parents. A lunch just cannot be epic. But when epic is exhausted, his kids will press something else into service – or coin something new.

        Even the deepest-seeming change – to the grammar – never destroys the language system. Some distinctions can disappear: classical Arabic has singular, dual and plural number; the modern dialects mostly use just singular and plural, like English. Latin was full of cases; its daughter languages – French, Spanish and so on – lack them, but their speakers get on with life just the same. Sometimes languages get more complex: the Romance languages also pressed freestanding Latin words into service until they wore down and became mere endings on verbs. That turned out OK, too.

        Spontaneous order doesn’t sit well with people. We are all tempted to think that complex systems need management, a benign but firm hand. But just as market economies turn out better than command economies, languages are too complex, and used by too many people, to submit to command management. Individual decisions can be bad ones, and merit correction, but we can be optimistic that, in the long run, change is inevitable and it will turn out all right. Broadly trusting the distributed intelligence of your fellow humans to keep things in order can be hard to do, but it’s the only way to go. Language is self-regulating. It’s a genius system – with no genius.`

    },
    {
        title:'7 Ways to Retain More of Every Book You Read',
        author:'James Clear',
        article:`There are many benefits to reading more books, but perhaps my favorite is this: A good book can give you a new way to interpret your past experiences.

        Whenever you learn a new mental model or idea, it's like the “software” in your brain gets updated. Suddenly, you can run all of your old data points through a new program. You can learn new lessons from old moments. As Patrick O'Shaughnessy says, “Reading changes the past.”
        
        Of course, this is only true if you internalize and remember insights from the books you read. Knowledge will only compound if it is retained. In other words, what matters is not simply reading more books, but getting more out of each book you read.
        
        Gaining knowledge is not the only reason to read, of course. Reading for pleasure or entertainment can be a wonderful use of time, but this article is about reading to learn. With that in mind, I'd like to share some of the best reading comprehension strategies I’ve found.
        1. Quit More Books

        It doesn't take long to figure out if something is worth reading. Skilled writing and high-quality ideas stick out.
        
        As a result, most people should probably start more books than they do. This doesn't mean you need to read each book page-by-page. You can skim the table of contents, chapter titles, and subheadings. Pick an interesting section and dive in for a few pages. Maybe flip through the book and glance at any bolded points or tables. In ten minutes, you'll have a reasonable idea of how good it is.
        
        Then comes the crucial step: Quit books quickly and without guilt or shame.
        
        Life is too short to waste it on average books. The opportunity cost is too high. There are so many amazing things to read. I think Patrick Collison, the founder of Stripe, put it nicely when he said, “Life is too short to not read the very best book you know of right now.”
        
        Here's my recommendation:
        
        Start more books. Quit most of them. Read the great ones twice.
        
        2. Choose Books You Can Use Instantly

        One way to improve reading comprehension is to choose books you can immediately apply. Putting the ideas you read into action is one of the best ways to secure them in your mind. Practice is a very effective form of learning.
        
        Choosing a book that you can use also provides a strong incentive to pay attention and remember the material. That’s particularly true when something important hangs in the balance. If you’re starting a business, for example, then you have a lot of motivation to get everything you can out of the sales book you’re reading. Similarly, someone who works in biology might read The Origin of Species more carefully than a random reader because it connects directly to their daily work.
        
        Of course, not every book is a practical, how-to guide that you can apply immediately, and that's fine. You can find wisdom in many different books. But I do find that I'm more likely to remember books that are relevant to my daily life.
        
        3. Create Searchable Notes

        Keep notes on what you read. You can do this however you like. It doesn't need to be a big production or a complicated system. Just do something to emphasize the important points and passages.
        
        I do this in different ways depending on the format I'm consuming. I highlight passages when reading on Kindle. I type out interesting quotes as I listen to audiobooks. I dog-ear pages and transcribe notes when reading a print book.
        
        But here's the real key: store your notes in a searchable format.
        
        There is no need to leave the task of reading comprehension solely up to your memory. I keep my notes in Evernote. I prefer Evernote over other options because 1) it is instantly searchable, 2) it is easy to use across multiple devices, and 3) you can create and save notes even when you're not connected to the internet.
        
        I get my notes into Evernote in three ways:
        
        I. Audiobook: I create a new Evernote file for each book and then type my notes directly into that file as I listen.
        
        II. Ebook: I highlight passages on my Kindle Paperwhite and use a program called Clippings to export all of my Kindle highlights directly into Evernote. Then, I add a summary of the book and any additional thoughts before posting it to my book summaries page.
        
        III. Print: Similar to my audiobook strategy, I type my notes as I read. If I come across a longer passage I want to transcribe, I place the book on a book stand as I type. (Typing notes while reading a print book can be annoying because you are always putting the book down and picking it back up, but this is the best solution I've found.)
        
        Of course, your notes don't have to be digital to be “searchable.” For example, you can use Post-It Notes to tag certain pages for future reference. As another option, Ryan Holiday suggests storing each note on an index card and categorizing them by the topic or book.
        
        The core idea is the same: Keeping searchable notes is essential for returning to ideas easily. An idea is only useful if you can find it when you need it.
        
        4. Combine Knowledge Trees

        One way to imagine a book is like a knowledge tree with a few fundamental concepts forming the trunk and the details forming the branches. You can learn more and improve reading comprehension by “linking branches” and integrating your current book with other knowledge trees.
        
        For example:
        
        While reading The Tell-Tale Brain by neuroscientist V.S. Ramachandran, I discovered that one of his key points connected to a previous idea I learned from social work researcher Brené Brown.
        In my notes for The Subtle Art of Not Giving a F*ck, I noted how Mark Manson's idea of “killing yourself” overlaps with Paul Graham's essay on keeping your identity small.
        As I read Mastery by George Leonard, I realized that while this book was about the process of improvement, it also shed some light on the connection between genetics and performance.
        I added each insight to my notes for that particular book.
        
        Connections like these help you remember what you read by “hooking” new information onto concepts and ideas you already understand. As Charlie Munger says, “If you get into the mental habit of relating what you’re reading to the basic structure of the underlying ideas being demonstrated, you gradually accumulate some wisdom.”
        
        When you read something that reminds you of another topic or immediately sparks a connection or idea, don’t allow that thought to come and go without notice. Write about what you’ve learned and how it connects to other ideas.
        
        5. Write a Short Summary

        As soon as I finish a book, I challenge myself to summarize the entire text in just three sentences. This constraint is just a game, of course, but it forces me to consider what was really important about the book.
        
        Some questions I consider when summarizing a book include:
        
        What are the main ideas?
        If I implemented one idea from this book right now, which one would it be?
        How would I describe the book to a friend?
        In many cases, I find that I can usually get just as much useful information from reading my one-paragraph summary and reviewing my notes as I would if I read the entire book again.
        
        If you feel like you can’t squeeze the whole book into three sentences, consider using the Feynman Technique.
        
        The Feynman Technique is a note-taking strategy named after the Nobel Prize-winning physicist Richard Feynman. It’s pretty simple: Write the name of the book at the top of a blank sheet of paper, then write down how you’d explain the book to someone who had never heard of it.
        
        If you find yourself stuck or if you see that there are holes in your understanding, review your notes or go back to the text and try again. Keep writing it out until you have a good handle on the main ideas and feel confident in your explanation.
        
        I’ve found that almost nothing reveals gaps in my thinking better than writing about an idea as if I am explaining it to a beginner. Ben Carlson, a financial analyst, says something similar, “I find the best way to figure out what I’ve learned from a book is to write something about it.”
        
        6. Surround the Topic

        I often think of the quote by Thomas Aquinas, “Beware the man of a single book.”
        
        If you only read one book on a topic and use that as the basis for your beliefs for an entire category of life, well, how sound are those beliefs? How accurate and complete is your knowledge?
        
        Reading a book takes effort, but too often, people use one book or one article as the basis for an entire belief system. This is even more true (and more difficult to overcome) when it comes to using our one, individual experience as the basis for our beliefs. As Morgan Housel noted, “Your personal experiences make up maybe 0.00000001% of what's happened in the world but maybe 80% of how you think the world works. We're all biased to our own personal history.”
        
        One way to attack this problem is to read a variety of books on the same topic. Dig in from different angles, look at the same problem through the eyes of various authors, and try to transcend the boundary of your own experience.
        
        7. Read It Twice

        I'd like to finish by returning to an idea I mentioned near the beginning of this article: read the great books twice. The philosopher Karl Popper explained the benefits nicely, “Anything worth reading is not only worth reading twice, but worth reading again and again. If a book is worthwhile, then you will always be able to make new discoveries in it and find things in it that you didn’t notice before, even though you have read it many times.”
        
        Additionally, revisiting great books is helpful because the problems you deal with change over time. Sure, when you read a book twice maybe you'll catch some stuff you missed the first time around, but it's more likely that new passages and ideas will be relevant to you. It's only natural for different sentences to leap out at you depending on the point you are at in life.
        
        You read the same book, but you never read it the same way. As Charles Chu noted, “I always return home to the same few authors. And, no matter how many times I return, I always find they have something new to say.”
        
        Of course, even if you didn't get something new out of each reading, it would still be worthwhile to revisit great books because ideas need to be repeated to be remembered. The writer David Cain says, “When we only learn something once, we don’t really learn it—at least not well enough for it to change us much. It may inspire momentarily, but then becomes quickly overrun by the decades of habits and conditioning that preceded it.” Returning to great ideas cements them in your mind.
        
        Nassim Taleb sums things up with a rule for all readers: “A good book gets better at the second reading. A great book at the third. Any book not worth rereading isn’t worth reading.”
        
        Where to Go From Here
        Knowledge compounds over time.
        
        In Chapter 1 of Atomic Habits, I wrote: “Learning one new idea won’t make you a genius, but a commitment to lifelong learning can be transformative.”
        
        One book will rarely change your life, even if it does deliver a lightbulb moment of insight. The key is to get a little wiser each day.
        
        Now that you know how to get more out of each book you read, you might be looking for some reading recommendations. Feel free to check out my book summaries or my public reading list.`
    },
    {
        title:'Living Deliberately: The Importance of Long Walks, Paintings, and Board Games',
        author:'Abhishek Chakraborty',
        article:`The essence of the philosophy of Digital Declutter is that digital technology and services aren’t inherently evil, but a bulk of them have been engineered to be addictive. They aren’t really free because we pay for them with our time—the time we could have used doing something else, perhaps something more meaningful. This leads to the question: what all can we do with the free time?

        Growing up, I didn’t have internet at my place until I was in college, and most of my school years were spent without a computer. We had landlines instead of mobile phones, so definitely I wasn’t texting all the time. I’m a 90s kid, but since I grew up in a lower-middle class family, having a computer was pretty expensive, and hence wasn’t a priority in my family.
        
        That left me with plenty of free time after school and studies. Interestingly, none of the kids in my neighbourhood had a computer or a mobile phone, so our schedules were always the same. School was over at 4 o’clock, and nobody had anything better to do in the afternoon than to come out and play. Most of my childhood days were spent playing cricket, taking long evening walks and bike trips, practising painting, and reading books. (Being honest, I struggled to read books, but I tried nonetheless.) Life was fun, and I had nothing to complain.
        
        Now that I look back and think about those days, I was unknowingly engaged in all sorts of activities that are essential to live deliberately. I consider three most important practises to live deliberately and enjoy life to the fullest: practising solitude, planning leisure, increasing real world interactions. I still try to practice them as much as possible.
        
        Practising Solitude

        I went to visit Bhutan in 2017. My guide was a man of many talents. Apart from a travel guide, he was a carpenter, worked in a temple, and was also a closet philosopher. I remember having memorable chats with him during our treks. He was a practising Buddhist. He shared the importance of the ‘practice of contemplation’ with me.
        
        Meditation is when you sit with yourself and don’t think of anything else—when you let your mind go blank. Contemplation, on the other hand, is when you sit with yourself and think deeply—about yourself, your thoughts, your actions, about the world, and about the work that you are doing. Contemplation helps you make sense of everything, take a step back, and look at everything holistically.
        
        As a kid, I enjoyed taking long walks in the evening. I’m sure as a teenager you had a tonne of problems in your life that you didn’t know how to deal with. All of us have been there. My evening walks gave me the opportunity to contemplate, to think about things, to form opinions, and make plans to deal with whatever was happening.
        
        I didn’t have a mobile phone, so I had little chance of getting distracted. I went to the local stadium to watch the sunset. I stayed until dusk. It was both a soothing and an amazing experience. Apart from my long walks, I also did a lot of my thinking through journaling. I asked questions, answered them, wrote down my viewpoints, and countered them—all in solitude. As adults, our problems have only gotten bigger and far more complex, but unfortunately, we don’t have enough solitude to contemplate.
        
        Contemplation, like meditation, needs solitude. It cannot be practised in any other form. You have to take time off to be with yourself. Solitude is important to develop a worldview.
        
        We are constantly listening to others—their ideas, their suggestions, they opinions. Solitude gives us the opportunity to listen to ourselves. It is important for the brain to process all that information it is being bombarded with—to organise and arrange it, make connections, tweak some, delete some, and store all of them in a digestible format so that they can fit into the whole array of information already in the brain, and so that they can be retrieved whenever needed. This practice dissipates confusion and promotes clear thinking.
        
        Learn to be with yourself. Be in the company of your thoughts. Practice solitude. You’ll become more self-aware, more mindful, and less confused.
        
        Planning Leisure
        Aristotle had said that high quality leisure is essential to a life well lived. Just to point it out, binging on Netflix while downing wine isn’t high quality leisure. Playing badminton, doing pottery, or playing an instrument are.
        
        As a kid, like all the other kids in my neighbourhood, I went to art school. I was an okay student there—not the best, but not the worst either. But to be honest, I really learned to draw only after I graduated from art school. I liked to draw in my free time. Since I had no exam to give or a teacher to please, I could experiment, fail, and learn. I immensely enjoyed it.
        
        I used to do one or two paintings a week, and try to learn something new every time. At one point I was fascinated with glass painting. I remember trying water colours as well. But my all time favourite remained pencil sketches. I picked up digital painting much later.
        
        I’m sure you have done similar activities as a kid—played the piano or took dance lessons. You should know that unlike playing video games, playing an instrument is hard work. It requires a lot of cognitive effort. There’s a stark difference between low quality distractions—such as scrolling an Instagram feed, and high quality leisure—such as learning to play the violin.

        As you know, a cognitively demanding activity makes the brain work. A high quality leisure also makes the brain work, but on something else. Most of the time, you brain doesn’t require a break—it gets plenty of rest when you sleep. It simply requires variety. It needs challenges, only a different kind. Having a cognitively demanding hobby allows your brain to draw analogies, make connections between unconnected topics, and eventually make you more creative.

        As kids, we all played sports or learned something else apart from studies. As adults, we should learn to pick up new things as well. I love to do cooking and experiment with recipes as part of my planned leisure. My girlfriend does gardening and embroidery as part of her planned leisure.

        There’s a reason it’s called planned leisure, and not just leisure. Because without planning, there’s a good chance you will default to the easiest form of leisure—social media, video games, or Netflix.

        You can pick up any activity you want. You don’t need to be good at it. You just gotta enjoy it, and this hobby needs to give you plenty of opportunity to grow. A couple of years back I had a flatmate who loved playing tennis. He was very poor at it, but still had a lot of passion to learn. He put in a lot of effort every week, and no matter how he played, he loved and enjoyed what he did.

        I’m an amateur guitar player. I’m not very good at it, but still I enjoy the process. I’m sure I’ll get better with time. Even if I don’t, it’s okay. “A life well-lived requires activities that serve no other purpose than the satisfaction the activity generates itself,” said Aristotle. So, if you love doing something challenging, do it just for the sake of the satisfaction it brings you, nothing else.

        Do something with your hands. Make a furniture, do some pottery. Assign yourself free time activities that are cognitively challenging, and make sure you don’t sleepwalk into low quality distractions.

        Increasing Real World Interactions

        I’m shy by nature. I cannot engage in smalltalk, and I don’t usually prefer crowded places unless I’m with my friends. Among my friends, I’m the most extroverted person, but in the company of strangers, I struggle a lot.

        Over the years, I have realised that I struggle only with smalltalk—when there’s no fixed agenda. But whenever there’s a topic of discussion, I’m as extroverted as they come. This is the case with almost all introverts. They like to talk; they just don’t like to smalltalk.

        After I realised this, I started attending small meetup groups on variety of topics, and fell in love with the experience. Book clubs and other offline communities are all in my forte because the agenda is preset—there’s a shared goal (in board gaming clubs) or a shared topic of discussion (in books and movie clubs).

        Back in 2014, I had designed and built a board game as part of a class project. During the process of research, I got fascinated by the idea of playing boardgames. Later, I became a regular at local board gaming meetups, and eventually started inviting friends over at my place to have dinner and play games. Given that I love to cook, it definitely helped a lot. It was the amount of fun that just cannot be had in online communities.

        As a kid, we had a small group of friends who often engaged in heated debates and discussions around religion, politics, science, and life. If you attend a local debate club, or if you ever find yourself in Kolkata and see a group of adults discussing football and politics in a tea stall, you’ll instantly know how different it is from online discussion forums. The sensory richness of an offline community which promotes real world encounters is such that a Subreddit, or a Facebook Group, or a BoardGameGeek community simply cannot provide.

        When I was studying design, I was heavily into street photography, which gave me multiple opportunities to engage with strangers. Like many, I’m a huge fan of Humans of New York by Brandon Stanton, where he interacts with strangers, learns about their experiences, takes their photos, and publishes these beautiful photo stories. I tried to emulate him on an extremely tiny scale. It was kind of bittersweet. There were many awkward moments, but there were nicer experiences as well.

        If you do it in person, you’ll realise that talking to a stranger is far richer than interacting with a stranger over chat or comments. It’s intimidating, especially when you have to explain why you want to take their photo, why do you engage in this silly hobby of photography, and why will they tell you their story. But when somebody gets you, lets you take their picture, and agrees to talk to you, it’s all worth it in the end. No online interaction can ever match that.

        In a board gaming session, there will be laughters, disappointments, and master-level social chess matches among friends, acquaintances, and strangers—all in the confines of a game. Players subconsciously study their opponent’s body language in search of clues about their strategy, and try to project themselves into the mind of the opponent to understand what they might be plotting in their next move. This play of complex emotions just isn’t possible in an online gaming session. The sting of defeat is all the more real when the victor is sitting right in front of you. We need to feel these emotions to feel alive, sometimes even to be alive.

        Join an interest club, or start one. Invite your friends for a boardgaming session at your place. Go on a trek with a group of strangers—I can promise you, the quality and richness of real world interactions is unbeatable.`
        
    },
    {
        title:'5 Mindsets That Create Success',
        author:'Mark Manson',
        article:`Five incredible success stories revealing five mindsets that create success.

        Here are five stories from five different people that reveal the mindsets of the most successful among us.

        1. You Always Have a Choice

        Ursula Burns was raised by her single mother in the projects of New York in the 1960s and ‘70s. Back in those days, she was born with three strikes against her: she was black, poor, and female. Life would be hard.
        
        Her mother scrimped and saved and worked extra jobs just to provide for Ursula and her siblings, but more importantly she constantly reminded them that where they were right now didn’t have to define them for the rest of their lives. They always had a choice. They could do the best with what they had.
        
        Ursula worked her ass off. She stayed on top of her studies and got into engineering school at the Brooklyn Polytechnic School, which was, not surprisingly, made up almost entirely of white affluent men. She soon realized she had a lot of catching up to do, both academically and socially. She was an outsider in every sense of the word.
        
        But somehow, she graduated from engineering school and worked her way up to become the CEO of Xerox, managing to turn the once-flailing company back to profitability. She also served as the head of the STEM Education Coalition under President Obama, and has been on the boards of some of the world’s largest companies, including Exxon Mobil, Uber, and VEON, the world’s 10th largest telecom company.

        Inspired by her mother’s encouragement, Burns developed early in life what psychologists call a “growth mindset,” which is essentially just the belief that one has a certain degree of personal influence over their life.

        Contrast this with a “fixed mindset,” which is the belief that you have little to no control over your life.

        The truth is, there are things in life you can control and things you can’t.

        You have absolutely no control over where you were born, your biological sex, how rich or poor your family is, what color your skin is, how tall you are, etc. These things do matter and they will obviously impact your life in major ways.

        But while you may not be to blame for your situation, you are always responsible for your situation.

        It wasn’t Ursula’s fault that she was born into a poor family. But instead of defining herself as poor and being a victim of her own circumstances, she turned that on its head and let her story inspire her life. She owned her scars and wore them openly instead of using them as an excuse to not even try.

        Similarly, it’s not your fault if you were born poor or fat or prone to mental illness. But it is your responsibility to figure out how to deal with your situation.

        No one else can heal your emotional wounds but you. No one else can fix your toxic relationship with money but you. No one else can lose that weight for you. No one else can make that person fall in love with you.

        That isn’t to say you have to do it all by yourself. You should seek out help if you need it, hire a trainer if you can afford it, and get financial help when your luck is down. But for better or worse, at the end of the day, it’s all on you.

        You have been/will be handed some real turds in life. You will have some advantages over others, some of which you earned and others you didn’t. Dwelling for too long on either of these will only lead you down the fixed-mindset rabbit hole at some point, and that’s a miserable hole to be in.

        
        2. Adopt a Bias Towards Action

        Chuck Close’s father died when he was eleven-years-old. As a teenager, he was told not to even think about going to college. He had several learning disabilities and couldn’t even add or subtract. His teachers told him trade school would be his only hope since he was pretty good with his hands. But he also suffered from a neuromuscular disorder that limited his mobility, so even that was iffy.

        Today, Chuck Close is an internationally-renowned painter and graphic artist whose work hangs on some of the most famous walls in the world.

        That would be amazing enough on its own, given his early life hardships, but what’s even more remarkable is that Close continued to produce world-class artwork after a blood clot left him paralyzed in his late 40s.

        How the hell does he do it?

        Well, he once said, in a note to his younger self, “Inspiration is for amateurs. The rest of us just show up and get to work. Every great idea I’ve ever had grew out of work itself.”

        Most people approach work and motivation in the completely opposite way: they wait to be inspired, then they get to work. The problem is that inspiration is a fickle beast. Some people will wait around forever for inspiration to just fall out of the sky or something. Others spend all their time and energy looking for ways to motivate themselves so they can finally get to work. And the irony of that sentence is completely lost on them.

        I’ve sometimes experienced this myself when people, who are not writers, come up to me all excited, telling me that I have to meet their friend’s mother, because she has this incredible book idea.

        So?

        I’ve got millions of book ideas. People who don’t create for a living think that the ideas are the hard part. No, ideas are easy. Everyone has ideas. But few people can execute on the ideas. Few people can deal with the possibility that their ideas might be bad. So their ideas stay ideas.

        Successful people don’t sit around and wait for their muse to come and inspire them to change the world. They just show up and get to work.

        So, do something.

        
        3. Let Go of the Need to Be Right

        Ray Dalio is one of the richest men you’ve probably never heard of. But before he got rich, he went flat broke because of how right he thought he was.

        In the early 1980s, Dalio was on the warpath, warning everyone and their financially unstable uncle that the stock market was about to crash and burn like it was 1929 all over again. Instead, starting in 1982, stocks went on an eight-year bull run and returned one of their best performances in history.

        Dalio went completely broke betting against the market. And, more importantly, he had to avoid a lot of Manhattan cocktail parties for a while.

        But after wiping the egg off his face, he realized it wasn’t necessarily his bad hypotheses or incorrect economic analyses that made him lose every penny he had. Because, in the end, it turned out he was right. The economy did crash… eight years after he said it would.

        No, it was his unrelenting belief in himself that he was right that made him go broke and look like a complete idiot.

        Dalio vowed to never let his ego overrun his decision making like this ever again. Today, he constantly analyzes even his most basic assumptions about the world and tries to poke holes in his own theories. He demands his employees—even his interns—give him brutally honest feedback about his views to try to prove him wrong.

        He realized that he’d rather be challenged and proven wrong about his beliefs than cling to them in a desperate attempt to show the world he was “right.”

        He’s now been an investor for over 50 years and has amassed a fortune in the tens of billions of dollars. Dalio’s company, Bridgewater Associates, is one of the world’s largest hedge funds and has consistently beat the market in good times and bad for decades now.

        Anyone can survive a bad idea, a stupid mistake, or dumb risk or twelve as long as you don’t cling to the need to be right about your beliefs.

        The fact is, you, me, and everyone on the planet are almost certainly wrong about… well, pretty much everything. And we can never be 100% sure we’re right about anything.

        We can only learn from our observations and hopefully be a little less wrong.

        I see this ability to be wrong as so central to living a good life that I dedicate a whole course to it in The Subtle Art School. The Challenge Your Own Beliefs Course will help you question everything you believe so you can hold your beliefs more lightly and don’t keep making the same mistakes in life. You’re welcome.

        
        4. See the World for What It Is, Not for What You Wish It Could Be

        Dr. Patrick Brown is a vegan. He gave up meat a long time ago for his own ethical reasons.

        And while he believes his moral standards of veganism are “right” and true and compassionate, he understands something a lot of people don’t: that you can’t change other people’s behavior by appealing to their moral code.

        In fact, he knows that exactly the opposite will happen when you try to persuade someone this way: they’ll double down and call you a fuckface and won’t invite you to their birthday party.

        So instead of proselytizing to the world about the moral and environmental impacts of eating meat or protesting outside of a major meat production facilities with a smug sense of moral superiority, he decided to appeal to something much more fundamental to human nature: their taste buds.

        Brown’s goal is to replace all animal-based meat production by 2035. To do that, he is trying to create food that 1) tastes/looks/smells/feels as good or better than real meat and 2) is at least the same price, if not cheaper.

        Dr. Brown started a company, Impossible Foods, the creators of the meat-free, plant-based Impossible Burger. His goal with the Impossible Burger is to create something that mimics ground beef in every way—appearance, texture, smell, and, of course, how it tastes—using no animal products whatsoever.

        And the Impossible Burger 2.0 appears to have come pretty damn close to doing just that.

        By appealing to human nature rather than railing against it, Brown has already made an incredible impact on the world, and he appears to just be getting started.

        Had he been like most zealots in the environmental and animal rights world and prattled on and on about how unethical it is to eat meat in the modern world, reigning down his judgment on everyone else with indignant fury—well, no one would have listened to him.

        And more importantly, he would have had virtually zero impact on the way we think about food production.

        
        5. Define Success Internally, Not Externally

        Amada Rosa Pérez was one of Colombia’s most famous supermodels. She worked on shoots in some of the most beautiful places in the world and was used to being lavished with attention and fame and money.

        Her career seemed to be on a trajectory that 99.9% of aspiring models only wish they could achieve.

        Then, in 2005, at the height of her career, Pérez inexplicably disappeared from the public eye.

        People suspected the worst—this was Colombia after all—kidnappings, ransoms, murder, etc. But the truth, as usual, was much stranger than fiction. Perez surfaced five years later and announced that she’d been born again. And she was retiring from modeling to work with the poor communities of Colombia.

        She remarked on how her definition of a successful model had changed—drastically so:

        “Being a model means being a benchmark, someone whose beliefs are worthy of being imitated, and I grew tired of being a model of superficiality. I grew tired of a world of lies, appearances, falsity, hypocrisy, and deception, a society full of anti-values that exalts violence, adultery, drugs, alcohol, fighting, and a world that exalts riches, pleasure, sexual immorality and fraud. I want to be a model that promotes the true dignity of women instead of being used for commercial purposes.”

        Prior to her religious conversion, she said she was always stressed, always in a hurry, always so easily upset over the tiniest of things. “Now I live in peace,” she said. “The world doesn’t appeal to me. I enjoy every moment God gives me.”

        I’m not a religious person by any means, and Pérez and I probably don’t share a lot of values about things like sexuality. But I totally get Pérez’s need for real meaning in her life—for something more than the superficialities of comfort and pleasure. It’s a main theme of pretty much all of my writing.

        I’ve railed against the all-too-common, toxic ideas in modern culture that more is better and that we can “have it all”. Because I don’t think happiness is something you should pursue for its own sake. In fact, happiness isn’t really the point to begin with, and trying to “feel good” all the time will only make you a miserable mess of a human being.

        In all of the stories told here, if you look a little closer, you’ll notice that each person—all of them successful by typical measures of success—had a higher purpose that transcended the typical measures of success.

        Even Ray Dalio, a multi-fucking-billionaire, has made it his life purpose to educate the world—for free—about what he calls his “principles” of life.

        My new book, Everything Is F*cked: A Book about Hope, is a deep dive into how we find hope and meaning in the world—and how finding hope and meaning in the world can really fuck us if we’re not careful.

        In almost every material aspect, the world is a better place than it ever has been. But instead of rejoicing in that fact, we’re living through a crisis of hope and meaning that threatens to reverse a lot of the progress we’ve made over the past two centuries or so.

        But Amada Rosa Pérez showed us that we can define success in a way that lifts others up, gives us a little peace, and maybe leaves the world a little less fucked than it was when we showed up.`

    },
    {
        title:'Transform Your Life By Transforming Your Habits',
        author:'Darius Foroux',
        article:`Where you are in your life is a result of your habits. Will Durant (not Aristotle) said it best:

        “We are what we repeatedly do. Excellence, then, is not an act, but a habit.”
        
        I think that’s also true for the opposite of excellence. Mediocrity is a result of mediocre habits.
        
        That means we can go from mediocrity to excellence by changing our habits.
        
        But how do you do that? Before we get into that, I want to clarify my statement: Habits change your life, but they do not guarantee success.
        
        Because that’s what “the habits of millionaires” type of articles and books tell us. We get it, Elon Musk sleeps 2 hours a day and eats Cheerios for dinner—or something like that.
        
        But what the writers of those type of moronic articles hide from you, the reader, is that correlation doesn’t mean causation.
        
        Waking up early, working hard, and taking cold showers do not cause success. No one illustrates that point better than Nassim Taleb, who wrote this in Fooled By Randomness:
        
        “Hard work, showing up on time, wearing a clean (preferably white) shirt, using deodorant, and some such conventional things contribute to success—they are certainly necessary but may be insufficient as they do not cause success.”
        
        So when I talk about habits, I don’t talk about outcomes. I talk about changing our actual behavior so that it improves the quality of our lives.
        
        Now that we’ve got that out of the way, here’s my four-step process for making life-changing habits stick.
        
        Step 1: Decide what habits are worth it

        Look, I can talk to you about the habits that have changed my life habits all day long, but that’s not helpful. The reason is that only YOU can decide what a good habit is.
        
        Deciding if a habit is worth it to you is critical to forming new habits. Too often we hear about something, and we think: “I should do that!”
        
        Really? Should I wake up an hour earlier? Should I take cold showers? Should I eat like a cave person? Should I run every day?
        
        Maybe waking up early is actually helpful to you. I don’t know. When I wake up early, I behave like a grumpy old man who hates people—that deteriorates the quality of my life.
        
        Hence, I don’t wake up very early (7 AM or earlier) no matter how many people tell me it will make me successful.
        
        Just ask yourself this:
        
        “Will habit x improve the quality of my life?”
        
        The reason you want to ask yourself that question is that we all need a reason to change. We need something that’s bigger than superficial reasons.
        
        “I want to read one book a week,” you might say. Why? So you can do what? What’s your vision? What are your goals?
        
        For example, I read two books a week because I’m a teacher/trainer. I need to learn every single day so I can do my job better. I do strength training because I want to be a useful person. I do many chores around my house and the office. I want to make and fix things. I need to be in good shape to lift heavy things or do strenuous tasks.
        
        That’s my why. What’s yours? Answer that. And then, adopt habits that bring you closer to the things you want in life.
        
        Step 2: Focus on one habit at a time

        I wrote about how I successfully formed a daily exercise habit. It was something I tried to do for years.
        
        There were many reasons I failed, one of them is that I always tried to form a million habits at the same time.
        
        I don’t know why, but sometimes I get on this whole self-improvement spree. I feel like reading more, writing more, working more, living better, eating healthier, you name it.
        
        It’s best to hold back the enthusiasm if you’re the same. In general, when you do too many things at the same time, you end up with chaos.
        
        And you always end up right back where you started. Sounds familiar?
        
        One of the reasons we try to do so many things at the same time is that we overestimate ourselves. We think we can achieve a lot in a short period. That’s false.
        
        We can achieve A LOT over a long period. That’s true.
        
        So focus on one thing at a time. Stack one habit on top of the other, one by one (just like in this post’s image, at the top).
        
        Step 3: Set the bar very low

        We often want to do big things, without understanding it. Starting a business or building a career requires effort. In fact, everything in life that’s remotely valuable requires a lot of work to achieve.
        
        So before we do something big, let’s start small. Similarly, before you change the world, change yourself first. Leo Tolstoy, the author of War and Peace, put it best:
        
        “Everyone thinks of changing the world, but no one thinks of changing himself.”
        
        Focus on small things. Build a strong foundation. Without it, we can never achieve anything meaningful.
        
        Want to run daily? Start by walking.
        Want to write a book? Write once sentence.
        Want to start a business? Get one client.
        Want to read two books a week? Read one page a day.
        Want to save for your retirement? Don’t buy another shirt you’re only going to wear once.
        Etc.

        Big things follow by themselves.
        
        Step 4: Use checklists

        I forget everything. A few years ago, I started a daily reading habit. I messed up very often in the beginning.
        
        I would read for five or six days straight, and then all of a sudden, I would completely forget about it.
        
        It’s like the desire for reading more just vanished from my head.
        
        You want to do something. You do it. And then you forget about it. Shit happens, right?
        
        No, that’s weak. Don’t let yourself off the hook like that.
        
        We must use checklists to remind ourselves of what we’re trying to achieve. Remember: We form habits to transform our lives—to make things BETTER.
        
        Check off your habits daily. One day, you’ll be surprised by how much your life changed by such, seemingly, small habits. At least, that’s what happened to me and the thousands of other people who focus on their habits. And I’m sure it will happen to you too.`        
    },
]
module.exports = articles


